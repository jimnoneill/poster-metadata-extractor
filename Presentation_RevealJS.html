<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scientific Poster Metadata Extraction</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/white.css">
    <style>
        .reveal .slides { text-align: left; }
        .reveal h1, .reveal h2 { text-align: center; }
        .reveal table { font-size: 0.8em; }
        .reveal code { background: #f8f8f8; padding: 2px 4px; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
</section><section>
title: "Scientific Poster Metadata Extraction: Three-Method Approach"
author: "Jamey O'Neill, PhD, MSc"
date: "August 29, 2025"
theme: "white"
transition: "slide"
</section><section>

<h1>Scientific Poster Metadata Extraction
#<h1>Three Approaches for Different Requirements

**Jamey O'Neill, PhD, MSc**  
August 29, 2025

</section><section>

<h1>The Challenge

#<h1>Why This Matters
- **Manual processing bottleneck**: Libraries process thousands annually
- **Inconsistent metadata quality**: Human extraction varies  
- **Scalability issues**: Massive conference poster volumes
- **Cost constraints**: Manual curation expensive at scale

#<h1>Target Metadata (Table 1)
1. Title, Authors, Affiliations
2. Summary, Keywords, Methods
3. Results, References, Funding

</section><section>

<h1>Our Three-Method Solution

| Method | Best For | Key Advantage |
|</section><section></section><section>--|</section><section></section><section></section><section>-|</section><section></section><section></section><section></section><section></section><section>|
| **DeepSeek API** | Quick deployment | Balance cost/accuracy |
| **Qwen Local** | Privacy-sensitive | Complete data control |
| **BioELECTRA+CRF** | Maximum accuracy | Zero hallucination |

#<h1>Design Philosophy
- **No one-size-fits-all** approach
- **Modular architecture** for easy comparison
- **Bootstrapping strategy** for data generation

</section><section>

<h1>Method 1: DeepSeek API
#<h1>The Pragmatic Choice

##<h1>Performance Characteristics
- **Accuracy**: 85-90% (requires validation)
- **Cost**: ~$0.003 per poster (200x cheaper than GPT-4)
- **Speed**: 5-15 seconds per poster
- **Setup**: Easy - just API key

##<h1>Best For
Production systems with API budget, high-volume processing

</section><section>

<h1>Method 1: Technical Details

#<h1>Enhanced Structured Prompting
```python
prompt = create_extraction_prompt(poster_text)
response = deepseek_client.chat.completions.create(
    model="deepseek-chat",
    messages=[system_prompt, user_prompt],
    temperature=0.1
)
```

#<h1>Live Results
- ‚úÖ **Title**: Correctly extracted
- ‚úÖ **Authors**: 5 identified with affiliations  
- ‚úÖ **Keywords**: 8 technical terms
- ‚è±Ô∏è **Time**: 25.6 seconds, **Cost**: $0.0007

</section><section>

<h1>Method 2: Qwen Local
#<h1>The Privacy-First Approach

##<h1>Performance Characteristics
- **Accuracy**: 80-85% (requires validation)
- **Cost**: $0 (electricity only)
- **Speed**: 10-40s single / ~1.1s batched (RTX 4090)
- **Privacy**: 100% local processing

##<h1>RTX 4090 Batching
- **32 posters simultaneously**
- **3,273 posters/hour throughput**
- **26,182 posters/day** (8-hour operation)

</section><section>

<h1>Method 2: Technical Implementation

#<h1>8-bit Quantization for Efficiency
```python
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16
)
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    quantization_config=bnb_config
)
```

#<h1>Best For
Privacy-sensitive environments, edge computing, budget-conscious deployments

</section><section>

<h1>Method 3: BioELECTRA+CRF
#<h1>The Future Possibility üß¨

##<h1>Why BioELECTRA?
- **üèÜ 2nd highest** on BLURB biomedical leaderboard
- **Domain-optimized**: Pre-trained on PubMed corpus
- **Zero hallucination**: Deterministic sequence labeling

##<h1>Expected Performance (With Training)
- **Accuracy**: 85-92% (estimated from BLURB benchmarks)
- **Speed**: <0.5 seconds per poster (fastest)
- **Hallucination**: 0% (deterministic BIO tagging)
- **Cost**: $0 (after training)

</section><section>

<h1>Method 3: Training Requirements

#<h1>BIO Tagging Example
```
Input:  "This study by Dr. Smith uses microfluidic synthesis"
Labels: ['O','O','O','B-AUTHOR','I-AUTHOR','O','B-METHOD','I-METHOD']
```

#<h1>Training Investment Needed
- **500-1000 labeled posters** required
- **40-60 hours** expert annotation time
- **2-4 hours** training on V100 GPU

#<h1>Current Status
Demo only - requires substantial training data investment

</section><section>

<h1>Comparative Analysis

| Feature | DeepSeek API | Qwen Local | BioELECTRA+CRF |
|</section><section></section><section></section><section>|</section><section></section><section></section><section></section><section>-|</section><section></section><section></section><section></section><section>|</section><section></section><section></section><section></section><section></section><section>-|
| **Accuracy** | 85-90%* | 80-85%* | 85-92%* |
| **Cost/poster** | $0.003 | $0 | $0 |
| **Speed** | 5-15s | 10-40s | <0.5s |
| **Privacy** | External API | 100% Local | 100% Local |
| **Hallucination** | Low-Med | Low | **None** |
| **Training** | No | No | **Required** |

*All accuracy estimates unvalidated - require Cochran sampling

</section><section>

<h1>Decision Framework

#<h1>Method Selection Guide
- **Immediate deployment** ‚Üí Method 1 (DeepSeek)
- **Privacy critical** ‚Üí Method 2 (Qwen)  
- **Maximum accuracy investment** ‚Üí Method 3 (BioELECTRA)

#<h1>Key Considerations
- Budget constraints vs accuracy requirements
- Privacy/security requirements
- Technical infrastructure capabilities
- Long-term scalability needs

</section><section>

<h1>Validation Framework
#<h1>Cochran's Sampling Methodology üìä

##<h1>Why Statistical Validation?
All accuracy estimates are **unvalidated** - proper validation essential

##<h1>Sample Size Requirements
- **1,000 posters** ‚Üí Validate **278 samples** (27.8%)
- **10,000 posters** ‚Üí Validate **370 samples** (3.7%)  
- **100,000+ posters** ‚Üí Validate **383 samples** (0.4%)

##<h1>Field-Specific Metrics
- **Title**: Exact match or semantic similarity >0.8
- **Authors**: Fuzzy matching (edit distance <2)
- **Keywords**: Overlap coefficient >0.6

</section><section>

<h1>System Architecture

```mermaid
graph TD
    A[PDF Input] --> B[PyMuPDF Text Extraction]
    B --> C{Method Selection}
    
    C -->|API Budget| D[Method 1: DeepSeek]
    C -->|Privacy Required| E[Method 2: Qwen Local]
    C -->|Max Accuracy| F[Method 3: BioELECTRA+CRF]
    
    D --> G[JSON Output]
    E --> G
    F --> G
    
    G --> H[Cochran Validation]
    H --> I[Production Deployment]
```

</section><section>

<h1>Live Demonstration Results
#<h1>Poster: "Drug-Polymer Interactions in PLGA/PLA-PEG Nanoparticles"

##<h1>Method 1 (DeepSeek) Performance
- ‚úÖ **Title**: "INFLUENCE OF DRUG-POLYMER INTERACTIONS..."
- ‚úÖ **Authors**: 5 identified (Merve Gul, Ida Genta, et al.)
- ‚úÖ **Affiliations**: University of Pavia, UPC-EEBE
- ‚úÖ **Keywords**: 8 technical terms extracted
- ‚è±Ô∏è **Processing**: 25.6 seconds
- üí∞ **Cost**: $0.0007

##<h1>Method 2 (Qwen) Performance  
- ‚úÖ **Title**: Correctly extracted
- ‚úÖ **Authors**: 5 identified
- ‚è±Ô∏è **Processing**: 47.5 seconds, **Cost**: $0

</section><section>

<h1>Implementation Roadmap

#<h1>Phase 1: Quick Start (Week 1)
1. **Deploy Method 1** (DeepSeek API) for immediate results
2. **Implement Cochran sampling** on first 100-500 extractions
3. **Measure actual accuracy** vs. 85-90% estimate

#<h1>Phase 2: Optimization (Month 1-2)
1. **Add Method 2** (Qwen Local) for privacy-sensitive content
2. **Compare accuracy** between methods on validation set
3. **Optimize prompts** based on failure analysis

#<h1>Phase 3: Advanced Implementation (Month 3-6)
1. **Collect training data** using Methods 1&2 auto-labeling
2. **Train BioELECTRA+CRF** model on 500-1000 labeled posters
3. **Deploy Method 3** for maximum accuracy production

</section><section>

<h1>Expected ROI

#<h1>Cost Comparison
- **Manual Processing**: $5-10 per poster (human time)
- **Method 1 (DeepSeek)**: $0.003 per poster 
- **Method 2 (Qwen)**: $0 per poster
- **Method 3 (BioELECTRA)**: $0 per poster (after training)

#<h1>ROI Analysis
- **99.97% cost reduction** vs manual processing
- **Consistent quality** vs variable human performance
- **24/7 availability** vs limited human resources
- **Scalable throughput** for conference-scale processing

</section><section>

<h1>Current Limitations

#<h1>Technical Constraints
- **Accuracy estimates unvalidated** - require statistical validation
- **English-only optimization** - multilingual support needed
- **Text-only processing** - image/diagram extraction not included
- **Method 3 requires training** - substantial data annotation

#<h1>Validation Requirements
- **Cochran sampling essential** before production use
- **Field-specific accuracy testing** needed
- **Cross-domain validation** for different poster types

</section><section>

<h1>Future Enhancements

#<h1>Immediate Improvements (3-6 months)
- **OCR integration** for scanned/image-only posters
- **Multilingual support** (Spanish, French, German)
- **Complete Method 3 training** with auto-labeled data

#<h1>Advanced Developments (6-12 months)
- **Multi-modal architecture** using LayoutLM for visual layout
- **Real-time API deployment** for library system integration
- **Cross-lingual transfer learning** for global collections

#<h1>Research Extensions (1+ years)
- **Graph-based extraction** for author-institution relationships
- **Temporal analysis** for research trend tracking
- **Automated quality assessment** with confidence scoring

</section><section>

<h1>Technical Specifications

#<h1>Hardware Requirements
| Method | CPU | RAM | GPU | Storage |
|</section><section></section><section>--|</section><section>--|</section><section>--|</section><section>--|</section><section></section><section></section><section>|
| **Method 1** | Any | 4GB | None | Minimal |
| **Method 2** | Modern | 16GB | 8GB VRAM | ~3GB |
| **Method 3** | Modern | 16GB | 8GB VRAM | ~800MB |

#<h1>Key Dependencies
- **Core**: Python 3.8+, PyMuPDF, transformers
- **Method 1**: openai, python-dotenv  
- **Method 2**: torch, bitsandbytes, accelerate
- **Method 3**: pytorch-crf, spacy (post-training)

</section><section>

<h1>Key Takeaways

#<h1>Critical Success Factors
1. **Three complementary approaches** address different organizational needs
2. **Statistical validation essential** - all estimates require Cochran sampling
3. **Immediate deployment possible** with Method 1 (DeepSeek API)
4. **Privacy-first option available** with Method 2 (Qwen Local)
5. **Future scalability** through Method 3 training investment

#<h1>Bottom Line
**Start with Method 1, validate with Cochran sampling, evolve to Method 3 for production scale**

**99.97% cost reduction vs manual processing**

</section><section>

<h1>Questions & Discussion

#<h1>Contact & Resources
- **Full Documentation**: README.md with complete technical specs
- **Implementation Code**: Available in `/src/` and `/notebooks/`
- **Live Results**: JSON outputs in `/output/` directory

##<h1>Key Repository Structure
```
poster_project/
‚îú‚îÄ‚îÄ src/                    <h1>Python implementations
‚îú‚îÄ‚îÄ notebooks/              <h1>Jupyter demonstrations  
‚îú‚îÄ‚îÄ output/                 <h1>Extraction results
‚îî‚îÄ‚îÄ data/                   <h1>Sample posters
```

**Thank you for your attention!**

</section><section>

<h1>Appendix: Validation Details

#<h1>Cochran's Formula
```
n = (Z¬≤ √ó p √ó (1-p)) / e¬≤
n_adjusted = n / (1 + (n-1)/N)
```

#<h1>Sample Size Table
| Population | Sample Size | Percentage |
|</section><section></section><section></section><section></section><section>|</section><section></section><section></section><section></section><section>-|</section><section></section><section></section><section></section><section>|
| 100 posters | 80 samples | 79.5% |
| 500 posters | 217 samples | 43.5% |
| 1,000 posters | 278 samples | 27.8% |
| 10,000 posters | 370 samples | 3.7% |
| 100,000+ posters | 383 samples | 0.4% |

</section><section>

<h1>Appendix: Method Details

#<h1>Method 1: DeepSeek API Configuration
```python
CONFIG = {
    'model': 'deepseek-chat',
    'base_url': 'https://api.deepseek.com/v1',
    'max_tokens': 2000,
    'temperature': 0.1,
    'cost_per_1m_tokens': 0.14
}
```

#<h1>Method 2: Qwen Local Setup
- **Model**: Qwen/Qwen2.5-1.5B-Instruct
- **Quantization**: 8-bit with float16 compute
- **Memory**: ~3GB model size when quantized
- **Batching**: Up to 32 posters on RTX 4090

        </section>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: false,
            transition: 'slide'
        });
    </script>
</body>
</html>