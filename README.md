# Scientific Poster Metadata Extraction

A comprehensive toolkit for extracting structured metadata from scientific poster PDFs using three distinct approaches, each optimized for different use cases and accuracy requirements.

## Important Note on Accuracy

**All accuracy estimates are unvalidated** - these are rough estimates based on limited testing and theoretical benchmarks. Actual accuracy can only be determined through proper validation using Cochran's random sampling methodology as outlined in this document. Please validate before production use.

## Three-Method Approach

### Method 1: DeepSeek API Extraction
**Notebooks**: [`01_method1_deepseek_api.ipynb`](notebooks/01_method1_deepseek_api.ipynb)

Cost-effective API-based extraction using DeepSeek's language model.

**Performance Characteristics:**
- **Estimated Accuracy**: 85-90% (unvalidated - requires Cochran sampling validation)
- **Cost**: ~$0.003 per poster (200x cheaper than GPT-4)
- **Speed**: 5-15 seconds per poster  
- **Hallucination Risk**: Low-Medium (mitigated by structured prompts)
- **Setup**: Easy - just requires API key

**Best For**: Production systems with budget constraints, high-volume processing

### Method 2: Qwen Local Extraction
**Notebooks**: [`02_method2_qwen_local.ipynb`](notebooks/02_method2_qwen_local.ipynb)

Local small language model (1.5B parameters) for privacy-sensitive environments.

**Performance Characteristics:**
- **Estimated Accuracy**: 80-85% (unvalidated - requires Cochran sampling validation)
- **Cost**: $0 (runs locally, only electricity costs)
- **Speed**: 10-40 seconds per poster (single), ~1.1s per poster (RTX 4090 batched)
- **Hallucination Risk**: Low (structured prompting)
- **Setup**: Medium - requires model download and GPU memory

**RTX 4090 Batching Capacity:**
- **Recommended batch size**: 32 posters simultaneously
- **Throughput**: ~3,273 posters/hour, ~26,182 posters/day (8hrs)
- **Memory efficiency**: 8-bit quantization enables large-scale processing

**Best For**: Privacy-sensitive environments, budget-conscious deployments, edge computing

### Method 3: BioELECTRA+CRF (DEMO)
**Notebooks**: [`03_method3_bioelectra_crf_demo.ipynb`](notebooks/03_method3_bioelectra_crf_demo.ipynb)

**DEMONSTRATION ONLY** - Future possibility requiring 500-1000 labeled posters for training.

**Performance Characteristics (Estimated):**
- **Estimated Accuracy**: 85-92% (theoretical - based on BLURB benchmarks, requires training & validation)
- **Cost**: $0 (after training - local inference only)  
- **Speed**: <0.5 seconds per poster (fastest of all methods)
- **Hallucination Risk**: 0% (deterministic sequence labeling)
- **Setup**: Complex - requires extensive training data

**Training Requirements**: 500-1000 manually labeled poster PDFs with BIO annotations

**Auto-Labeling Plan**: The training data for Method 3 will be generated by auto-labeling 1,000 posters (or however many needed) using our top-performing Methods 1 (DeepSeek) and 2 (Qwen) to bootstrap the CRF training dataset. This approach leverages LLM-generated annotations as weak supervision for the final deterministic model.

## Approach Comparison

| Feature | Method 1 (DeepSeek) | Method 2 (Qwen Local) | Method 3 (BioELECTRA) |
|---------|--------------------|-----------------------|----------------------|
| **Accuracy** | 85-90% (unvalidated) | 80-85% (unvalidated) | 85-92% (theoretical) |
| **Cost per poster** | $0.003 | $0 | $0 |
| **Speed** | 5-15s | 10-40s | <0.5s |
| **Privacy** | External API | Local | Local |
| **Setup complexity** | Easy | Medium | Complex |
| **Hallucination** | Low-Med | Low | None |
| **Training required** | No | No | Yes (500-1000 posters) |

## Quality Validation Framework

### Cochran's Sampling for Manual Validation

We strongly recommend validating extraction quality using **Cochran's formula** for statistically significant sample size determination:

```
n = (Z² × p × (1-p)) / e²
n_adjusted = n / (1 + (n-1)/N)  # Finite population correction
```

**Where:**
- **Z** = 1.96 (95% confidence level)
- **p** = 0.5 (maximum variability assumption)
- **e** = 0.05 (±5% margin of error)
- **N** = total population size (number of posters)

**Sample Sizes by Dataset (with finite population correction):**
- **100 posters**: Validate ~80 randomly selected outputs (79.5%)
- **500 posters**: Validate ~217 randomly selected outputs (43.5%)
- **1000 posters**: Validate ~278 randomly selected outputs (27.8%)
- **10,000 posters**: Validate ~370 randomly selected outputs (3.7%)
- **100,000+ posters**: Validate ~383 randomly selected outputs (0.4%)

**Key Insight**: For smaller datasets (<1000), you must validate a high percentage. Only when scaling to tens of thousands of posters does the required validation percentage become practical (under 5%).

**Validation Process:**
1. Extract metadata from full dataset
2. Randomly sample using calculated sample size
3. Expert manual review of sampled outputs
4. Calculate accuracy metrics (precision, recall, F1)
5. Apply correction factors to full dataset if needed

This ensures statistically significant quality assessment across all methods.

### Accuracy Measurement Guidelines

**Field-specific accuracy calculation:**
- **Title**: Exact match or semantic similarity >0.8
- **Authors**: Name matching with fuzzy string matching (edit distance <2)
- **Keywords**: Overlap coefficient >0.6 with expert annotations
- **Methods/Results**: BLEU score >0.7 compared to expert summaries

## Project Structure

```
poster_project/
├── notebooks/
│   ├── 01_method1_deepseek_api.ipynb      # DeepSeek API extraction
│   ├── 02_method2_qwen_local.ipynb        # Qwen local model
│   └── 03_method3_bioelectra_crf_demo.ipynb # BioELECTRA demo
├── scripts/
│   ├── method1_deepseek_api.py            # API extraction script
│   ├── method2_qwen_local.py             # Local model script
│   └── method3_bioelectra_crf_demo.py    # Demo script
├── output/                               # Extraction results
├── src/                                 # Reusable modules
├── test-poster.pdf                      # Sample poster for testing
├── requirements.txt                     # Dependencies
└── README.md                           # This file
```

## Installation & Setup

### 1. Environment Setup
```bash
git clone https://github.com/jimnoneill/poster-metadata-extractor.git
cd poster-metadata-extractor
pip install -r requirements.txt
```

### 2. API Configuration (Method 1)
```bash
cp env.example .env
# Edit .env and add your DEEPSEEK_API_KEY
```

### 3. GPU Setup (Method 2)
For optimal performance with Qwen local model:
- CUDA-capable GPU with 8GB+ VRAM
- PyTorch with CUDA support

## Usage

### Quick Start
```python
# Method 1: DeepSeek API
from scripts.method1_deepseek_api import extract_poster_metadata
results = extract_poster_metadata("your-poster.pdf")

# Method 2: Qwen Local  
from scripts.method2_qwen_local import extract_poster_metadata_qwen
results = extract_poster_metadata_qwen("your-poster.pdf")

# Method 3: Demo only
from scripts.method3_bioelectra_crf_demo import bioelectra_crf_demo
demo_results = bioelectra_crf_demo()
```

### Notebook Execution
All notebooks are ready to run with pre-executed outputs:
1. Open desired method notebook in Jupyter
2. Set API keys if using Method 1
3. Run all cells to see extraction results

## Key Technologies by Method

### Method 1 (DeepSeek API)
- **DeepSeek Chat**: Cost-effective LLM with competitive performance
- **OpenAI-compatible API**: Easy integration
- **Structured prompting**: JSON schema enforcement
- **PyMuPDF**: PDF text extraction

### Method 2 (Qwen Local)
- **Qwen2.5-1.5B-Instruct**: Compact multilingual model
- **Transformers**: HuggingFace model loading
- **BitsAndBytes**: 8-bit quantization for memory efficiency
- **Few-shot prompting**: Task-specific extraction

### Method 3 (BioELECTRA Demo)
- **BioELECTRA**: Biomedical domain-optimized transformer (2nd on BLURB)
- **Conditional Random Fields**: Sequence labeling for entity extraction
- **BIO tagging**: Named entity recognition scheme
- **PyTorch CRF**: Implementation of CRF layer

## Recommendations

### For Production Use
1. **Start with Method 1** (DeepSeek API) for immediate deployment
2. **Implement Cochran sampling** for quality validation  
3. **Consider Method 2** for privacy-sensitive applications
4. **Plan Method 3** as long-term solution with proper training data

### For BioELECTRA Training (Method 3)
- **Collect 500-1000 poster PDFs** with diverse layouts and fields
- **Manual BIO annotation** (~40-60 expert hours)  
- **Entity types**: Title, Authors, Affiliations, Methods, Results, Funding
- **Alternative simpler approaches**: Rule-based NER, spaCy custom models
- **Validation**: Cross-validation on held-out test set

## License
MIT License - see LICENSE file for details.

## Contributing
1. Fork the repository
2. Create feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -am 'Add improvement'`)
4. Push to branch (`git push origin feature/improvement`)
5. Create Pull Request

## Citation
```bibtex
@software{oneill2024poster,
  title={Scientific Poster Metadata Extraction Toolkit},
  author={O'Neill, Jim},
  year={2024},
  url={https://github.com/jimnoneill/poster-metadata-extractor}
}
```