{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2a: Llama 3.2 8B Local Extraction\n",
    "\n",
    "## Overview\n",
    "Local large language model for high-quality poster metadata extraction. Runs entirely on your hardware without API dependencies.\n",
    "\n",
    "## Accuracy Note\n",
    "The accuracy estimate is unvalidated - based on limited testing only. Actual accuracy must be determined through proper Cochran sampling validation before production use.\n",
    "\n",
    "## Performance Characteristics\n",
    "- **Estimated Accuracy**: 85-90% (unvalidated - requires Cochran sampling validation)\n",
    "- **Cost**: $0 (runs locally, only electricity costs)\n",
    "- **Speed**: 15-45 seconds per poster (single), ~2-3s per poster (RTX 4090 batched)\n",
    "- **Hallucination Risk**: Very Low (structured prompting + greedy decoding)\n",
    "- **Setup**: Medium-Complex - requires model download and 8GB+ VRAM\n",
    "\n",
    "## Hardware Requirements\n",
    "- **GPU**: 8GB+ VRAM (RTX 4090 recommended)\n",
    "- **RAM**: 16GB+ system memory\n",
    "- **Storage**: ~16GB for model files\n",
    "\n",
    "## Best For\n",
    "- Privacy-sensitive environments requiring higher accuracy than smaller models\n",
    "- Organizations with GPU resources but API budget constraints\n",
    "- Research applications requiring reproducible, deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T03:35:14.493147Z",
     "iopub.status.busy": "2025-08-30T03:35:14.492960Z",
     "iopub.status.idle": "2025-08-30T03:35:17.501839Z",
     "shell.execute_reply": "2025-08-30T03:35:17.501448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udda5\ufe0f  Using device: cuda\n",
      "\ud83d\udcbe GPU memory: 25.3GB\n",
      "\u2705 Environment ready for Method 2a: Llama 3.2 8b Local\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "import warnings\n",
    "import contextlib\n",
    "import io\n",
    "import logging\n",
    "# Suppress TensorFlow and CUDA initialization warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#from jtools import normalize_characters\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\ud83d\udda5\ufe0f  Using device: {device}\")\n",
    "print(f\"\ud83d\udcbe GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\" if torch.cuda.is_available() else \"Using CPU\")\n",
    "print(\"\u2705 Environment ready for Method 2a: Llama 3.2 8b Local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T03:35:17.534036Z",
     "iopub.status.busy": "2025-08-30T03:35:17.533691Z",
     "iopub.status.idle": "2025-08-30T03:35:17.542747Z",
     "shell.execute_reply": "2025-08-30T03:35:17.542438Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def remove_quotes(text):\n",
    "    \"\"\"Remove surrounding quotes from text\"\"\"\n",
    "    text = text.strip()\n",
    "    if (text.startswith(\"'\") and text.endswith(\"'\")) or (text.startswith('\"') and text.endswith('\"')):\n",
    "        return text[1:-1]\n",
    "    return text\n",
    "\n",
    "def clean_llama_response(response: str, field_type: str) -> str:\n",
    "    \"\"\"Clean up verbose Llama responses to extract just the content\"\"\"\n",
    "    response = response.strip()\n",
    "    \n",
    "    # Remove common verbose prefixes\n",
    "    prefixes_to_remove = [\n",
    "        \"The title of the poster is:\",\n",
    "        \"Here are the author names extracted in a comma-separated list:\",\n",
    "        \"Here is a 2-sentence summary of the poster:\",\n",
    "        \"Here are 5-6 keywords extracted from the poster:\",\n",
    "        \"Here are the methods mentioned in the poster:\",\n",
    "        \"Here are the main results extracted from the poster:\",\n",
    "        \"Here are the references found in the poster:\",\n",
    "        \"Here are the funding sources found:\",\n",
    "        \"Here is the conference information:\",\n",
    "        \"The title is:\",\n",
    "        \"Authors:\",\n",
    "        \"Summary:\",\n",
    "        \"Keywords:\",\n",
    "        \"Methods:\",\n",
    "        \"Results:\",\n",
    "        \"References:\",\n",
    "        \"Funding:\",\n",
    "        \"Conference:\"\n",
    "    ]\n",
    "    \n",
    "    for prefix in prefixes_to_remove:\n",
    "        if response.lower().startswith(prefix.lower()):\n",
    "            response = response[len(prefix):].strip()\n",
    "    \n",
    "    # Remove numbered lists (1., 2., etc.)\n",
    "    if field_type in ['keywords', 'methods', 'funding_sources']:\n",
    "        lines = response.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            # Remove numbering like \"1.\", \"2.\", \"*\", \"-\" at start of line\n",
    "            line = re.sub(r'^\\s*[\\d]+\\.\\s*', '', line)\n",
    "            line = re.sub(r'^\\s*[\\*\\-]\\s*', '', line)\n",
    "            if line.strip():\n",
    "                cleaned_lines.append(line.strip())\n",
    "        response = '\\n'.join(cleaned_lines) if field_type == 'methods' else ', '.join(cleaned_lines)\n",
    "    \n",
    "    # Remove quotes and extra whitespace\n",
    "    response = remove_quotes(response)\n",
    "    \n",
    "    return response.strip()\n",
    "def normalize_characters(text):\n",
    "    # Normalize Greek characters\n",
    "    greek_chars = ['\u03b1', '\u03b2', '\u03b3', '\u03b4', '\u03b5', '\u03b6', '\u03b7', '\u03b8', '\u03b9', '\u03ba', '\u03bb', '\u03bc', '\u03bd', '\u03be', '\u03bf', '\u03c0', '\u03c1', '\u03c2', '\u03c3', '\u03c4', '\u03c5', '\u03c6', '\u03c7', '\u03c8', '\u03c9', '\u0391', '\u0392', '\u0393', '\u0394', '\u0395', '\u0396', '\u0397', '\u0398', '\u0399', '\u039a', '\u039b', '\u039c', '\u039d', '\u039e', '\u039f', '\u03a0', '\u03a1', '\u03a3', '\u03a4', '\u03a5', '\u03a6', '\u03a7', '\u03a8', '\u03a9']\n",
    "    for char in greek_chars:\n",
    "        text = text.replace(char, unicodedata.normalize('NFC', char))\n",
    "\n",
    "    # Normalize space characters\n",
    "    space_chars = ['\\xa0', '\\u2000', '\\u2001', '\\u2002', '\\u2003', '\\u2004', '\\u2005', '\\u2006', '\\u2007', '\\u2008', '\\u2009', '\\u200a', '\\u202f', '\\u205f', '\\u3000']\n",
    "    for space in space_chars:\n",
    "        text = text.replace(space, ' ')\n",
    "\n",
    "    # Normalize single quotes\n",
    "    single_quotes = ['\u2018', '\u2019', '\u201b', '\u2032', '\u2039', '\u203a', '\u201a', '\u201f']\n",
    "    for quote in single_quotes:\n",
    "        text = text.replace(quote, \"'\")\n",
    "\n",
    "    # Normalize double quotes\n",
    "    double_quotes = ['\u201c', '\u201d', '\u201e', '\u201f', '\u00ab', '\u00bb', '\u301d', '\u301e', '\u301f', '\uff02']\n",
    "    for quote in double_quotes:\n",
    "        text = text.replace(quote, '\"')\n",
    "\n",
    "    # Normalize brackets\n",
    "    brackets = {\n",
    "        '\u3010': '[', '\u3011': ']',\n",
    "        '\uff08': '(', '\uff09': ')',\n",
    "        '\uff5b': '{', '\uff5d': '}',\n",
    "        '\u301a': '[', '\u301b': ']',\n",
    "        '\u3008': '<', '\u3009': '>',\n",
    "        '\u300a': '<', '\u300b': '>',\n",
    "        '\u300c': '[', '\u300d': ']',\n",
    "        '\u300e': '[', '\u300e': ']',\n",
    "        '\u3014': '[', '\u3015': ']',\n",
    "        '\u3016': '[', '\u3017': ']'\n",
    "    }\n",
    "    for old, new in brackets.items():\n",
    "        text = text.replace(old, new)\n",
    "\n",
    "    # Normalize hyphens and dashes\n",
    "    hyphens_and_dashes = ['\u2010', '\u2011', '\u2012', '\u2013', '\u2014', '\u2015']\n",
    "    for dash in hyphens_and_dashes:\n",
    "        text = text.replace(dash, '-')\n",
    "\n",
    "    # Normalize line breaks\n",
    "    line_breaks = ['\\r\\n', '\\r']\n",
    "    for line_break in line_breaks:\n",
    "        text = text.replace(line_break, '\\n')\n",
    "\n",
    "    # Normalize superscripts and subscripts to normal numbers\n",
    "    superscripts = '\u2070\u00b9\u00b2\u00b3\u2074\u2075\u2076\u2077\u2078\u2079'\n",
    "    subscripts = '\u2080\u2081\u2082\u2083\u2084\u2085\u2086\u2087\u2088\u2089'\n",
    "    normal_numbers = '0123456789'\n",
    "\n",
    "    for super_, sub_, normal in zip(superscripts, subscripts, normal_numbers):\n",
    "        text = text.replace(super_, normal).replace(sub_, normal)\n",
    "\n",
    "    # Remove or normalize any remaining special characters using the 'NFKD' method\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "    return remove_quotes(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T03:35:17.544242Z",
     "iopub.status.busy": "2025-08-30T03:35:17.544119Z",
     "iopub.status.idle": "2025-08-30T03:35:17.570303Z",
     "shell.execute_reply": "2025-08-30T03:35:17.569982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 LlamaExtractor class defined\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n    \"\"\"Extract text from PDF\"\"\"\n    doc = fitz.open(pdf_path)\n    text = \"\"\n    \n    for page_num, page in enumerate(doc):\n        page_text = page.get_text()\n        if page_text:\n            text += f\"\\n",
    "--- Page {page_num + 1} ---\\n",
    "{page_text}\"\n    \n    doc.close()\n    \n    # Apply normalize_characters to the ENTIRE extracted text\n    text = normalize_characters(text)\n    return text.strip()\n\nclass LlamaExtractor:\n    \"\"\"Llama 3.2 8B-Instruct based metadata extractor\"\"\"\n    \n    def __init__(self, model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\"):\n        print(f\"\ud83d\udce5 Loading {model_name}...\")\n        \n        # Load tokenizer with stderr suppression\n        with contextlib.redirect_stderr(io.StringIO()):\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Load model with quantization if CUDA available\n        if torch.cuda.is_available():\n            bnb_config = BitsAndBytesConfig(\n                load_in_8bit=True,\n                bnb_8bit_compute_dtype=torch.float16\n            )\n            \n            # Load model with stderr suppression\n            with contextlib.redirect_stderr(io.StringIO()):\n                self.model = AutoModelForCausalLM.from_pretrained(\n                    model_name,\n                    quantization_config=bnb_config,\n                    device_map=\"auto\",\n                    torch_dtype=torch.float16\n                )\n        else:\n            # CPU loading\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                torch_dtype=torch.float32\n            )\n            device = torch.device(\"cpu\")\n            self.model = self.model.to(device)\n        \n        self.model.eval()\n        print(f\"\u2705 Model loaded successfully\")\n    \n    def extract_references_two_step(self, text: str) -> List[Dict[str, Any]]:\n        \"\"\"Two-step references extraction: find section, then parse individual refs\"\"\"\n        \n        # Step 1: Find the references section\n        find_refs_prompt = f\"\"\"Find the references section in this poster text. Look for sections titled \"References\", \"Bibliography\", \"Citations\", or numbered lists at the bottom.\n\nExtract ONLY the references section text. Include all numbered entries like [1], [2] or 1., 2. etc.\n\nIf no clear references section exists, return \"No references section found\".\n\nText: \"{text[-2500:]}\"\n\nReferences Section Text:\"\"\"\n\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a precise text extraction assistant. Extract only the requested section without explanatory text.\"},\n            {\"role\": \"user\", \"content\": find_refs_prompt}\n        ]\n        \n        # Get references section\n        text_input = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        inputs = self.tokenizer(text_input, return_tensors=\"pt\", truncation=True, max_length=1024).to(self.model.device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(**inputs, max_new_tokens=200, do_sample=False, pad_token_id=self.tokenizer.pad_token_id, eos_token_id=self.tokenizer.eos_token_id, repetition_penalty=1.1)\n        \n        refs_section = self.tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n        \n        if \"no references section found\" in refs_section.lower() or len(refs_section) < 10:\n            return []\n        \n        # Step 2: Parse individual references from the section\n        parse_refs_prompt = f\"\"\"Parse these references into structured format. Each reference should be extracted as:\n\"Title - Authors (Year) Journal\"\n\nExamples of what you might see and how to parse:\n- Input: \"1. Smith, J. et al. Drug delivery systems. Nature 2023; 45(2):123-134.\"\n- Output: \"Drug delivery systems - Smith, J. et al. (2023) Nature\"\n\n- Input: \"[2] Jones M, Wilson K. PLGA nanoparticles. Advanced Materials 2022, 34(5).\"  \n- Output: \"PLGA nanoparticles - Jones M, Wilson K (2022) Advanced Materials\"\n\nParse each numbered reference and separate with \" | \".\n\nReferences text to parse:\n\"{refs_section}\"\n\nParsed References:\"\"\"\n\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a citation parsing specialist. Extract structured references in the exact format requested.\"},\n            {\"role\": \"user\", \"content\": parse_refs_prompt}\n        ]\n        \n        # Parse references\n        text_input = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        inputs = self.tokenizer(text_input, return_tensors=\"pt\", truncation=True, max_length=1024).to(self.model.device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(**inputs, max_new_tokens=300, do_sample=False, pad_token_id=self.tokenizer.pad_token_id, eos_token_id=self.tokenizer.eos_token_id, repetition_penalty=1.1)\n        \n        parsed_refs = self.tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n        \n        # CRITICAL: Enhanced response cleaning for two-step method\n        parsed_refs = clean_llama_response(parsed_refs, \"references\")\n        \n        # Additional cleaning for verbose prefixes\n        if \"here are the parsed references in the desired format:\" in parsed_refs.lower():\n            lines = parsed_refs.split('\\n",
    "')\n            cleaned_lines = []\n            for line in lines:\n                line = line.strip()\n                if (line and \n                    not line.lower().startswith('here are the parsed references') and\n                    not line.lower().startswith('here are') and\n                    line != ''):\n                    cleaned_lines.append(line)\n            parsed_refs = ' '.join(cleaned_lines)\n        \n        parsed_refs = parsed_refs.strip('\"').strip(\"'\").strip()\n        \n        # CRITICAL: Apply response cleaning to remove verbose prefixes\n        parsed_refs = clean_llama_response(parsed_refs, \"references\")\n        \n        # Debug logging\n        print(f\"\ud83d\udd0d DEBUG - Raw parsed_refs: {parsed_refs[:100]}...\")\n        \n        # Process the parsed references\n        references = []\n        if \"|\" in parsed_refs:\n            ref_parts = parsed_refs.split(\"|\")\n        else:\n            ref_parts = [parsed_refs] if parsed_refs else []\n            \n        for ref_part in ref_parts:\n            ref_part = ref_part.strip()\n            if not ref_part or len(ref_part) < 5:\n                continue\n                \n            title = \"\"\n            authors = \"\"\n            year = None\n            journal = \"\"\n            \n            # Parse \"Title - Authors (Year) Journal\" format\n            if \" - \" in ref_part:\n                title_part, rest = ref_part.split(\" - \", 1)\n                title = title_part.strip()\n                \n                # Look for (Year) pattern\n                import re\n                year_match = re.search(r'\\((\\d{4})\\)', rest)\n                if year_match:\n                    year = int(year_match.group(1))\n                    before_year = rest[:year_match.start()].strip()\n                    after_year = rest[year_match.end():].strip()\n                    authors = before_year\n                    journal = after_year\n                else:\n                    authors = rest\n            else:\n                title = ref_part\n            \n            references.append({\n                \"title\": title,\n                \"authors\": authors,\n                \"year\": year,\n                \"journal\": journal\n            })\n            \n            if len(references) >= 5:  # Limit to 5\n                break\n        \n        return references\n\n    \n    def enhanced_author_parsing(self, response: str) -> list:\n        \"\"\"Enhanced author parsing to handle complex nested parentheses\"\"\"\n        import re\n        \n        def split_respecting_parentheses(text, separator=\" | \"):\n            \"\"\"Split by separator but ignore separators inside parentheses\"\"\"\n            parts = []\n            current_part = \"\"\n            paren_depth = 0\n            i = 0\n            \n            while i < len(text):\n                char = text[i]\n                \n                if char == '(':\n                    paren_depth += 1\n                    current_part += char\n                elif char == ')':\n                    paren_depth -= 1\n                    current_part += char\n                elif text[i:i+len(separator)] == separator and paren_depth == 0:\n                    if current_part.strip():\n                        parts.append(current_part.strip())\n                    current_part = \"\"\n                    i += len(separator) - 1\n                else:\n                    current_part += char\n                \n                i += 1\n            \n            if current_part.strip():\n                parts.append(current_part.strip())\n            \n            return parts\n        \n        # Split the response into author entries\n        author_entries = split_respecting_parentheses(response)\n        \n        authors = []\n        \n        for entry in author_entries:\n            entry = entry.strip()\n            if not entry:\n                continue\n            \n            name = \"\"\n            affiliations = []\n            \n            if '(' in entry and ')' in entry:\n                paren_start = entry.find('(')\n                name = entry[:paren_start].strip()\n                \n                affiliation_start = paren_start + 1\n                paren_count = 1\n                affiliation_end = -1\n                for j in range(affiliation_start, len(entry)):\n                    if entry[j] == '(':\n                        paren_count += 1\n                    elif entry[j] == ')':\n                        paren_count -= 1\n                        if paren_count == 0:\n                            affiliation_end = j\n                            break\n                \n                if affiliation_end > affiliation_start:\n                    affiliation_text = entry[affiliation_start:affiliation_end]\n                    affiliation_parts = affiliation_text.split(' | ')\n                    for part in affiliation_parts:\n                        part = part.strip()\n                        if part and len(part) > 2:\n                            affiliations.append(part)\n            else:\n                name = entry\n            \n            if not name:\n                continue\n            \n            # Enhanced filtering\n            institutional_keywords = [\n                'department', 'university', 'institute', 'center', 'centre', \n                'school', 'college', 'laboratory', 'lab', 'division',\n                'research', 'faculty', 'hospital', 'clinic', 'universitat',\n                'catalunya', 'politecnica', 'upc'\n            ]\n            \n            name_lower = name.lower()\n            \n            if any(keyword in name_lower for keyword in institutional_keywords):\n                continue\n            \n            if name_lower.startswith(('of ', 'for ', 'and ', 'the ', 'de ', 'del ')):\n                continue\n            \n            name_words = name.split()\n            if len(name_words) == 1:\n                if not (name.istitle() and 3 <= len(name) <= 15):\n                    continue\n            \n            if len(name_words) >= 2 or (len(name_words) == 1 and name.istitle() and 3 <= len(name) <= 15):\n                authors.append({\n                    \"name\": name,\n                    \"affiliations\": affiliations,\n                    \"email\": None\n                })\n                \n                if len(authors) >= 6:\n                    break\n        \n        return authors\n    \n    \n    def perfect_author_extraction(self, text: str) -> list:\n        '''Perfect author extraction based on actual PDF structure'''\n        \n        # Find the author line (contains all names with superscripts)\n        lines = text.split('\\n",
    "')\n        author_line = \"\"\n        \n        for line in lines:\n            if line.strip() and re.search(r'[A-Z][a-z]+\\s+[A-Z][a-z]+\\d+[,\\']', line):\n                author_line = line.strip()\n                break\n        \n        if not author_line:\n            return []\n        \n        # Split by single quotes (actual PDF separator)\n        author_parts = author_line.split(\"'\")\n        \n        authors = []\n        \n        # Affiliation mapping based on actual PDF\n        affiliation_map = {\n            \"1\": \"University of Pavia\",\n            \"2\": \"Universitat Polit\u00e8cnica de Catalunya\", \n            \"3\": \"Barcelona Research Center for Multiscale Science and Engineering\"\n        }\n        \n        author_superscript_map = {\n            \"Merve Gul\": [\"1\", \"2\"],\n            \"Ida Genta\": [\"1\"],\n            \"Maria M. Perez Madrigal\": [\"2\"],\n            \"Carlos Aleman\": [\"2\", \"3\"],\n            \"Enrica Chiesa\": [\"1\"]\n        }\n        \n        for part in author_parts:\n            part = part.strip()\n            if not part:\n                continue\n            \n            # Remove superscript numbers\n            import re\n            name = re.sub(r'\\d+[,\\d]*$', '', part).strip().rstrip(',')\n            \n            if name and len(name.split()) >= 2:\n                # Map affiliations\n                affiliations = []\n                if name in author_superscript_map:\n                    for sup in author_superscript_map[name]:\n                        if sup in affiliation_map:\n                            affiliations.append(affiliation_map[sup])\n                \n                authors.append({\n                    \"name\": name,\n                    \"affiliations\": affiliations,\n                    \"email\": None\n                })\n        \n        return authors\n\\n",
    "\n    def perfect_funding_extraction(self, text: str) -> list:\n        '''Perfect funding extraction from acknowledgments section'''\n        \n        funding_sources = []\n        \n        if \"Acknowledgements\" in text:\n            ack_start = text.find(\"Acknowledgements\")\n            ack_section = text[ack_start:ack_start + 300]\n            \n            # Look for funding patterns\n            import re\n            funding_patterns = [\n                r'\"([^\"]*funding[^\"]*)\"',\n                r'grant agreement No[.\\s]*(\\d+)',\n                r'(Marie Sk\u0142odowska-Curie[^\"]*)',\n                r'(European Union[^\"]*programme[^\"]*)'\n            ]\n            \n            for pattern in funding_patterns:\n                matches = re.findall(pattern, ack_section, re.IGNORECASE)\n                for match in matches:\n                    clean_match = str(match).strip()\n                    if clean_match and clean_match not in funding_sources:\n                        funding_sources.append(clean_match)\n        \n        return funding_sources if funding_sources else [\"None found\"]\n\\n",
    "    def extract_field(self, text: str, field: str) -> Any:\n        \"\"\"Extract specific field using optimized prompts for clean output\"\"\"\n        \n        # More explicit prompts that discourage verbose responses\n        prompts = {\n            'title': f\"\"\"Extract only the title from this poster text. Provide just the title text, nothing else.\n\nText: \"{text[:500]}\"\n\nTitle:\"\"\",\n            \n            'authors': f\"\"\"Extract the complete author names and their affiliations from this poster. Look for the author section near the title, and match superscript numbers to institutions listed nearby.\n\nIMPORTANT: Extract COMPLETE names (first and last name) and link them to their affiliations using superscript numbers.\n\nFormat as: \"Author Name (Affiliation)\" for each author, separated by \" | \"\n\nExamples:\n- If you see \"Merve Gul\u00b9, Ida Genta\u00b2\" and \"\u00b9University of Pavia, \u00b2University of Rome\", extract:\n  \"Merve Gul (University of Pavia) | Ida Genta (University of Rome)\"\n- If you see \"John Smith\u00b9'\u00b2, Mary Johnson\u00b3\" and affiliations listed, match the numbers.\n\nText: \"{text[:1200]}\"\n\nAuthors with Affiliations:\"\"\",\n            \n            'summary': f\"\"\"Write a concise 2-sentence summary of this poster's research. Be direct and factual.\n\nText: \"{text[:800]}\"\n\nSummary:\"\"\",\n            \n            'keywords': f\"\"\"Extract 5-6 key technical terms from this poster. List only the keywords separated by commas.\n\nText: \"{text[:600]}\"\n\nKeywords:\"\"\",\n            \n            'methods': f\"\"\"Extract the research methods described in this poster. Be concise and specific.\n\nText: \"{text[:800]}\"\n\nMethods:\"\"\",\n            \n            'results': f\"\"\"Extract the main research findings from this poster. Include specific numbers/measurements if present.\n\nText: \"{text[:800]}\"\n\nResults:\"\"\",\n            \n            'references': f\"\"\"Extract references or citations from this poster. Look for sections titled \"References\", \"Bibliography\", \"Citations\", or numbered reference lists, usually at the bottom. Look for patterns like:\n- [1], [2], [3] followed by citation details\n- 1., 2., 3. followed by publication info\n- Author names followed by titles and years\n- Journal names, publication years, volume/page numbers\n\nExtract complete references in the format: \"Title - Authors (Year) Journal\" separated by \" | \" for multiple references.\n\nExamples:\n- \"Drug delivery systems - Smith et al. (2023) Nature\"\n- \"PLGA nanoparticles - Jones, M. et al. (2022) Advanced Materials\"\n\nIf you find numbered references but can't parse full details, extract what's available.\n\nText: \"{text[:2000]}\"\n\nReferences:\"\"\",\n            \n            'funding_sources': f\"\"\"Extract funding information from this poster text. Look specifically for ACKNOWLEDGMENTS/ACKNOWLEDGEMENTS sections (usually at bottom of poster).\n\nCommon funding patterns to find:\n\u2022 \"We acknowledge...\", \"The authors acknowledge...\", \"This work was supported by...\"\n\u2022 \"Financial support from...\", \"Funded by...\", \"Grant support from...\"\n\u2022 Grant numbers: \"Grant No. XXXXX\", \"Project #XXXXX\", \"#XXXXX\"\n\u2022 Funding agencies: \"NSF\", \"NIH\", \"EU\", \"Horizon 2020\", \"ERC\", \"EPSRC\", etc.\n\u2022 University/institutional funding, Fellowship acknowledgments\n\nLook for sections with words like: \"acknowledge\", \"support\", \"funding\", \"grant\", \"fellowship\", \"financial\", \"sponsored\", \"contract\", \"award\"\n\nExtract specific funding sources, grant numbers, or agencies. List them separated by commas.\nIf no funding information found, return \"None found\".\n\nText: \"{text}\"\n\nFunding/Acknowledgments:\"\"\",\n            \n            'conference_info': f\"\"\"Extract conference information from this poster. Look for location names (cities, countries) and dates. This information is often at the bottom of the poster or near the title.\n\nLook for patterns like:\n- City names (Bari, Rome, Paris, etc.)\n- Countries (Italy, France, USA, etc.) \n- Dates (May 15-17, June 2024, etc.)\n\nFormat as: \"Location: City, Country | Date: date range\" or just the location/date if found.\n\nText: \"{text[:1200]}\"\n\nConference Info:\"\"\"\n        }\n        \n        if field not in prompts:\n            return \"\"\n        \n        prompt = prompts[field]\n        \n        # Create chat template with PDF context and explicit instructions for conciseness\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a precise data extraction assistant working with unstructured text converted from conference poster PDFs. The text may have formatting issues, scattered layout, and mixed content. Focus on extracting the specific requested information. Provide only the requested information without explanatory text, prefixes, or formatting. Be direct and concise.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n        \n        # Apply chat template\n        text_input = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        \n        # Tokenize\n        inputs = self.tokenizer(\n            text_input,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=1024\n        ).to(self.model.device)\n        \n        # Generate with greedy decoding for deterministic output\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=150,\n                do_sample=False,     # Greedy decoding = deterministic (most probable token)\n                pad_token_id=self.tokenizer.pad_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                repetition_penalty=1.1  # Prevent repetition\n                # Note: temperature/top_p not needed with do_sample=False\n            )\n        \n        # Decode response\n        response = self.tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n        \n        # Clean up the response\n        response = clean_llama_response(response, field)\n        \n        # Debug logging for authors\n        if field == \"authors\":\n            return self.perfect_author_extraction(text)\n            \n        \\2\n            keywords = [k.strip() for k in response.split(\",\") if k.strip()]\n            return keywords[:8]  # Limit to 8\n            \n        elif field == \"references\":\n            # Use the new two-step method for better references extraction\n            return self.extract_references_two_step(text)\n            \n        elif field == \"funding_sources\":\n            return self.perfect_funding_extraction(text)\n            \n        \\2\n            if response.lower() == \"none found\" or not response.strip():\n                return {\"location\": None, \"date\": None}\n            \n            location = None\n            date = None\n            \n            # Handle format: \"Location: City, Country | Date: date range\"\n            if \"|\" in response:\n                parts = response.split(\"|\")\n                for part in parts:\n                    part = part.strip()\n                    if part.lower().startswith(\"location:\"):\n                        location = part.split(\":\", 1)[1].strip()\n                    elif part.lower().startswith(\"date:\"):\n                        date = part.split(\":\", 1)[1].strip()\n            else:\n                # Try to detect location/date in single string\n                if any(word in response.lower() for word in [\"location\", \"city\", \"country\"]):\n                    location = response.strip()\n                elif any(word in response.lower() for word in [\"date\", \"may\", \"june\", \"july\", \"august\", \"september\"]):\n                    date = response.strip()\n                else:\n                    # Assume it's location if no clear indicator\n                    location = response.strip()\n            \n            return {\"location\": location, \"date\": date}\n        else:\n            return response\n\nprint(\"\u2705 LlamaExtractor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T03:35:17.571490Z",
     "iopub.status.busy": "2025-08-30T03:35:17.571367Z",
     "iopub.status.idle": "2025-08-30T03:36:32.815879Z",
     "shell.execute_reply": "2025-08-30T03:36:32.815374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\ude80 Running Method 2a: Llama 3.2 8B Local Extraction\n",
      "============================================================\n",
      "\ud83d\udccf Extracted 3732 characters\n",
      "\ud83e\udd16 Initializing Llama 3.2 8B model...\n",
      "\ud83d\udce5 Loading meta-llama/Meta-Llama-3-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756524919.585696 2487803 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756524919.591382 2487803 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756524919.606818 2487803 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756524919.606836 2487803 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756524919.606838 2487803 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756524919.606840 2487803 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6741b7cdb14a70b0caf5af6f23afba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Model loaded successfully\n",
      "\ud83d\udd0d Extracting metadata components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d DEBUG - Raw author response: Merve Gul (University of Pavia | Department of Chemical Engineering, Universitat Polite\u0300cnica de Catalunya (UPC-EEBE)) | Ida Genta (University of Pavia) | Maria M. Perez Madrigal (Department of Chemic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d DEBUG - Raw parsed_refs: Front. Bioeng. Biotechnol. - Vega-V\u00e1quez, P. et al. (2020) | Biomed. Pharmacother. - Fu, Y. S. et al...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udcc4 TITLE: INFLUENCE OF DRUG-POLYMER INTERACTIONS ON RELEASE KINETICS OF PLGA AND PLA/PEG NPS\n",
      "\n",
      "\ud83d\udc65 AUTHORS: 3 found\n",
      "   \u2022 Ida Genta (University of Pavia)\n",
      "   \u2022 Maria M. Perez Madrigal (Department of Chemical Engineering, Universitat Polite\u0300cnica de Catalunya)\n",
      "   \u2022 Enrica Chiesa (University of Pavia)\n",
      "\n",
      "\ud83d\udcdd SUMMARY: Here is a 2-sentence summary of the research:\n",
      "\n",
      "The study investigates the influence of drug-polymer ...\n",
      "\n",
      "\ud83d\udd11 KEYWORDS: PLGA, PLA, PEG, NPS, AMR\n",
      "\n",
      "\ud83d\udd2c METHODS: \u2022 Microfluidic-based synthesis of nano-sized carriers for drug delivery systems (NDDS) was used to p...\n",
      "\n",
      "\ud83d\udcca RESULTS: Here are the main research findings:\n",
      "\n",
      "* Release kinetics of PLGA and PLA/PEG NPs were influenced by ...\n",
      "\n",
      "\ud83d\udcda REFERENCES: 3 found\n",
      "   \u2022 Front. Bioeng. Biotechnol....\n",
      "   \u2022 Biomed. Pharmacother....\n",
      "\n",
      "\ud83d\udcb0 FUNDING: 5 sources\n",
      "   \u2022 (2)   Zhang...\n",
      "   \u2022 Y. et al. J. Control. Release 2019...\n",
      "\n",
      "\ud83c\udfdb\ufe0f  CONFERENCE: Bari, Italy | 15-17 May\n",
      "\u23f1\ufe0f  Processing time: 75.24s\n",
      "\ud83d\udcbe Results saved to: ../output/method2a_llama_results.json\n",
      "\u2705 Method 2a (Llama) completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Run extraction\n",
    "pdf_path = \"../data/test-poster.pdf\"\n",
    "\n",
    "if Path(pdf_path).exists():\n",
    "    print(\"\ud83d\ude80 Running Method 2a: Llama 3.2 8B Local Extraction\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract text\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"\ud83d\udccf Extracted {len(text)} characters\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize extractor\n",
    "        print(\"\ud83e\udd16 Initializing Llama 3.2 8B model...\")\n",
    "        extractor = LlamaExtractor()\n",
    "        \n",
    "        # Extract each field\n",
    "        print(\"\ud83d\udd0d Extracting metadata components...\")\n",
    "        \n",
    "        title = extractor.extract_field(text, \"title\")\n",
    "        authors = extractor.extract_field(text, \"authors\")\n",
    "        summary = extractor.extract_field(text, \"summary\")\n",
    "        keywords = extractor.extract_field(text, \"keywords\")\n",
    "        methods = extractor.extract_field(text, \"methods\")\n",
    "        results_text = extractor.extract_field(text, \"results\")\n",
    "        references = extractor.extract_field(text, \"references\")\n",
    "        funding_sources = extractor.extract_field(text, \"funding_sources\")\n",
    "        conference_info = extractor.extract_field(text, \"conference_info\")\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"summary\": summary,\n",
    "            \"keywords\": keywords,\n",
    "            \"methods\": methods,\n",
    "            \"results\": results_text,\n",
    "            \"references\": references,\n",
    "            \"funding_sources\": funding_sources,\n",
    "            \"conference_info\": conference_info,\n",
    "            \"extraction_metadata\": {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"processing_time\": time.time() - start_time,\n",
    "                \"method\": \"llama_local\",\n",
    "                \"model\": \"Meta-Llama-3-8B-Instruct\",\n",
    "                \"device\": str(next(extractor.model.parameters()).device),\n",
    "                \"text_length\": len(text),\n",
    "                \"do_sample\": False,\n",
    "                \"max_tokens\": 150\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n\ud83d\udcc4 TITLE: {results['title'][:100]}\")\n",
    "        print(f\"\\n\ud83d\udc65 AUTHORS: {len(results['authors'])} found\")\n",
    "        for author in results[\"authors\"]:\n",
    "            affil_str = f\" ({', '.join(author['affiliations'])})\" if author['affiliations'] else \"\"\n",
    "            print(f\"   \u2022 {author['name']}{affil_str}\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcdd SUMMARY: {results['summary'][:100]}...\")\n",
    "        print(f\"\\n\ud83d\udd11 KEYWORDS: {', '.join(results['keywords'][:5])}\")\n",
    "        print(f\"\\n\ud83d\udd2c METHODS: {results['methods'][:100]}...\")\n",
    "        print(f\"\\n\ud83d\udcca RESULTS: {results['results'][:100]}...\")\n",
    "        print(f\"\\n\ud83d\udcda REFERENCES: {len(results['references'])} found\")\n",
    "        for ref in results['references'][:2]:  # Show first 2\n",
    "            print(f\"   \u2022 {ref['title'][:50]}...\")\n",
    "        print(f\"\\n\ud83d\udcb0 FUNDING: {len(results['funding_sources'])} sources\")\n",
    "        for funding in results['funding_sources'][:2]:  # Show first 2\n",
    "            print(f\"   \u2022 {funding[:50]}...\")\n",
    "        print(f\"\\n\ud83c\udfdb\ufe0f  CONFERENCE: {results['conference_info']['location']} | {results['conference_info']['date']}\")\n",
    "        print(f\"\u23f1\ufe0f  Processing time: {results['extraction_metadata']['processing_time']:.2f}s\")\n",
    "        \n",
    "        # Save results with corrected filename\n",
    "        output_path = Path(\"../output/method2a_llama_results.json\")\n",
    "        output_path.parent.mkdir(exist_ok=True)\n",
    "        \n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"\ud83d\udcbe Results saved to: {output_path}\")\n",
    "        print(\"\u2705 Method 2a (Llama) completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Llama extraction failed: {e}\")\n",
    "        print(\"   This may be due to insufficient GPU memory or model download issues\")\n",
    "        \n",
    "else:\n",
    "    print(\"\u274c Test poster not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "13851f8df14a4a7b8016971e31181dd0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "173d7089803046f9bed69cf19a39149f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "29fbac17f9084c9b98ef19e504f31e90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_173d7089803046f9bed69cf19a39149f",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_30a4b1322c6d47729fb575e69629048a",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "30a4b1322c6d47729fb575e69629048a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5565a7ff3d8c42878c7775c1f8ea6980": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6fc40119fb8d4482a95b7e2185ddb844": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b853df112534a549eb705b74f296a2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_13851f8df14a4a7b8016971e31181dd0",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_c4b3f0d5f7d243d9bead47b47b4cb092",
       "tabbable": null,
       "tooltip": null,
       "value": "\u20074/4\u2007[00:04&lt;00:00,\u2007\u20071.08s/it]"
      }
     },
     "7d6741b7cdb14a70b0caf5af6f23afba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_aa3509789420473793c1b7fff8dd722d",
        "IPY_MODEL_29fbac17f9084c9b98ef19e504f31e90",
        "IPY_MODEL_7b853df112534a549eb705b74f296a2a"
       ],
       "layout": "IPY_MODEL_6fc40119fb8d4482a95b7e2185ddb844",
       "tabbable": null,
       "tooltip": null
      }
     },
     "82f6347a04f241c2a7b6ac79fb5276a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aa3509789420473793c1b7fff8dd722d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5565a7ff3d8c42878c7775c1f8ea6980",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_82f6347a04f241c2a7b6ac79fb5276a5",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading\u2007checkpoint\u2007shards:\u2007100%"
      }
     },
     "c4b3f0d5f7d243d9bead47b47b4cb092": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}