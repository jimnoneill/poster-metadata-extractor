{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 5: Qwen2-VL Vision Direct Extraction\n",
    "\n",
    "**Direct Image Processing for Scientific Posters**\n",
    "\n",
    "This notebook demonstrates how to use **Qwen2-VL-2B-Instruct** for scientific poster metadata extraction using direct image analysis without any text extraction.\n",
    "\n",
    "## ✨ Key Advantages:\n",
    "- **Direct Image Processing**: Analyzes poster visually like humans do\n",
    "- **Same Prompt Style**: Uses identical direct prompt as DeepSeek/Mistral\n",
    "- **No Text Extraction**: Pure vision-based processing\n",
    "- **Reusable & Generalizable**: Works with any scientific poster\n",
    "\n",
    "## 🎯 Results Preview:\n",
    "- ✅ **5/5 Authors** extracted with affiliations\n",
    "- ✅ **Complete JSON** structure in one go\n",
    "- ✅ **~43 seconds** processing time\n",
    "- ✅ **Direct approach** - processes images without OCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T22:20:42.175140Z",
     "iopub.status.busy": "2025-08-30T22:20:42.174918Z",
     "iopub.status.idle": "2025-08-30T22:20:48.025293Z",
     "shell.execute_reply": "2025-08-30T22:20:48.024776Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 15:20:46.243269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756592446.261606 2724074 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756592446.267226 2724074 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756592446.282755 2724074 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756592446.282770 2724074 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756592446.282773 2724074 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756592446.282775 2724074 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-30 15:20:46.287605: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 All imports successful!\n",
      "🔥 CUDA available: True\n",
      "🎯 GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "import time\n",
    "from PIL import Image\n",
    "import io\n",
    "import re\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "print(\"📦 All imports successful!\")\n",
    "print(f\"🔥 CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎯 GPU: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 The Direct Vision Prompt\n",
    "\n",
    "This is the same simple, direct prompt that works so well with DeepSeek/Mistral - but for images!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T22:20:48.027191Z",
     "iopub.status.busy": "2025-08-30T22:20:48.026657Z",
     "iopub.status.idle": "2025-08-30T22:20:48.032335Z",
     "shell.execute_reply": "2025-08-30T22:20:48.031943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Core vision functions defined\n",
      "🎯 Same prompt style as DeepSeek/Mistral - but for images!\n"
     ]
    }
   ],
   "source": [
    "def convert_pdf_to_images(pdf_path: str, dpi: int = 200) -> List[Image.Image]:\n",
    "    \"\"\"Convert PDF pages to high-quality PIL Images for vision models\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    \n",
    "    print(f\"📄 Converting PDF to images (DPI: {dpi})...\")\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        \n",
    "        # Convert to image with specified DPI\n",
    "        mat = fitz.Matrix(dpi/72, dpi/72)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        images.append(img)\n",
    "        \n",
    "        print(f\"   Page {page_num + 1}: {img.size[0]}x{img.size[1]} pixels\")\n",
    "    \n",
    "    doc.close()\n",
    "    return images\n",
    "\n",
    "def create_vision_prompt() -> str:\n",
    "    \"\"\"Create the same direct prompt style as DeepSeek/Mistral for vision models\"\"\"\n",
    "    return \"\"\"Analyze this scientific poster image and extract metadata. Return ONLY valid JSON with no explanations or formatting:\n",
    "\n",
    "{\n",
    "  \"title\": \"exact poster title\",\n",
    "  \"authors\": [\n",
    "    {\"name\": \"Full Name\", \"affiliations\": [\"Institution\"], \"email\": null}\n",
    "  ],\n",
    "  \"summary\": \"brief research summary\",\n",
    "  \"keywords\": [\"key\", \"terms\"],\n",
    "  \"methods\": \"methodology description\",\n",
    "  \"results\": \"main findings and results\",\n",
    "  \"references\": [\n",
    "    {\"title\": \"paper title\", \"authors\": \"authors\", \"year\": 2024, \"journal\": \"journal\"}\n",
    "  ],\n",
    "  \"funding_sources\": [\"funding info\"],\n",
    "  \"conference_info\": {\"location\": \"location\", \"date\": \"date\"}\n",
    "}\"\"\"\n",
    "\n",
    "def load_qwen2vl_model():\n",
    "    \"\"\"Load Qwen2-VL model and processor\"\"\"\n",
    "    print(\"🤖 Loading Qwen2-VL model...\")\n",
    "    \n",
    "    model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "    \n",
    "    try:\n",
    "        model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        print(f\"✅ Qwen2-VL loaded successfully\")\n",
    "        return model, processor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load Qwen2-VL: {e}\")\n",
    "        return None, None\n",
    "\n",
    "print(\"✅ Core vision functions defined\")\n",
    "print(\"🎯 Same prompt style as DeepSeek/Mistral - but for images!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T22:20:48.033766Z",
     "iopub.status.busy": "2025-08-30T22:20:48.033469Z",
     "iopub.status.idle": "2025-08-30T22:20:48.041140Z",
     "shell.execute_reply": "2025-08-30T22:20:48.040786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vision extraction functions defined\n",
      "🎯 Ready to process any scientific poster image!\n"
     ]
    }
   ],
   "source": [
    "def extract_with_qwen2vl(model, processor, image: Image.Image, prompt: str) -> str:\n",
    "    \"\"\"Extract metadata using Qwen2-VL vision model\"\"\"\n",
    "    print(\"🔄 Generating response with Qwen2-VL...\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare conversation format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Process inputs\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        inputs = inputs.to(\"cuda\")\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=3000,  # Increased for complete outputs\n",
    "                do_sample=False,\n",
    "                pad_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Trim input tokens and decode\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        \n",
    "        response = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        \n",
    "        return response.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Generation failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_vision_response(response: str) -> Dict[str, Any]:\n",
    "    \"\"\"Clean and parse vision response to valid JSON\"\"\"\n",
    "    print(f\"📝 Raw response length: {len(response)} chars\")\n",
    "    \n",
    "    # Remove markdown formatting\n",
    "    response = response.replace('```json', '').replace('```', '').strip()\n",
    "    \n",
    "    # Find JSON object boundaries\n",
    "    start_idx = response.find('{')\n",
    "    end_idx = response.rfind('}')\n",
    "    \n",
    "    if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
    "        json_str = response[start_idx:end_idx + 1]\n",
    "        \n",
    "        # Basic cleanup\n",
    "        json_str = re.sub(r',\\s*}', '}', json_str)  # Remove trailing commas\n",
    "        json_str = re.sub(r',\\s*]', ']', json_str)  # Remove trailing commas in arrays\n",
    "        \n",
    "        try:\n",
    "            # Parse and clean the structure\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            # Ensure all required fields exist with defaults\n",
    "            cleaned_data = {\n",
    "                \"title\": data.get(\"title\", \"Unknown Title\"),\n",
    "                \"authors\": data.get(\"authors\", []),\n",
    "                \"summary\": data.get(\"summary\", \"No summary available\"),\n",
    "                \"keywords\": data.get(\"keywords\", []),\n",
    "                \"methods\": data.get(\"methods\", \"No methods described\"),\n",
    "                \"results\": str(data.get(\"results\", \"No results available\")),  # Ensure string\n",
    "                \"references\": data.get(\"references\", []),  # Keep all references\n",
    "                \"funding_sources\": data.get(\"funding_sources\", []),\n",
    "                \"conference_info\": data.get(\"conference_info\", {})\n",
    "            }\n",
    "            \n",
    "            return cleaned_data\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"⚠️ JSON parsing failed: {e}\")\n",
    "            # Return minimal structure if parsing fails\n",
    "            return {\n",
    "                \"title\": \"Extraction Failed\",\n",
    "                \"authors\": [],\n",
    "                \"summary\": \"Could not parse response\",\n",
    "                \"keywords\": [],\n",
    "                \"methods\": \"Parsing error\",\n",
    "                \"results\": \"Parsing error\", \n",
    "                \"references\": [],\n",
    "                \"funding_sources\": [],\n",
    "                \"conference_info\": {}\n",
    "            }\n",
    "    \n",
    "    # If no JSON found, return empty structure\n",
    "    return {\n",
    "        \"title\": \"No JSON Found\",\n",
    "        \"authors\": [],\n",
    "        \"summary\": \"No structured data extracted\",\n",
    "        \"keywords\": [],\n",
    "        \"methods\": \"No data\",\n",
    "        \"results\": \"No data\",\n",
    "        \"references\": [],\n",
    "        \"funding_sources\": [],\n",
    "        \"conference_info\": {}\n",
    "    }\n",
    "\n",
    "print(\"✅ Vision extraction functions defined\")\n",
    "print(\"🎯 Ready to process any scientific poster image!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Run Vision-Based Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T22:20:48.042544Z",
     "iopub.status.busy": "2025-08-30T22:20:48.042232Z",
     "iopub.status.idle": "2025-08-30T22:21:36.224051Z",
     "shell.execute_reply": "2025-08-30T22:21:36.223562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running Method 5: Qwen2-VL Vision Direct Extraction\n",
      "=================================================================\n",
      "🤖 Loading Qwen2-VL model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239ebf5e134a44b48fa6dd463f79eee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Qwen2-VL loaded successfully\n",
      "📄 Converting PDF to images (DPI: 200)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Page 1: 5512x7874 pixels\n",
      "📸 Processing image: 5512x7874 pixels\n",
      "🔄 Generating response with Qwen2-VL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Raw response length: 2845 chars\n",
      "\n",
      "📊 EXTRACTION RESULTS:\n",
      "==================================================\n",
      "📄 TITLE: INFLUENCE OF DRUG-POLYMER INTERACTIONS ON RELEASE KINETICS OF PLGA AND PLA/PEG NPS\n",
      "👥 AUTHORS: 5 found\n",
      "   1. Merve Gul - ['Department of Drug Sciences, University of Pavia']\n",
      "   2. Ida Genta - ['Department of Chemical Engineering, Universitat Politècnica de Catalunya (UPC-EEBE)']\n",
      "   3. Maria M. Perez Madrigal - ['Barcelona Research Center for Multiscale Science and Engineering, EEBE, Universitat Politècnica de Catalunya']\n",
      "   4. Carlos Aleman - ['Barcelona Research Center for Multiscale Science and Engineering, EEBE, Universitat Politècnica de Catalunya']\n",
      "   5. Enrica Chiesa - ['Barcelona Research Center for Multiscale Science and Engineering, EEBE, Universitat Politècnica de Catalunya']\n",
      "💰 FUNDING: []\n",
      "📚 REFERENCES: 0 found\n",
      "⏱️ Processing time: 42.2 seconds\n",
      "💾 Results saved to: ../output/method5_qwen2vl_vision_results.json\n",
      "✅ Method 5 completed successfully!\n",
      "🎯 Same prompt as DeepSeek/Mistral, working vision results!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"🚀 Running Method 5: Qwen2-VL Vision Direct Extraction\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    # Load model\n",
    "    model, processor = load_qwen2vl_model()\n",
    "    \n",
    "    if model is not None:\n",
    "        # Convert PDF to image\n",
    "        pdf_path = \"../data/test-poster.pdf\"\n",
    "        images = convert_pdf_to_images(pdf_path, dpi=200)\n",
    "        \n",
    "        if images:\n",
    "            # Use first page\n",
    "            image = images[0]\n",
    "            print(f\"📸 Processing image: {image.size[0]}x{image.size[1]} pixels\")\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = create_vision_prompt()\n",
    "            \n",
    "            # Extract metadata\n",
    "            start_time = time.time()\n",
    "            response = extract_with_qwen2vl(model, processor, image, prompt)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if response:\n",
    "                # Clean and parse response\n",
    "                results = clean_vision_response(response)\n",
    "                \n",
    "                # Display results\n",
    "                print(\"\\n📊 EXTRACTION RESULTS:\")\n",
    "                print(\"=\" * 50)\n",
    "                print(f\"📄 TITLE: {results.get('title', 'N/A')}\")\n",
    "                print(f\"👥 AUTHORS: {len(results.get('authors', []))} found\")\n",
    "                for i, author in enumerate(results.get('authors', []), 1):\n",
    "                    print(f\"   {i}. {author.get('name', 'N/A')} - {author.get('affiliations', ['N/A'])}\")\n",
    "                \n",
    "                print(f\"💰 FUNDING: {results.get('funding_sources', ['None found'])}\")\n",
    "                print(f\"📚 REFERENCES: {len(results.get('references', []))} found\")\n",
    "                print(f\"⏱️ Processing time: {end_time - start_time:.1f} seconds\")\n",
    "                \n",
    "                # Save results\n",
    "                output_path = \"../output/method5_qwen2vl_vision_results.json\"\n",
    "                os.makedirs(\"../output\", exist_ok=True)\n",
    "                \n",
    "                with open(output_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=2)\n",
    "                \n",
    "                print(f\"💾 Results saved to: {output_path}\")\n",
    "                print(\"✅ Method 5 completed successfully!\")\n",
    "                print(\"🎯 Same prompt as DeepSeek/Mistral, working vision results!\")\n",
    "                \n",
    "            else:\n",
    "                print(\"❌ No response generated\")\n",
    "        else:\n",
    "            print(\"❌ No images extracted from PDF\")\n",
    "    else:\n",
    "        print(\"❌ Failed to load model\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ CUDA not available - vision models require GPU\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "058f44b3f6e440eebfbf0945b770ee52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1dd9438c5daf4a13b93917e9486849e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_336087d370c248539c40e2de6ddfd64b",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ffdea10088db4f2997459d1b3d7c8c08",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "239ebf5e134a44b48fa6dd463f79eee3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ab5f5302b5c341abb73c2a239c7fd479",
        "IPY_MODEL_1dd9438c5daf4a13b93917e9486849e2",
        "IPY_MODEL_8fe7db347b5d466bad11c2d60461a173"
       ],
       "layout": "IPY_MODEL_7913e5e23895427e8f117dc9b13035a5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "336087d370c248539c40e2de6ddfd64b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7913e5e23895427e8f117dc9b13035a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "896a51e4583b4450a46ef9c5d567a957": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8fe7db347b5d466bad11c2d60461a173": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_896a51e4583b4450a46ef9c5d567a957",
       "placeholder": "​",
       "style": "IPY_MODEL_058f44b3f6e440eebfbf0945b770ee52",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:01&lt;00:00,  1.01it/s]"
      }
     },
     "ab5f5302b5c341abb73c2a239c7fd479": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cca194ca9b89482abc4026977d67a49c",
       "placeholder": "​",
       "style": "IPY_MODEL_fed29608d700441b81e66a6822b43450",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "cca194ca9b89482abc4026977d67a49c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fed29608d700441b81e66a6822b43450": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ffdea10088db4f2997459d1b3d7c8c08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
