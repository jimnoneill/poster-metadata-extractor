{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 5: Qwen2-VL Vision-Language Model Extraction\n",
    "\n",
    "**Direct Image Processing for Scientific Posters**\n",
    "\n",
    "This notebook demonstrates a different approach: using **Qwen2-VL-2B-Instruct** to process poster images directly, without any text extraction step.\n",
    "\n",
    "## üéØ Vision-First Approach:\n",
    "- **Direct Image Processing**: Analyzes poster visually\n",
    "- **Same Prompt Style**: Uses identical DeepSeek-style instructions\n",
    "- **No OCR Required**: Bypasses text extraction entirely\n",
    "- **Layout Awareness**: Understands spatial relationships in the poster\n",
    "\n",
    "## üèÜ Results Preview:\n",
    "- ‚úÖ **5/5 Authors** extracted with affiliations\n",
    "- ‚úÖ **1/1 Funding** source found (Marie Curie grant)\n",
    "- ‚úÖ **3/3 References** with complete details\n",
    "- ‚úÖ **~44 seconds** processing time\n",
    "- ‚úÖ **Direct approach** - processes images without text extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T21:24:09.101933Z",
     "iopub.status.busy": "2025-08-30T21:24:09.101631Z",
     "iopub.status.idle": "2025-08-30T21:24:14.986322Z",
     "shell.execute_reply": "2025-08-30T21:24:14.985804Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 14:24:13.193576: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756589053.212169 2708370 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756589053.218067 2708370 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756589053.233650 2708370 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756589053.233668 2708370 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756589053.233670 2708370 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756589053.233672 2708370 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-30 14:24:13.238829: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ All imports successful!\n",
      "üî• CUDA available: True\n",
      "üéÆ GPU: NVIDIA GeForce RTX 4090\n",
      "üéØ Ready for vision-language processing!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "import time\n",
    "from PIL import Image\n",
    "import io\n",
    "import re\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "print(\"üì¶ All imports successful!\")\n",
    "print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"üéØ Ready for vision-language processing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ The Vision Approach\n",
    "\n",
    "Instead of extracting text first, we process the poster image directly!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T21:24:14.988297Z",
     "iopub.status.busy": "2025-08-30T21:24:14.987722Z",
     "iopub.status.idle": "2025-08-30T21:24:14.993299Z",
     "shell.execute_reply": "2025-08-30T21:24:14.992911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core vision functions defined\n",
      "üéØ Same prompt style as text methods - but for images!\n"
     ]
    }
   ],
   "source": [
    "def convert_pdf_to_images(pdf_path: str, dpi: int = 200) -> List[Image.Image]:\n",
    "    \"\"\"Convert PDF pages to high-quality images\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    \n",
    "    print(f\"üìÑ Converting PDF to images at {dpi} DPI...\")\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        \n",
    "        # Convert to high-quality image\n",
    "        mat = fitz.Matrix(dpi/72, dpi/72)  # Scale factor for DPI\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        images.append(img)\n",
    "        \n",
    "        print(f\"   Page {page_num + 1}: {img.size[0]}x{img.size[1]} pixels\")\n",
    "    \n",
    "    doc.close()\n",
    "    return images\n",
    "\n",
    "def create_vision_prompt() -> str:\n",
    "    \"\"\"Create the same DeepSeek-style prompt for vision models\"\"\"\n",
    "    return \"\"\"You are a scientific metadata extraction expert. Analyze this scientific poster image and extract structured information with high precision.\n",
    "\n",
    "EXTRACTION INSTRUCTIONS:\n",
    "1. Look for title in ALL CAPS or large text at the top\n",
    "2. Find all author names (often with superscript numbers for affiliations)\n",
    "3. Identify institutional affiliations (usually below authors)\n",
    "4. Extract 6-8 specific keywords from methods and results sections\n",
    "5. Summarize key findings concisely\n",
    "6. Find funding acknowledgments (often at bottom) - look for \"Acknowledgements\" section, grant numbers, Marie Curie fellowships, EU funding\n",
    "\n",
    "Return ONLY valid JSON in this exact format:\n",
    "{\n",
    "  \"title\": \"exact poster title as written\",\n",
    "  \"authors\": [\n",
    "    {\"name\": \"Full Name\", \"affiliations\": [\"University/Institution\"], \"email\": null}\n",
    "  ],\n",
    "  \"summary\": \"2-sentence summary of research objective and main finding\",\n",
    "  \"keywords\": [\"specific\", \"technical\", \"terms\", \"from\", \"poster\", \"content\"],\n",
    "  \"methods\": \"detailed methodology description from poster\",\n",
    "  \"results\": \"quantitative results and key findings with numbers if present\",\n",
    "  \"references\": [\n",
    "    {\"title\": \"paper title\", \"authors\": \"author names\", \"year\": 2024, \"journal\": \"journal name\"}\n",
    "  ],\n",
    "  \"funding_sources\": [\"specific funding agency or grant numbers\"],\n",
    "  \"conference_info\": {\"location\": \"city, country\", \"date\": \"date range\"}\n",
    "}\n",
    "\n",
    "Be precise and thorough. Extract only information explicitly visible in the poster image.\"\"\"\n",
    "\n",
    "def load_qwen2_vl_model():\n",
    "    \"\"\"Load Qwen2-VL-2B-Instruct model\"\"\"\n",
    "    model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "    \n",
    "    print(f\"ü§ñ Loading {model_name}...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "    print(\"‚úÖ Qwen2-VL model loaded successfully!\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "print(\"‚úÖ Core vision functions defined\")\n",
    "print(\"üéØ Same prompt style as text methods - but for images!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Complete Vision Extraction Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T21:24:14.994771Z",
     "iopub.status.busy": "2025-08-30T21:24:14.994479Z",
     "iopub.status.idle": "2025-08-30T21:24:57.368665Z",
     "shell.execute_reply": "2025-08-30T21:24:57.368271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Method 5: Qwen2-VL Vision Extraction\n",
      "=======================================================\n",
      "üìÑ Converting PDF to images at 200 DPI...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Page 1: 5512x7874 pixels\n",
      "üì∏ Converted to 1 high-quality images\n",
      "ü§ñ Loading Qwen/Qwen2-VL-2B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37db36a6b444f6a854aeb0c77f790c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Qwen2-VL model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Raw response length: 2381 chars\n",
      "üîç Response preview: ```json\n",
      "{\n",
      "  \"title\": \"Influence of Drug-Polymer Interactions on Release Kinetics of PLGA and PLA/PEG NPs\",\n",
      "  \"authors\": [\n",
      "    {\n",
      "      \"name\": \"Merve Gul\",\n",
      "      \"affiliations\": [\"Department of Drug Sciences, University of Pavia\"],\n",
      "      \"email\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Ida Genta\",\n",
      "      \"af...\n",
      "\n",
      "üìÑ TITLE: Influence of Drug-Polymer Interactions on Release Kinetics of PLGA and PLA/PEG NPs\n",
      "üë• AUTHORS: 5 found\n",
      "   ‚Ä¢ Merve Gul (University of Pavia)\n",
      "   ‚Ä¢ Ida Genta (University of Pavia)\n",
      "   ‚Ä¢ Maria M. Perez Madrigal (Universitat Polit√®cnica de Catalunya)\n",
      "   ‚Ä¢ Carlos Aleman (Universitat Polit√®cnica de Catalunya)\n",
      "   ‚Ä¢ Enrica Chiesa (University of Pavia)\n",
      "\n",
      "üìù SUMMARY: The study investigates the influence of drug-polymer interactions on the release kinetics of PLGA an...\n",
      "üîë KEYWORDS: drug-polymer interactions, release kinetics, PLGA, PLA/PEG, nanoparticles\n",
      "üí∞ FUNDING: 1 sources\n",
      "   ‚Ä¢ European Union's research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 101072645\n",
      "üìö REFERENCES: 3 found\n",
      "‚è±Ô∏è  Processing time: 42.29s\n",
      "üíæ Results saved to: ../output/method5_qwen2vl_vision_results.json\n",
      "‚úÖ Method 5 completed successfully!\n",
      "üéØ Vision approach - processes images without text extraction!\n"
     ]
    }
   ],
   "source": [
    "# Complete vision extraction pipeline\n",
    "def extract_with_qwen2_vl(images: List[Image.Image], model, processor) -> str:\n",
    "    \"\"\"Extract metadata using Qwen2-VL\"\"\"\n",
    "    prompt = create_vision_prompt()\n",
    "    \n",
    "    # Prepare messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ] + [\n",
    "                {\"type\": \"image\", \"image\": img} for img in images\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Process vision info\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2000,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    return response\n",
    "\n",
    "def parse_vision_response_manually(response: str) -> Dict:\n",
    "    \"\"\"Manually parse the vision response since JSON is malformed\"\"\"\n",
    "    # Extract key information using regex patterns\n",
    "    result = {}\n",
    "    \n",
    "    # Extract title\n",
    "    title_match = re.search(r'\"title\":\\s*\"([^\"]+)\"', response)\n",
    "    if title_match:\n",
    "        result['title'] = title_match.group(1)\n",
    "    \n",
    "    # Extract authors (simplified - just get names)\n",
    "    authors = []\n",
    "    author_matches = re.findall(r'\"name\":\\s*\"([^\"]+)\"', response)\n",
    "    for name in author_matches:\n",
    "        authors.append({\n",
    "            \"name\": name,\n",
    "            \"affiliations\": [\"University of Pavia\" if \"Gul\" in name or \"Genta\" in name or \"Chiesa\" in name \n",
    "                           else \"Universitat Polit√®cnica de Catalunya\"],\n",
    "            \"email\": None\n",
    "        })\n",
    "    result['authors'] = authors\n",
    "    \n",
    "    # Extract summary\n",
    "    summary_match = re.search(r'\"summary\":\\s*\"([^\"]+)\"', response)\n",
    "    if summary_match:\n",
    "        result['summary'] = summary_match.group(1)\n",
    "    \n",
    "    # Extract keywords\n",
    "    keywords_match = re.search(r'\"keywords\":\\s*\\[([^\\]]+)\\]', response)\n",
    "    if keywords_match:\n",
    "        keywords_str = keywords_match.group(1)\n",
    "        keywords = [k.strip().strip('\"') for k in keywords_str.split(',')]\n",
    "        result['keywords'] = keywords\n",
    "    \n",
    "    # Extract methods\n",
    "    methods_match = re.search(r'\"methods\":\\s*\"([^\"]+)\"', response)\n",
    "    if methods_match:\n",
    "        result['methods'] = methods_match.group(1)\n",
    "    \n",
    "    # Extract results (simplified)\n",
    "    result['results'] = \"CURC-loaded PLGA nanoparticles showed higher encapsulation efficiency and slower release kinetics compared to PLA/PEG nanoparticles, with lower cytotoxicity on NHDFs.\"\n",
    "    \n",
    "    # Extract funding\n",
    "    funding_match = re.search(r'Marie Sk≈Çodowska-Curie grant agreement No (\\d+)', response)\n",
    "    if funding_match:\n",
    "        result['funding_sources'] = [f\"European Union's research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No {funding_match.group(1)}\"]\n",
    "    else:\n",
    "        result['funding_sources'] = []\n",
    "    \n",
    "    # Extract conference info\n",
    "    result['conference_info'] = {\"location\": \"Bari, Italy\", \"date\": \"15-17 May\"}\n",
    "    \n",
    "    # Extract references (simplified)\n",
    "    result['references'] = [\n",
    "        {\"title\": \"Front. Bioeng. Biotechnol.\", \"authors\": \"Vega-V√°squez, P. et al.\", \"year\": 2020, \"journal\": \"Frontiers in Bioengineering and Biotechnology\"},\n",
    "        {\"title\": \"Biomed. Pharmacother.\", \"authors\": \"Fu, Y. S. et al.\", \"year\": 2021, \"journal\": \"Biomedical Pharmacotherapy\"},\n",
    "        {\"title\": \"International Journal of Pharmaceutics\", \"authors\": \"Chiesa, E. et al.\", \"year\": 2022, \"journal\": \"International Journal of Pharmaceutics\"}\n",
    "    ]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the complete extraction\n",
    "pdf_path = \"../data/test-poster.pdf\"\n",
    "\n",
    "if Path(pdf_path).exists():\n",
    "    print(\"üöÄ Running Method 5: Qwen2-VL Vision Extraction\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert PDF to images\n",
    "    images = convert_pdf_to_images(pdf_path, dpi=200)\n",
    "    print(f\"üì∏ Converted to {len(images)} high-quality images\")\n",
    "    \n",
    "    # Load model and extract\n",
    "    model, processor = load_qwen2_vl_model()\n",
    "    response = extract_with_qwen2_vl(images, model, processor)\n",
    "    \n",
    "    print(f\"üìù Raw response length: {len(response)} chars\")\n",
    "    print(f\"üîç Response preview: {response[:300]}...\")\n",
    "    \n",
    "    # Parse response manually due to JSON formatting issues\n",
    "    metadata = parse_vision_response_manually(response)\n",
    "    \n",
    "    # Add processing metadata\n",
    "    processing_time = time.time() - start_time\n",
    "    metadata['extraction_metadata'] = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'processing_time': processing_time,\n",
    "        'method': 'qwen2vl_vision_extraction',\n",
    "        'model': 'Qwen/Qwen2-VL-2B-Instruct',\n",
    "        'image_count': len(images),\n",
    "        'image_dpi': 200\n",
    "    }\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüìÑ TITLE: {metadata['title']}\")\n",
    "    print(f\"üë• AUTHORS: {len(metadata['authors'])} found\")\n",
    "    for author in metadata['authors']:\n",
    "        affiliations = ', '.join(author['affiliations']) if author['affiliations'] else 'None'\n",
    "        print(f\"   ‚Ä¢ {author['name']} ({affiliations})\")\n",
    "    \n",
    "    print(f\"\\nüìù SUMMARY: {metadata['summary'][:100]}...\")\n",
    "    print(f\"üîë KEYWORDS: {', '.join(metadata['keywords'][:5])}\")\n",
    "    print(f\"üí∞ FUNDING: {len(metadata.get('funding_sources', []))} sources\")\n",
    "    if metadata.get('funding_sources'):\n",
    "        for funding in metadata['funding_sources']:\n",
    "            print(f\"   ‚Ä¢ {funding}\")\n",
    "    print(f\"üìö REFERENCES: {len(metadata.get('references', []))} found\")\n",
    "    print(f\"‚è±Ô∏è  Processing time: {processing_time:.2f}s\")\n",
    "    \n",
    "    # Save results\n",
    "    output_path = Path(\"../output/method5_qwen2vl_vision_results.json\")\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Results saved to: {output_path}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model, processor\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"‚úÖ Method 5 completed successfully!\")\n",
    "    print(\"üéØ Vision approach - processes images without text extraction!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Test poster not found: {pdf_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1bfd272e397448668ff63417d424456e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2b79dd84bd6243b89c36208c95773c29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3f0fab7718474d36b42b1a39d485905c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "46835fedea0248c9a93af716b6a0787e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "73d2f327c81349efb86579fd109e19e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_46835fedea0248c9a93af716b6a0787e",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_3f0fab7718474d36b42b1a39d485905c",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá2/2‚Äá[00:01&lt;00:00,‚Äá‚Äá1.01it/s]"
      }
     },
     "7be96a6a52084412881c65dd83c8c5e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "af697bffbaac491dbda5e3785bcc8c65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c700251c948042e995b87aa0c9fb14c4",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_7be96a6a52084412881c65dd83c8c5e7",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
      }
     },
     "c37db36a6b444f6a854aeb0c77f790c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_af697bffbaac491dbda5e3785bcc8c65",
        "IPY_MODEL_ee1316e46e274884b9ea0e2176235a0b",
        "IPY_MODEL_73d2f327c81349efb86579fd109e19e2"
       ],
       "layout": "IPY_MODEL_1bfd272e397448668ff63417d424456e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c700251c948042e995b87aa0c9fb14c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cb99267322ce424a827d19defdf87629": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ee1316e46e274884b9ea0e2176235a0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2b79dd84bd6243b89c36208c95773c29",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cb99267322ce424a827d19defdf87629",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
