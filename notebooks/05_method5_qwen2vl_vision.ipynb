{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 5: Qwen2-VL Vision Direct Extraction\n",
    "\n",
    "**Direct Image Processing for Scientific Posters**\n",
    "\n",
    "This notebook demonstrates how to use **Qwen2-VL-2B-Instruct** for scientific poster metadata extraction using direct image analysis without any text extraction.\n",
    "\n",
    "## ✨ Key Advantages:\n",
    "- **Direct Image Processing**: Analyzes poster visually like humans do\n",
    "- **Same Prompt Style**: Uses identical direct prompt as DeepSeek/Mistral\n",
    "- **No Text Extraction**: Pure vision-based processing\n",
    "- **Reusable & Generalizable**: Works with any scientific poster\n",
    "\n",
    "## 🎯 Results Preview:\n",
    "- ✅ **5/5 Authors** extracted with affiliations\n",
    "- ✅ **Complete JSON** structure in one go\n",
    "- ✅ **~43 seconds** processing time\n",
    "- ✅ **Direct approach** - processes images without OCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T22:25:23.238688Z",
     "iopub.status.busy": "2025-08-30T22:25:23.238397Z",
     "iopub.status.idle": "2025-08-30T22:25:29.098055Z",
     "shell.execute_reply": "2025-08-30T22:25:29.097566Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 15:25:27.314613: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756592727.333197 2725683 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756592727.339074 2725683 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756592727.354519 2725683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756592727.354536 2725683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756592727.354538 2725683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756592727.354540 2725683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-30 15:25:27.359393: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 All imports successful!\n",
      "🔥 CUDA available: True\n",
      "🎯 GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "import time\n",
    "from PIL import Image\n",
    "import io\n",
    "import re\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "print(\"📦 All imports successful!\")\n",
    "print(f\"🔥 CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎯 GPU: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 The Direct Vision Prompt\n",
    "\n",
    "This is the same simple, direct prompt that works so well with DeepSeek/Mistral - but for images!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T22:25:29.099958Z",
     "iopub.status.busy": "2025-08-30T22:25:29.099434Z",
     "iopub.status.idle": "2025-08-30T22:25:29.105423Z",
     "shell.execute_reply": "2025-08-30T22:25:29.105037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Core vision functions defined\n",
      "🎯 Same prompt style as DeepSeek/Mistral - but for images!\n"
     ]
    }
   ],
   "source": [
    "def convert_pdf_to_images(pdf_path: str, dpi: int = 200) -> List[Image.Image]:\n",
    "    \"\"\"Convert PDF pages to high-quality PIL Images for vision models\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    \n",
    "    print(f\"📄 Converting PDF to images (DPI: {dpi})...\")\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        \n",
    "        # Convert to image with specified DPI\n",
    "        mat = fitz.Matrix(dpi/72, dpi/72)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        images.append(img)\n",
    "        \n",
    "        print(f\"   Page {page_num + 1}: {img.size[0]}x{img.size[1]} pixels\")\n",
    "    \n",
    "    doc.close()\n",
    "    return images\n",
    "\n",
    "def create_vision_prompt() -> str:\n",
    "    \"\"\"Create the same detailed prompt style as Mistral for vision models\"\"\"\n",
    "    return \"\"\"You are a scientific metadata extraction expert. Analyze this scientific poster image and extract structured information with high precision.\n",
    "\n",
    "EXTRACTION INSTRUCTIONS:\n",
    "1. Look for title in ALL CAPS or large text at the top\n",
    "2. Find all author names (often with superscript numbers for affiliations)\n",
    "3. Identify institutional affiliations (usually below authors)\n",
    "4. Extract 6-8 specific keywords from methods and results sections\n",
    "5. Summarize key findings concisely\n",
    "6. Find funding acknowledgments (often at bottom) - look for \"Acknowledgements\" section, grant numbers, Marie Curie fellowships, EU funding\n",
    "7. Look for references section (usually at bottom in small text) - extract paper titles, authors, years, journals\n",
    "8. Find conference information - location and dates (often at top or bottom of poster)\n",
    "\n",
    "IMPORTANT: Look carefully at ALL parts of the poster including small text at the bottom for references and funding information.\n",
    "\n",
    "Return ONLY valid JSON in this exact format:\n",
    "{\n",
    "  \"title\": \"exact poster title as written\",\n",
    "  \"authors\": [\n",
    "    {\"name\": \"Full Name\", \"affiliations\": [\"University/Institution\"], \"email\": null}\n",
    "  ],\n",
    "  \"summary\": \"2-sentence summary of research objective and main finding\",\n",
    "  \"keywords\": [\"specific\", \"technical\", \"terms\", \"from\", \"poster\", \"content\"],\n",
    "  \"methods\": \"detailed methodology description from poster\",\n",
    "  \"results\": \"quantitative results and key findings with numbers if present\",\n",
    "  \"references\": [\n",
    "    {\"title\": \"paper title\", \"authors\": \"author names\", \"year\": 2024, \"journal\": \"journal name\"}\n",
    "  ],\n",
    "  \"funding_sources\": [\"specific funding agency or grant numbers\"],\n",
    "  \"conference_info\": {\"location\": \"city, country\", \"date\": \"date range\"}\n",
    "}\n",
    "\n",
    "Be precise and thorough. Extract only information explicitly visible in the poster image.\"\"\"\n",
    "\n",
    "def load_qwen2vl_model():\n",
    "    \"\"\"Load Qwen2-VL model and processor\"\"\"\n",
    "    print(\"🤖 Loading Qwen2-VL model...\")\n",
    "    \n",
    "    model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "    \n",
    "    try:\n",
    "        model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        print(f\"✅ Qwen2-VL loaded successfully\")\n",
    "        return model, processor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load Qwen2-VL: {e}\")\n",
    "        return None, None\n",
    "\n",
    "print(\"✅ Core vision functions defined\")\n",
    "print(\"🎯 Same prompt style as DeepSeek/Mistral - but for images!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T22:25:29.106882Z",
     "iopub.status.busy": "2025-08-30T22:25:29.106573Z",
     "iopub.status.idle": "2025-08-30T22:25:29.114242Z",
     "shell.execute_reply": "2025-08-30T22:25:29.113815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vision extraction functions defined\n",
      "🎯 Ready to process any scientific poster image!\n"
     ]
    }
   ],
   "source": [
    "def extract_with_qwen2vl(model, processor, image: Image.Image, prompt: str) -> str:\n",
    "    \"\"\"Extract metadata using Qwen2-VL vision model\"\"\"\n",
    "    print(\"🔄 Generating response with Qwen2-VL...\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare conversation format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Process inputs\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        inputs = inputs.to(\"cuda\")\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=3000,  # Increased for complete outputs\n",
    "                do_sample=False,\n",
    "                pad_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Trim input tokens and decode\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        \n",
    "        response = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        \n",
    "        return response.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Generation failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_vision_response(response: str) -> Dict[str, Any]:\n",
    "    \"\"\"Clean and parse vision response to valid JSON\"\"\"\n",
    "    print(f\"📝 Raw response length: {len(response)} chars\")\n",
    "    \n",
    "    # Remove markdown formatting\n",
    "    response = response.replace('```json', '').replace('```', '').strip()\n",
    "    \n",
    "    # Find JSON object boundaries\n",
    "    start_idx = response.find('{')\n",
    "    end_idx = response.rfind('}')\n",
    "    \n",
    "    if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
    "        json_str = response[start_idx:end_idx + 1]\n",
    "        \n",
    "        # Basic cleanup\n",
    "        json_str = re.sub(r',\\s*}', '}', json_str)  # Remove trailing commas\n",
    "        json_str = re.sub(r',\\s*]', ']', json_str)  # Remove trailing commas in arrays\n",
    "        \n",
    "        try:\n",
    "            # Parse and clean the structure\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            # Ensure all required fields exist with defaults\n",
    "            cleaned_data = {\n",
    "                \"title\": data.get(\"title\", \"Unknown Title\"),\n",
    "                \"authors\": data.get(\"authors\", []),\n",
    "                \"summary\": data.get(\"summary\", \"No summary available\"),\n",
    "                \"keywords\": data.get(\"keywords\", []),\n",
    "                \"methods\": data.get(\"methods\", \"No methods described\"),\n",
    "                \"results\": str(data.get(\"results\", \"No results available\")),  # Ensure string\n",
    "                \"references\": data.get(\"references\", []),  # Keep all references\n",
    "                \"funding_sources\": data.get(\"funding_sources\", []),\n",
    "                \"conference_info\": data.get(\"conference_info\", {})\n",
    "            }\n",
    "            \n",
    "            return cleaned_data\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"⚠️ JSON parsing failed: {e}\")\n",
    "            # Return minimal structure if parsing fails\n",
    "            return {\n",
    "                \"title\": \"Extraction Failed\",\n",
    "                \"authors\": [],\n",
    "                \"summary\": \"Could not parse response\",\n",
    "                \"keywords\": [],\n",
    "                \"methods\": \"Parsing error\",\n",
    "                \"results\": \"Parsing error\", \n",
    "                \"references\": [],\n",
    "                \"funding_sources\": [],\n",
    "                \"conference_info\": {}\n",
    "            }\n",
    "    \n",
    "    # If no JSON found, return empty structure\n",
    "    return {\n",
    "        \"title\": \"No JSON Found\",\n",
    "        \"authors\": [],\n",
    "        \"summary\": \"No structured data extracted\",\n",
    "        \"keywords\": [],\n",
    "        \"methods\": \"No data\",\n",
    "        \"results\": \"No data\",\n",
    "        \"references\": [],\n",
    "        \"funding_sources\": [],\n",
    "        \"conference_info\": {}\n",
    "    }\n",
    "\n",
    "print(\"✅ Vision extraction functions defined\")\n",
    "print(\"🎯 Ready to process any scientific poster image!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Run Vision-Based Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T22:25:29.115674Z",
     "iopub.status.busy": "2025-08-30T22:25:29.115358Z",
     "iopub.status.idle": "2025-08-30T22:26:19.209508Z",
     "shell.execute_reply": "2025-08-30T22:26:19.208965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running Method 5: Qwen2-VL Vision Direct Extraction\n",
      "=================================================================\n",
      "🤖 Loading Qwen2-VL model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aac09707ac947949365a79e38269b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Qwen2-VL loaded successfully\n",
      "📄 Converting PDF to images (DPI: 300)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joneill/myenv/lib/python3.12/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (97653348 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Page 1: 8268x11811 pixels\n",
      "📸 Processing image: 8268x11811 pixels\n",
      "🔄 Generating response with Qwen2-VL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Raw response length: 2659 chars\n",
      "\n",
      "📊 EXTRACTION RESULTS:\n",
      "==================================================\n",
      "📄 TITLE: INFLUENCE OF DRUG-POLYMER INTERACTIONS ON RELEASE KINETICS OF PLGA AND PLA/PEG NPS\n",
      "👥 AUTHORS: 5 found\n",
      "   1. Merve Gul - ['Department of Drug Sciences, University of Pavia']\n",
      "   2. Ida Genta - ['Department of Chemical Engineering, Universitat Politècnica de Catalunya (UPC-EEBE)']\n",
      "   3. Maria M. Perez Madrigal - ['Barcelona Research Center for Multiscale Science and Engineering, EEBE, Universitat Politècnica de Catalunya']\n",
      "   4. Carlos Aleman - ['Barcelona Research Center for Multiscale Science and Engineering, EEBE, Universitat Politècnica de Catalunya']\n",
      "   5. Enrica Chiesa - ['Barcelona Research Center for Multiscale Science and Engineering, EEBE, Universitat Politècnica de Catalunya']\n",
      "💰 FUNDING: [\"European Union's research and innovation programme under the Marie Skłodowska-Curie grant agreement No 101072645\"]\n",
      "📚 REFERENCES: 3 found\n",
      "⏱️ Processing time: 42.2 seconds\n",
      "💾 Results saved to: ../output/method5_qwen2vl_vision_results.json\n",
      "✅ Method 5 completed successfully!\n",
      "🎯 Same prompt as DeepSeek/Mistral, working vision results!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"🚀 Running Method 5: Qwen2-VL Vision Direct Extraction\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    # Load model\n",
    "    model, processor = load_qwen2vl_model()\n",
    "    \n",
    "    if model is not None:\n",
    "        # Convert PDF to image\n",
    "        pdf_path = \"../data/test-poster.pdf\"\n",
    "        images = convert_pdf_to_images(pdf_path, dpi=300)  # Higher DPI for better small text reading\n",
    "        \n",
    "        if images:\n",
    "            # Use first page\n",
    "            image = images[0]\n",
    "            print(f\"📸 Processing image: {image.size[0]}x{image.size[1]} pixels\")\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = create_vision_prompt()\n",
    "            \n",
    "            # Extract metadata\n",
    "            start_time = time.time()\n",
    "            response = extract_with_qwen2vl(model, processor, image, prompt)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if response:\n",
    "                # Clean and parse response\n",
    "                results = clean_vision_response(response)\n",
    "                \n",
    "                # Display results\n",
    "                print(\"\\n📊 EXTRACTION RESULTS:\")\n",
    "                print(\"=\" * 50)\n",
    "                print(f\"📄 TITLE: {results.get('title', 'N/A')}\")\n",
    "                print(f\"👥 AUTHORS: {len(results.get('authors', []))} found\")\n",
    "                for i, author in enumerate(results.get('authors', []), 1):\n",
    "                    print(f\"   {i}. {author.get('name', 'N/A')} - {author.get('affiliations', ['N/A'])}\")\n",
    "                \n",
    "                print(f\"💰 FUNDING: {results.get('funding_sources', ['None found'])}\")\n",
    "                print(f\"📚 REFERENCES: {len(results.get('references', []))} found\")\n",
    "                print(f\"⏱️ Processing time: {end_time - start_time:.1f} seconds\")\n",
    "                \n",
    "                # Save results\n",
    "                output_path = \"../output/method5_qwen2vl_vision_results.json\"\n",
    "                os.makedirs(\"../output\", exist_ok=True)\n",
    "                \n",
    "                with open(output_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=2)\n",
    "                \n",
    "                print(f\"💾 Results saved to: {output_path}\")\n",
    "                print(\"✅ Method 5 completed successfully!\")\n",
    "                print(\"🎯 Same prompt as DeepSeek/Mistral, working vision results!\")\n",
    "                \n",
    "            else:\n",
    "                print(\"❌ No response generated\")\n",
    "        else:\n",
    "            print(\"❌ No images extracted from PDF\")\n",
    "    else:\n",
    "        print(\"❌ Failed to load model\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ CUDA not available - vision models require GPU\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1aac09707ac947949365a79e38269b5f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3864df4e85d74e409f0998cc1b2b40ec",
        "IPY_MODEL_a288ee8d4046428fbbae28fabb662afa",
        "IPY_MODEL_9349c6eb79274e05b505060457a6bbda"
       ],
       "layout": "IPY_MODEL_a4545c3cec174897bd7917a35d496aaf",
       "tabbable": null,
       "tooltip": null
      }
     },
     "37d7b11b108448bc853faf2744f8fc01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3864df4e85d74e409f0998cc1b2b40ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6741d36e6d984c2ba1ddb7cc56fee133",
       "placeholder": "​",
       "style": "IPY_MODEL_37d7b11b108448bc853faf2744f8fc01",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "5b4d3ec813c54ee2b795f6c3593e9e6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6741d36e6d984c2ba1ddb7cc56fee133": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "90df1b0b644c49eab67e5a4f212832e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9349c6eb79274e05b505060457a6bbda": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_90df1b0b644c49eab67e5a4f212832e2",
       "placeholder": "​",
       "style": "IPY_MODEL_c5f35dfdc20f480aa60375a32129854e",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:01&lt;00:00,  2.08it/s]"
      }
     },
     "a288ee8d4046428fbbae28fabb662afa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ef23ce37bb604c39b61fe4a7e4524c4a",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5b4d3ec813c54ee2b795f6c3593e9e6b",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "a4545c3cec174897bd7917a35d496aaf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c5f35dfdc20f480aa60375a32129854e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ef23ce37bb604c39b61fe4a7e4524c4a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
