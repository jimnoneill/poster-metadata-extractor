{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2a: Llama 3.2 8B Local Extraction\n",
    "\n",
    "## Overview\n",
    "Local large language model for high-quality poster metadata extraction. Runs entirely on your hardware without API dependencies.\n",
    "\n",
    "## Accuracy Note\n",
    "The accuracy estimate is unvalidated - based on limited testing only. Actual accuracy must be determined through proper Cochran sampling validation before production use.\n",
    "\n",
    "## Performance Characteristics\n",
    "- **Estimated Accuracy**: 85-90% (unvalidated - requires Cochran sampling validation)\n",
    "- **Cost**: $0 (runs locally, only electricity costs)\n",
    "- **Speed**: 15-45 seconds per poster (single), ~2-3s per poster (RTX 4090 batched)\n",
    "- **Hallucination Risk**: Very Low (structured prompting + greedy decoding)\n",
    "- **Setup**: Medium-Complex - requires model download and 8GB+ VRAM\n",
    "\n",
    "## Hardware Requirements\n",
    "- **GPU**: 8GB+ VRAM (RTX 4090 recommended)\n",
    "- **RAM**: 16GB+ system memory\n",
    "- **Storage**: ~16GB for model files\n",
    "\n",
    "## Best For\n",
    "- Privacy-sensitive environments requiring higher accuracy than smaller models\n",
    "- Organizations with GPU resources but API budget constraints\n",
    "- Research applications requiring reproducible, deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T01:31:49.786403Z",
     "iopub.status.busy": "2025-08-30T01:31:49.786119Z",
     "iopub.status.idle": "2025-08-30T01:31:52.789797Z",
     "shell.execute_reply": "2025-08-30T01:31:52.789316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: cuda\n",
      "üíæ GPU memory: 25.3GB\n",
      "‚úÖ Environment ready for Method 2a: Llama 3.2 8b Local\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "import warnings\n",
    "import contextlib\n",
    "import io\n",
    "import logging\n",
    "# Suppress TensorFlow and CUDA initialization warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#from jtools import normalize_characters\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "print(f\"üíæ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\" if torch.cuda.is_available() else \"Using CPU\")\n",
    "print(\"‚úÖ Environment ready for Method 2a: Llama 3.2 8b Local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T01:31:52.822354Z",
     "iopub.status.busy": "2025-08-30T01:31:52.821838Z",
     "iopub.status.idle": "2025-08-30T01:31:52.831259Z",
     "shell.execute_reply": "2025-08-30T01:31:52.830864Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def remove_quotes(text):\n",
    "    \"\"\"Remove surrounding quotes from text\"\"\"\n",
    "    text = text.strip()\n",
    "    if (text.startswith(\"'\") and text.endswith(\"'\")) or (text.startswith('\"') and text.endswith('\"')):\n",
    "        return text[1:-1]\n",
    "    return text\n",
    "\n",
    "def clean_llama_response(response: str, field_type: str) -> str:\n",
    "    \"\"\"Clean up verbose Llama responses to extract just the content\"\"\"\n",
    "    response = response.strip()\n",
    "    \n",
    "    # Remove common verbose prefixes\n",
    "    prefixes_to_remove = [\n",
    "        \"The title of the poster is:\",\n",
    "        \"Here are the author names extracted in a comma-separated list:\",\n",
    "        \"Here is a 2-sentence summary of the poster:\",\n",
    "        \"Here are 5-6 keywords extracted from the poster:\",\n",
    "        \"Here are the methods mentioned in the poster:\",\n",
    "        \"Here are the main results extracted from the poster:\",\n",
    "        \"Here are the references found in the poster:\",\n",
    "        \"Here are the funding sources found:\",\n",
    "        \"Here is the conference information:\",\n",
    "        \"The title is:\",\n",
    "        \"Authors:\",\n",
    "        \"Summary:\",\n",
    "        \"Keywords:\",\n",
    "        \"Methods:\",\n",
    "        \"Results:\",\n",
    "        \"References:\",\n",
    "        \"Funding:\",\n",
    "        \"Conference:\"\n",
    "    ]\n",
    "    \n",
    "    for prefix in prefixes_to_remove:\n",
    "        if response.lower().startswith(prefix.lower()):\n",
    "            response = response[len(prefix):].strip()\n",
    "    \n",
    "    # Remove numbered lists (1., 2., etc.)\n",
    "    if field_type in ['keywords', 'methods', 'funding_sources']:\n",
    "        lines = response.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            # Remove numbering like \"1.\", \"2.\", \"*\", \"-\" at start of line\n",
    "            line = re.sub(r'^\\s*[\\d]+\\.\\s*', '', line)\n",
    "            line = re.sub(r'^\\s*[\\*\\-]\\s*', '', line)\n",
    "            if line.strip():\n",
    "                cleaned_lines.append(line.strip())\n",
    "        response = '\\n'.join(cleaned_lines) if field_type == 'methods' else ', '.join(cleaned_lines)\n",
    "    \n",
    "    # Remove quotes and extra whitespace\n",
    "    response = remove_quotes(response)\n",
    "    \n",
    "    return response.strip()\n",
    "def normalize_characters(text):\n",
    "    # Normalize Greek characters\n",
    "    greek_chars = ['Œ±', 'Œ≤', 'Œ≥', 'Œ¥', 'Œµ', 'Œ∂', 'Œ∑', 'Œ∏', 'Œπ', 'Œ∫', 'Œª', 'Œº', 'ŒΩ', 'Œæ', 'Œø', 'œÄ', 'œÅ', 'œÇ', 'œÉ', 'œÑ', 'œÖ', 'œÜ', 'œá', 'œà', 'œâ', 'Œë', 'Œí', 'Œì', 'Œî', 'Œï', 'Œñ', 'Œó', 'Œò', 'Œô', 'Œö', 'Œõ', 'Œú', 'Œù', 'Œû', 'Œü', 'Œ†', 'Œ°', 'Œ£', 'Œ§', 'Œ•', 'Œ¶', 'Œß', 'Œ®', 'Œ©']\n",
    "    for char in greek_chars:\n",
    "        text = text.replace(char, unicodedata.normalize('NFC', char))\n",
    "\n",
    "    # Normalize space characters\n",
    "    space_chars = ['\\xa0', '\\u2000', '\\u2001', '\\u2002', '\\u2003', '\\u2004', '\\u2005', '\\u2006', '\\u2007', '\\u2008', '\\u2009', '\\u200a', '\\u202f', '\\u205f', '\\u3000']\n",
    "    for space in space_chars:\n",
    "        text = text.replace(space, ' ')\n",
    "\n",
    "    # Normalize single quotes\n",
    "    single_quotes = ['‚Äò', '‚Äô', '‚Äõ', '‚Ä≤', '‚Äπ', '‚Ä∫', '‚Äö', '‚Äü']\n",
    "    for quote in single_quotes:\n",
    "        text = text.replace(quote, \"'\")\n",
    "\n",
    "    # Normalize double quotes\n",
    "    double_quotes = ['‚Äú', '‚Äù', '‚Äû', '‚Äü', '¬´', '¬ª', '„Äù', '„Äû', '„Äü', 'ÔºÇ']\n",
    "    for quote in double_quotes:\n",
    "        text = text.replace(quote, '\"')\n",
    "\n",
    "    # Normalize brackets\n",
    "    brackets = {\n",
    "        '„Äê': '[', '„Äë': ']',\n",
    "        'Ôºà': '(', 'Ôºâ': ')',\n",
    "        'ÔΩõ': '{', 'ÔΩù': '}',\n",
    "        '„Äö': '[', '„Äõ': ']',\n",
    "        '„Äà': '<', '„Äâ': '>',\n",
    "        '„Ää': '<', '„Äã': '>',\n",
    "        '„Äå': '[', '„Äç': ']',\n",
    "        '„Äé': '[', '„Äé': ']',\n",
    "        '„Äî': '[', '„Äï': ']',\n",
    "        '„Äñ': '[', '„Äó': ']'\n",
    "    }\n",
    "    for old, new in brackets.items():\n",
    "        text = text.replace(old, new)\n",
    "\n",
    "    # Normalize hyphens and dashes\n",
    "    hyphens_and_dashes = ['‚Äê', '‚Äë', '‚Äí', '‚Äì', '‚Äî', '‚Äï']\n",
    "    for dash in hyphens_and_dashes:\n",
    "        text = text.replace(dash, '-')\n",
    "\n",
    "    # Normalize line breaks\n",
    "    line_breaks = ['\\r\\n', '\\r']\n",
    "    for line_break in line_breaks:\n",
    "        text = text.replace(line_break, '\\n')\n",
    "\n",
    "    # Normalize superscripts and subscripts to normal numbers\n",
    "    superscripts = '‚Å∞¬π¬≤¬≥‚Å¥‚Åµ‚Å∂‚Å∑‚Å∏‚Åπ'\n",
    "    subscripts = '‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚Çâ'\n",
    "    normal_numbers = '0123456789'\n",
    "\n",
    "    for super_, sub_, normal in zip(superscripts, subscripts, normal_numbers):\n",
    "        text = text.replace(super_, normal).replace(sub_, normal)\n",
    "\n",
    "    # Remove or normalize any remaining special characters using the 'NFKD' method\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "    return remove_quotes(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T01:31:52.832689Z",
     "iopub.status.busy": "2025-08-30T01:31:52.832565Z",
     "iopub.status.idle": "2025-08-30T01:31:52.847013Z",
     "shell.execute_reply": "2025-08-30T01:31:52.846638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LlamaExtractor class defined\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        page_text = page.get_text()\n",
    "        if page_text:\n",
    "            text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
    "            text = normalize_characters(text)\n",
    "    doc.close()\n",
    "    return text.strip()\n",
    "\n",
    "class LlamaExtractor:\n",
    "    \"\"\"Llama 3.2 8B-Instruct based metadata extractor\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
    "        print(f\"üì• Loading {model_name}...\")\n",
    "        \n",
    "        # Load tokenizer with stderr suppression\n",
    "        with contextlib.redirect_stderr(io.StringIO()):\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model with quantization if CUDA available\n",
    "        if torch.cuda.is_available():\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                bnb_8bit_compute_dtype=torch.float16\n",
    "            )\n",
    "            \n",
    "            # Load model with stderr suppression\n",
    "            with contextlib.redirect_stderr(io.StringIO()):\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    quantization_config=bnb_config,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "        else:\n",
    "            # CPU loading\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float32\n",
    "            )\n",
    "            device = torch.device(\"cpu\")\n",
    "            self.model = self.model.to(device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(f\"‚úÖ Model loaded successfully\")\n",
    "    \n",
    "    def extract_field(self, text: str, field: str) -> Any:\n",
    "        \"\"\"Extract specific field using optimized prompts for clean output\"\"\"\n",
    "        \n",
    "        # More explicit prompts that discourage verbose responses\n",
    "        prompts = {\n",
    "            'title': f\"\"\"Extract only the title from this poster text. Provide just the title text, nothing else.\n",
    "\n",
    "Text: \"{text[:500]}\"\n",
    "\n",
    "Title:\"\"\",\n",
    "            \n",
    "            'authors': f\"\"\"Extract author names and affiliations from this poster. Format as: \"Name1 (Institution1) | Name2 (Institution2)\" or just names if no affiliations found.\n",
    "\n",
    "Text: \"{text[:600]}\"\n",
    "\n",
    "Authors:\"\"\",\n",
    "            \n",
    "            'summary': f\"\"\"Write a concise 2-sentence summary of this poster's research. Be direct and factual.\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Summary:\"\"\",\n",
    "            \n",
    "            'keywords': f\"\"\"Extract 5-6 key technical terms from this poster. List only the keywords separated by commas.\n",
    "\n",
    "Text: \"{text[:600]}\"\n",
    "\n",
    "Keywords:\"\"\",\n",
    "            \n",
    "            'methods': f\"\"\"Extract the research methods described in this poster. Be concise and specific.\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Methods:\"\"\",\n",
    "            \n",
    "            'results': f\"\"\"Extract the main research findings from this poster. Include specific numbers/measurements if present.\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Results:\"\"\",\n",
    "            \n",
    "            'references': f\"\"\"Extract references or citations from this poster. Format as: \"Title1 (Authors, Year, Journal) | Title2 (Authors, Year, Journal)\" or \"None found\" if no references.\n",
    "\n",
    "Text: \"{text[:1000]}\"\n",
    "\n",
    "References:\"\"\",\n",
    "            \n",
    "            'funding_sources': f\"\"\"Extract funding sources, grants, or acknowledgments from this poster. List funding agencies or grant numbers separated by commas, or \"None found\".\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Funding:\"\"\",\n",
    "            \n",
    "            'conference_info': f\"\"\"Extract conference information from this poster. Format as: \"Location: City, Country | Date: date range\" or \"None found\" if not mentioned.\n",
    "\n",
    "Text: \"{text[:600]}\"\n",
    "\n",
    "Conference:\"\"\"\n",
    "        }\n",
    "        \n",
    "        if field not in prompts:\n",
    "            return \"\"\n",
    "        \n",
    "        prompt = prompts[field]\n",
    "        \n",
    "        # Create chat template with explicit instructions for conciseness\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise data extraction assistant. Provide only the requested information without explanatory text, prefixes, or formatting. Be direct and concise.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text_input = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text_input,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # Generate with greedy decoding for deterministic output\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                do_sample=False,     # Greedy decoding = deterministic (most probable token)\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1  # Prevent repetition\n",
    "                # Note: temperature/top_p not needed with do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Clean up the response\n",
    "        response = clean_llama_response(response, field)\n",
    "        \n",
    "        # Parse response based on field\n",
    "        if field == \"authors\":\n",
    "            authors = []\n",
    "            # Handle both simple names and \"Name (Institution)\" format\n",
    "            if \"|\" in response:\n",
    "                # Format: \"Name1 (Institution1) | Name2 (Institution2)\"\n",
    "                author_parts = response.split(\"|\")\n",
    "            else:\n",
    "                # Format: \"Name1, Name2, Name3\" or \"Name1 (Inst1), Name2 (Inst2)\"\n",
    "                author_parts = response.split(\",\")\n",
    "            \n",
    "            for author_part in author_parts:\n",
    "                author_part = author_part.strip()\n",
    "                if not author_part:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if has institution in parentheses\n",
    "                if \"(\" in author_part and \")\" in author_part:\n",
    "                    name_part = author_part.split(\"(\")[0].strip()\n",
    "                    affil_part = author_part.split(\"(\")[1].split(\")\")[0].strip()\n",
    "                    authors.append({\n",
    "                        \"name\": name_part,\n",
    "                        \"affiliations\": [affil_part],\n",
    "                        \"email\": None\n",
    "                    })\n",
    "                else:\n",
    "                    authors.append({\n",
    "                        \"name\": author_part,\n",
    "                        \"affiliations\": [],\n",
    "                        \"email\": None\n",
    "                    })\n",
    "            return authors[:6]  # Limit to 6\n",
    "            \n",
    "        elif field == \"keywords\":\n",
    "            keywords = [k.strip() for k in response.split(\",\") if k.strip()]\n",
    "            return keywords[:8]  # Limit to 8\n",
    "            \n",
    "        elif field == \"references\":\n",
    "            if response.lower() == \"none found\" or not response.strip():\n",
    "                return []\n",
    "            \n",
    "            references = []\n",
    "            # Handle format: \"Title1 (Authors, Year, Journal) | Title2 (Authors, Year, Journal)\"\n",
    "            if \"|\" in response:\n",
    "                ref_parts = response.split(\"|\")\n",
    "            else:\n",
    "                ref_parts = [response]\n",
    "                \n",
    "            for ref_part in ref_parts:\n",
    "                ref_part = ref_part.strip()\n",
    "                if not ref_part or ref_part.lower() == \"none found\":\n",
    "                    continue\n",
    "                    \n",
    "                # Try to parse \"Title (Authors, Year, Journal)\" format\n",
    "                if \"(\" in ref_part and \")\" in ref_part:\n",
    "                    title = ref_part.split(\"(\")[0].strip()\n",
    "                    details = ref_part.split(\"(\")[1].split(\")\")[0].strip()\n",
    "                    \n",
    "                    # Split details by comma and try to extract year\n",
    "                    detail_parts = [d.strip() for d in details.split(\",\")]\n",
    "                    year = None\n",
    "                    journal = \"\"\n",
    "                    authors = \"\"\n",
    "                    \n",
    "                    for part in detail_parts:\n",
    "                        if part.isdigit() and len(part) == 4:  # Likely a year\n",
    "                            year = int(part)\n",
    "                        elif year is None:\n",
    "                            authors = part if not authors else authors + \", \" + part\n",
    "                        else:\n",
    "                            journal = part if not journal else journal + \", \" + part\n",
    "                    \n",
    "                    references.append({\n",
    "                        \"title\": title,\n",
    "                        \"authors\": authors,\n",
    "                        \"year\": year,\n",
    "                        \"journal\": journal\n",
    "                    })\n",
    "                else:\n",
    "                    # Fallback: treat whole thing as title\n",
    "                    references.append({\n",
    "                        \"title\": ref_part,\n",
    "                        \"authors\": \"\",\n",
    "                        \"year\": None,\n",
    "                        \"journal\": \"\"\n",
    "                    })\n",
    "            return references[:5]  # Limit to 5\n",
    "            \n",
    "        elif field == \"funding_sources\":\n",
    "            if response.lower() == \"none found\" or not response.strip():\n",
    "                return []\n",
    "            \n",
    "            funding = [f.strip() for f in response.split(\",\") if f.strip() and f.strip().lower() != \"none found\"]\n",
    "            return funding[:5]  # Limit to 5\n",
    "            \n",
    "        elif field == \"conference_info\":\n",
    "            if response.lower() == \"none found\" or not response.strip():\n",
    "                return {\"location\": None, \"date\": None}\n",
    "            \n",
    "            location = None\n",
    "            date = None\n",
    "            \n",
    "            # Handle format: \"Location: City, Country | Date: date range\"\n",
    "            if \"|\" in response:\n",
    "                parts = response.split(\"|\")\n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    if part.lower().startswith(\"location:\"):\n",
    "                        location = part.split(\":\", 1)[1].strip()\n",
    "                    elif part.lower().startswith(\"date:\"):\n",
    "                        date = part.split(\":\", 1)[1].strip()\n",
    "            else:\n",
    "                # Try to detect location/date in single string\n",
    "                if any(word in response.lower() for word in [\"location\", \"city\", \"country\"]):\n",
    "                    location = response.strip()\n",
    "                elif any(word in response.lower() for word in [\"date\", \"may\", \"june\", \"july\", \"august\", \"september\"]):\n",
    "                    date = response.strip()\n",
    "                else:\n",
    "                    # Assume it's location if no clear indicator\n",
    "                    location = response.strip()\n",
    "            \n",
    "            return {\"location\": location, \"date\": date}\n",
    "        else:\n",
    "            return response\n",
    "\n",
    "print(\"‚úÖ LlamaExtractor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T01:31:52.848392Z",
     "iopub.status.busy": "2025-08-30T01:31:52.848099Z",
     "iopub.status.idle": "2025-08-30T01:32:28.773262Z",
     "shell.execute_reply": "2025-08-30T01:32:28.772841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Method 2a: Llama 3.2 8B Local Extraction\n",
      "============================================================\n",
      "üìè Extracted 3732 characters\n",
      "ü§ñ Initializing Llama 3.2 8B model...\n",
      "üì• Loading meta-llama/Meta-Llama-3-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756517514.853929 2450263 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756517514.859618 2450263 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756517514.875174 2450263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756517514.875192 2450263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756517514.875194 2450263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756517514.875196 2450263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7807a7fe2c2546ecbfb97f47eaddc092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully\n",
      "üîç Extracting metadata components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ TITLE: INFLUENCE OF DRUG-POLYMER INTERACTIONS ON RELEASE KINETICS OF PLGA AND PLA/PEG NPS\n",
      "\n",
      "üë• AUTHORS: 6 found\n",
      "   ‚Ä¢ Merve Gul (University of Pavia\n",
      "   ‚Ä¢ Department of Chemical Engineering, Universitat PoliteÃÄcnica de Catalunya)\n",
      "   ‚Ä¢ Ida Genta (University of Pavia)\n",
      "   ‚Ä¢ Maria M. Perez Madrigal (Universitat PoliteÃÄcnica de Catalunya)\n",
      "   ‚Ä¢ Carlos Aleman (Universitat PoliteÃÄcnica de Catalunya\n",
      "   ‚Ä¢ Barcelona Research Center for Multiscale Science and Engineering)\n",
      "\n",
      "üìù SUMMARY: The study investigates the influence of drug-polymer interactions on release kinetics of poly(lactic...\n",
      "\n",
      "üîë KEYWORDS: PLGA, PLA, PEG, NPS, AMR\n",
      "\n",
      "üî¨ METHODS: ‚Ä¢ Microfluidic-based synthesis of nano-sized carriers for drug delivery systems (NDDS)...\n",
      "\n",
      "üìä RESULTS: ‚Ä¢ The release kinetics of PLGA and PLA/PEG NPs were influenced by drug-polymer interactions.\n",
      "‚Ä¢ The e...\n",
      "\n",
      "üìö REFERENCES: 2 found\n",
      "   ‚Ä¢ ...\n",
      "   ‚Ä¢ ...\n",
      "\n",
      "üí∞ FUNDING: 1 sources\n",
      "   ‚Ä¢ None found....\n",
      "\n",
      "üèõÔ∏è  CONFERENCE: None | None\n",
      "‚è±Ô∏è  Processing time: 35.92s\n",
      "üíæ Results saved to: ../output/method2a_llama_results.json\n",
      "‚úÖ Method 2a (Llama) completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Run extraction\n",
    "pdf_path = \"../data/test-poster.pdf\"\n",
    "\n",
    "if Path(pdf_path).exists():\n",
    "    print(\"üöÄ Running Method 2a: Llama 3.2 8B Local Extraction\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract text\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"üìè Extracted {len(text)} characters\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize extractor\n",
    "        print(\"ü§ñ Initializing Llama 3.2 8B model...\")\n",
    "        extractor = LlamaExtractor()\n",
    "        \n",
    "        # Extract each field\n",
    "        print(\"üîç Extracting metadata components...\")\n",
    "        \n",
    "        title = extractor.extract_field(text, \"title\")\n",
    "        authors = extractor.extract_field(text, \"authors\")\n",
    "        summary = extractor.extract_field(text, \"summary\")\n",
    "        keywords = extractor.extract_field(text, \"keywords\")\n",
    "        methods = extractor.extract_field(text, \"methods\")\n",
    "        results_text = extractor.extract_field(text, \"results\")\n",
    "        references = extractor.extract_field(text, \"references\")\n",
    "        funding_sources = extractor.extract_field(text, \"funding_sources\")\n",
    "        conference_info = extractor.extract_field(text, \"conference_info\")\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"summary\": summary,\n",
    "            \"keywords\": keywords,\n",
    "            \"methods\": methods,\n",
    "            \"results\": results_text,\n",
    "            \"references\": references,\n",
    "            \"funding_sources\": funding_sources,\n",
    "            \"conference_info\": conference_info,\n",
    "            \"extraction_metadata\": {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"processing_time\": time.time() - start_time,\n",
    "                \"method\": \"llama_local\",\n",
    "                \"model\": \"Meta-Llama-3-8B-Instruct\",\n",
    "                \"device\": str(next(extractor.model.parameters()).device),\n",
    "                \"text_length\": len(text),\n",
    "                \"do_sample\": False,\n",
    "                \"max_tokens\": 150\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüìÑ TITLE: {results['title'][:100]}\")\n",
    "        print(f\"\\nüë• AUTHORS: {len(results['authors'])} found\")\n",
    "        for author in results[\"authors\"]:\n",
    "            affil_str = f\" ({', '.join(author['affiliations'])})\" if author['affiliations'] else \"\"\n",
    "            print(f\"   ‚Ä¢ {author['name']}{affil_str}\")\n",
    "        \n",
    "        print(f\"\\nüìù SUMMARY: {results['summary'][:100]}...\")\n",
    "        print(f\"\\nüîë KEYWORDS: {', '.join(results['keywords'][:5])}\")\n",
    "        print(f\"\\nüî¨ METHODS: {results['methods'][:100]}...\")\n",
    "        print(f\"\\nüìä RESULTS: {results['results'][:100]}...\")\n",
    "        print(f\"\\nüìö REFERENCES: {len(results['references'])} found\")\n",
    "        for ref in results['references'][:2]:  # Show first 2\n",
    "            print(f\"   ‚Ä¢ {ref['title'][:50]}...\")\n",
    "        print(f\"\\nüí∞ FUNDING: {len(results['funding_sources'])} sources\")\n",
    "        for funding in results['funding_sources'][:2]:  # Show first 2\n",
    "            print(f\"   ‚Ä¢ {funding[:50]}...\")\n",
    "        print(f\"\\nüèõÔ∏è  CONFERENCE: {results['conference_info']['location']} | {results['conference_info']['date']}\")\n",
    "        print(f\"‚è±Ô∏è  Processing time: {results['extraction_metadata']['processing_time']:.2f}s\")\n",
    "        \n",
    "        # Save results with corrected filename\n",
    "        output_path = Path(\"../output/method2a_llama_results.json\")\n",
    "        output_path.parent.mkdir(exist_ok=True)\n",
    "        \n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Results saved to: {output_path}\")\n",
    "        print(\"‚úÖ Method 2a (Llama) completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Llama extraction failed: {e}\")\n",
    "        print(\"   This may be due to insufficient GPU memory or model download issues\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Test poster not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1b4afa676bf7477da1f84b6a19e15658": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "51b9acd4b3b84595b8430692ac790fe9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "694ef8cccad645839859dae8920d8934": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6c61151d10344186b8d0d56f9120c94c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_880ca6e37df84f66ba6478744b71260d",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_1b4afa676bf7477da1f84b6a19e15658",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
      }
     },
     "7807a7fe2c2546ecbfb97f47eaddc092": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6c61151d10344186b8d0d56f9120c94c",
        "IPY_MODEL_e3c21f2810384a729d4849effc282b7a",
        "IPY_MODEL_7e1d57a062bc4b12968825e80498d81c"
       ],
       "layout": "IPY_MODEL_51b9acd4b3b84595b8430692ac790fe9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7b6014dba22845c4936bbfb6f5de623b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7c086526b1e1442a8b09502cd8d4068a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e1d57a062bc4b12968825e80498d81c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7b6014dba22845c4936bbfb6f5de623b",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_694ef8cccad645839859dae8920d8934",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá4/4‚Äá[00:05&lt;00:00,‚Äá‚Äá1.12s/it]"
      }
     },
     "880ca6e37df84f66ba6478744b71260d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca34966dc9674a37a9a5d31b3c3a7042": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e3c21f2810384a729d4849effc282b7a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7c086526b1e1442a8b09502cd8d4068a",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ca34966dc9674a37a9a5d31b3c3a7042",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
