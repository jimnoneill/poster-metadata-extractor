{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2a: Llama 3.2 8B Local Extraction\n",
    "\n",
    "## Overview\n",
    "Local large language model for high-quality poster metadata extraction. Runs entirely on your hardware without API dependencies.\n",
    "\n",
    "## Accuracy Note\n",
    "The accuracy estimate is unvalidated - based on limited testing only. Actual accuracy must be determined through proper Cochran sampling validation before production use.\n",
    "\n",
    "## Performance Characteristics\n",
    "- **Estimated Accuracy**: 85-90% (unvalidated - requires Cochran sampling validation)\n",
    "- **Cost**: $0 (runs locally, only electricity costs)\n",
    "- **Speed**: 15-45 seconds per poster (single), ~2-3s per poster (RTX 4090 batched)\n",
    "- **Hallucination Risk**: Very Low (structured prompting + greedy decoding)\n",
    "- **Setup**: Medium-Complex - requires model download and 8GB+ VRAM\n",
    "\n",
    "## Hardware Requirements\n",
    "- **GPU**: 8GB+ VRAM (RTX 4090 recommended)\n",
    "- **RAM**: 16GB+ system memory\n",
    "- **Storage**: ~16GB for model files\n",
    "\n",
    "## Best For\n",
    "- Privacy-sensitive environments requiring higher accuracy than smaller models\n",
    "- Organizations with GPU resources but API budget constraints\n",
    "- Research applications requiring reproducible, deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T01:53:42.362706Z",
     "iopub.status.busy": "2025-08-30T01:53:42.362409Z",
     "iopub.status.idle": "2025-08-30T01:53:45.360766Z",
     "shell.execute_reply": "2025-08-30T01:53:45.360331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️  Using device: cuda\n",
      "💾 GPU memory: 25.3GB\n",
      "✅ Environment ready for Method 2a: Llama 3.2 8b Local\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "import warnings\n",
    "import contextlib\n",
    "import io\n",
    "import logging\n",
    "# Suppress TensorFlow and CUDA initialization warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#from jtools import normalize_characters\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🖥️  Using device: {device}\")\n",
    "print(f\"💾 GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\" if torch.cuda.is_available() else \"Using CPU\")\n",
    "print(\"✅ Environment ready for Method 2a: Llama 3.2 8b Local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T01:53:45.394281Z",
     "iopub.status.busy": "2025-08-30T01:53:45.393920Z",
     "iopub.status.idle": "2025-08-30T01:53:45.402954Z",
     "shell.execute_reply": "2025-08-30T01:53:45.402645Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def remove_quotes(text):\n",
    "    \"\"\"Remove surrounding quotes from text\"\"\"\n",
    "    text = text.strip()\n",
    "    if (text.startswith(\"'\") and text.endswith(\"'\")) or (text.startswith('\"') and text.endswith('\"')):\n",
    "        return text[1:-1]\n",
    "    return text\n",
    "\n",
    "def clean_llama_response(response: str, field_type: str) -> str:\n",
    "    \"\"\"Clean up verbose Llama responses to extract just the content\"\"\"\n",
    "    response = response.strip()\n",
    "    \n",
    "    # Remove common verbose prefixes\n",
    "    prefixes_to_remove = [\n",
    "        \"The title of the poster is:\",\n",
    "        \"Here are the author names extracted in a comma-separated list:\",\n",
    "        \"Here is a 2-sentence summary of the poster:\",\n",
    "        \"Here are 5-6 keywords extracted from the poster:\",\n",
    "        \"Here are the methods mentioned in the poster:\",\n",
    "        \"Here are the main results extracted from the poster:\",\n",
    "        \"Here are the references found in the poster:\",\n",
    "        \"Here are the funding sources found:\",\n",
    "        \"Here is the conference information:\",\n",
    "        \"The title is:\",\n",
    "        \"Authors:\",\n",
    "        \"Summary:\",\n",
    "        \"Keywords:\",\n",
    "        \"Methods:\",\n",
    "        \"Results:\",\n",
    "        \"References:\",\n",
    "        \"Funding:\",\n",
    "        \"Conference:\"\n",
    "    ]\n",
    "    \n",
    "    for prefix in prefixes_to_remove:\n",
    "        if response.lower().startswith(prefix.lower()):\n",
    "            response = response[len(prefix):].strip()\n",
    "    \n",
    "    # Remove numbered lists (1., 2., etc.)\n",
    "    if field_type in ['keywords', 'methods', 'funding_sources']:\n",
    "        lines = response.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            # Remove numbering like \"1.\", \"2.\", \"*\", \"-\" at start of line\n",
    "            line = re.sub(r'^\\s*[\\d]+\\.\\s*', '', line)\n",
    "            line = re.sub(r'^\\s*[\\*\\-]\\s*', '', line)\n",
    "            if line.strip():\n",
    "                cleaned_lines.append(line.strip())\n",
    "        response = '\\n'.join(cleaned_lines) if field_type == 'methods' else ', '.join(cleaned_lines)\n",
    "    \n",
    "    # Remove quotes and extra whitespace\n",
    "    response = remove_quotes(response)\n",
    "    \n",
    "    return response.strip()\n",
    "def normalize_characters(text):\n",
    "    # Normalize Greek characters\n",
    "    greek_chars = ['α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ψ', 'ω', 'Α', 'Β', 'Γ', 'Δ', 'Ε', 'Ζ', 'Η', 'Θ', 'Ι', 'Κ', 'Λ', 'Μ', 'Ν', 'Ξ', 'Ο', 'Π', 'Ρ', 'Σ', 'Τ', 'Υ', 'Φ', 'Χ', 'Ψ', 'Ω']\n",
    "    for char in greek_chars:\n",
    "        text = text.replace(char, unicodedata.normalize('NFC', char))\n",
    "\n",
    "    # Normalize space characters\n",
    "    space_chars = ['\\xa0', '\\u2000', '\\u2001', '\\u2002', '\\u2003', '\\u2004', '\\u2005', '\\u2006', '\\u2007', '\\u2008', '\\u2009', '\\u200a', '\\u202f', '\\u205f', '\\u3000']\n",
    "    for space in space_chars:\n",
    "        text = text.replace(space, ' ')\n",
    "\n",
    "    # Normalize single quotes\n",
    "    single_quotes = ['‘', '’', '‛', '′', '‹', '›', '‚', '‟']\n",
    "    for quote in single_quotes:\n",
    "        text = text.replace(quote, \"'\")\n",
    "\n",
    "    # Normalize double quotes\n",
    "    double_quotes = ['“', '”', '„', '‟', '«', '»', '〝', '〞', '〟', '＂']\n",
    "    for quote in double_quotes:\n",
    "        text = text.replace(quote, '\"')\n",
    "\n",
    "    # Normalize brackets\n",
    "    brackets = {\n",
    "        '【': '[', '】': ']',\n",
    "        '（': '(', '）': ')',\n",
    "        '｛': '{', '｝': '}',\n",
    "        '〚': '[', '〛': ']',\n",
    "        '〈': '<', '〉': '>',\n",
    "        '《': '<', '》': '>',\n",
    "        '「': '[', '」': ']',\n",
    "        '『': '[', '『': ']',\n",
    "        '〔': '[', '〕': ']',\n",
    "        '〖': '[', '〗': ']'\n",
    "    }\n",
    "    for old, new in brackets.items():\n",
    "        text = text.replace(old, new)\n",
    "\n",
    "    # Normalize hyphens and dashes\n",
    "    hyphens_and_dashes = ['‐', '‑', '‒', '–', '—', '―']\n",
    "    for dash in hyphens_and_dashes:\n",
    "        text = text.replace(dash, '-')\n",
    "\n",
    "    # Normalize line breaks\n",
    "    line_breaks = ['\\r\\n', '\\r']\n",
    "    for line_break in line_breaks:\n",
    "        text = text.replace(line_break, '\\n')\n",
    "\n",
    "    # Normalize superscripts and subscripts to normal numbers\n",
    "    superscripts = '⁰¹²³⁴⁵⁶⁷⁸⁹'\n",
    "    subscripts = '₀₁₂₃₄₅₆₇₈₉'\n",
    "    normal_numbers = '0123456789'\n",
    "\n",
    "    for super_, sub_, normal in zip(superscripts, subscripts, normal_numbers):\n",
    "        text = text.replace(super_, normal).replace(sub_, normal)\n",
    "\n",
    "    # Remove or normalize any remaining special characters using the 'NFKD' method\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "    return remove_quotes(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T01:53:45.404375Z",
     "iopub.status.busy": "2025-08-30T01:53:45.404253Z",
     "iopub.status.idle": "2025-08-30T01:53:45.419419Z",
     "shell.execute_reply": "2025-08-30T01:53:45.419106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LlamaExtractor class defined\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        page_text = page.get_text()\n",
    "        if page_text:\n",
    "            text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    # Apply normalize_characters to the ENTIRE extracted text\n",
    "    text = normalize_characters(text)\n",
    "    return text.strip()\n",
    "\n",
    "class LlamaExtractor:\n",
    "    \"\"\"Llama 3.2 8B-Instruct based metadata extractor\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
    "        print(f\"📥 Loading {model_name}...\")\n",
    "        \n",
    "        # Load tokenizer with stderr suppression\n",
    "        with contextlib.redirect_stderr(io.StringIO()):\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model with quantization if CUDA available\n",
    "        if torch.cuda.is_available():\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                bnb_8bit_compute_dtype=torch.float16\n",
    "            )\n",
    "            \n",
    "            # Load model with stderr suppression\n",
    "            with contextlib.redirect_stderr(io.StringIO()):\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    quantization_config=bnb_config,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "        else:\n",
    "            # CPU loading\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float32\n",
    "            )\n",
    "            device = torch.device(\"cpu\")\n",
    "            self.model = self.model.to(device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(f\"✅ Model loaded successfully\")\n",
    "    \n",
    "    def extract_field(self, text: str, field: str) -> Any:\n",
    "        \"\"\"Extract specific field using optimized prompts for clean output\"\"\"\n",
    "        \n",
    "        # More explicit prompts that discourage verbose responses\n",
    "        prompts = {\n",
    "            'title': f\"\"\"Extract only the title from this poster text. Provide just the title text, nothing else.\n",
    "\n",
    "Text: \"{text[:500]}\"\n",
    "\n",
    "Title:\"\"\",\n",
    "            \n",
    "            'authors': f\"\"\"Extract ONLY the author names from this poster. List FULL NAMES (first and last name) separated by commas. Do NOT include departments, institutions, or affiliations in the names.\n",
    "\n",
    "Examples of correct format:\n",
    "- \"John Smith, Mary Johnson, David Wilson\"\n",
    "- \"Maria Garcia, Robert Chen\"\n",
    "\n",
    "Do NOT include words like \"Department\", \"University\", \"Center\", \"Institute\" as part of author names.\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Author Names:\"\"\",\n",
    "            \n",
    "            'summary': f\"\"\"Write a concise 2-sentence summary of this poster's research. Be direct and factual.\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Summary:\"\"\",\n",
    "            \n",
    "            'keywords': f\"\"\"Extract 5-6 key technical terms from this poster. List only the keywords separated by commas.\n",
    "\n",
    "Text: \"{text[:600]}\"\n",
    "\n",
    "Keywords:\"\"\",\n",
    "            \n",
    "            'methods': f\"\"\"Extract the research methods described in this poster. Be concise and specific.\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Methods:\"\"\",\n",
    "            \n",
    "            'results': f\"\"\"Extract the main research findings from this poster. Include specific numbers/measurements if present.\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Results:\"\"\",\n",
    "            \n",
    "            'references': f\"\"\"Extract references or citations from this poster. Format as: \"Title1 (Authors, Year, Journal) | Title2 (Authors, Year, Journal)\" or \"None found\" if no references.\n",
    "\n",
    "Text: \"{text[:1000]}\"\n",
    "\n",
    "References:\"\"\",\n",
    "            \n",
    "            'funding_sources': f\"\"\"Extract funding sources, grants, or acknowledgments from this poster. List funding agencies or grant numbers separated by commas, or \"None found\".\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Funding:\"\"\",\n",
    "            \n",
    "            'conference_info': f\"\"\"Extract conference information from this poster. Format as: \"Location: City, Country | Date: date range\" or \"None found\" if not mentioned.\n",
    "\n",
    "Text: \"{text[:600]}\"\n",
    "\n",
    "Conference:\"\"\"\n",
    "        }\n",
    "        \n",
    "        if field not in prompts:\n",
    "            return \"\"\n",
    "        \n",
    "        prompt = prompts[field]\n",
    "        \n",
    "        # Create chat template with explicit instructions for conciseness\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise data extraction assistant. Provide only the requested information without explanatory text, prefixes, or formatting. Be direct and concise.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text_input = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text_input,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # Generate with greedy decoding for deterministic output\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                do_sample=False,     # Greedy decoding = deterministic (most probable token)\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1  # Prevent repetition\n",
    "                # Note: temperature/top_p not needed with do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Clean up the response\n",
    "        response = clean_llama_response(response, field)\n",
    "        \n",
    "        # Parse response based on field\n",
    "        if field == \"authors\":\n",
    "            authors = []\n",
    "            # Clean response should be comma-separated names only\n",
    "            author_names = [name.strip() for name in response.split(\",\") if name.strip()]\n",
    "            \n",
    "            # Filter out non-author names (departments, institutions, etc.)\n",
    "            filtered_names = []\n",
    "            for name in author_names:\n",
    "                # Skip if it contains institutional keywords\n",
    "                institutional_keywords = [\n",
    "                    'department', 'university', 'institute', 'center', 'centre', \n",
    "                    'school', 'college', 'laboratory', 'lab', 'division',\n",
    "                    'research', 'faculty', 'hospital', 'clinic'\n",
    "                ]\n",
    "                \n",
    "                # Convert to lowercase for checking\n",
    "                name_lower = name.lower()\n",
    "                \n",
    "                # Skip if it's clearly an institution\n",
    "                if any(keyword in name_lower for keyword in institutional_keywords):\n",
    "                    continue\n",
    "                \n",
    "                # Skip if it starts with prepositions or looks like an affiliation\n",
    "                if (name_lower.startswith(('of ', 'for ', 'and ', 'the ')) or\n",
    "                    len(name.split()) > 4):  # Names shouldn't be too long\n",
    "                    continue\n",
    "                \n",
    "                # Clean up any remaining parentheses from the name\n",
    "                name = name.split('(')[0].strip()\n",
    "                name = name.split(')')[0].strip()\n",
    "                \n",
    "                if name:\n",
    "                    filtered_names.append(name)\n",
    "            \n",
    "            # Create author objects with empty affiliations (we'll extract those separately if needed)\n",
    "            for name in filtered_names[:6]:  # Limit to 6 authors\n",
    "                authors.append({\n",
    "                    \"name\": name,\n",
    "                    \"affiliations\": [],\n",
    "                    \"email\": None\n",
    "                })\n",
    "            \n",
    "            return authors\n",
    "            \n",
    "        elif field == \"keywords\":\n",
    "            keywords = [k.strip() for k in response.split(\",\") if k.strip()]\n",
    "            return keywords[:8]  # Limit to 8\n",
    "            \n",
    "        elif field == \"references\":\n",
    "            if response.lower() == \"none found\" or not response.strip():\n",
    "                return []\n",
    "            \n",
    "            references = []\n",
    "            # Handle format: \"Title1 (Authors, Year, Journal) | Title2 (Authors, Year, Journal)\"\n",
    "            if \"|\" in response:\n",
    "                ref_parts = response.split(\"|\")\n",
    "            else:\n",
    "                ref_parts = [response]\n",
    "                \n",
    "            for ref_part in ref_parts:\n",
    "                ref_part = ref_part.strip()\n",
    "                if not ref_part or ref_part.lower() == \"none found\":\n",
    "                    continue\n",
    "                    \n",
    "                # Try to parse \"Title (Authors, Year, Journal)\" format\n",
    "                if \"(\" in ref_part and \")\" in ref_part:\n",
    "                    title = ref_part.split(\"(\")[0].strip()\n",
    "                    details = ref_part.split(\"(\")[1].split(\")\")[0].strip()\n",
    "                    \n",
    "                    # Split details by comma and try to extract year\n",
    "                    detail_parts = [d.strip() for d in details.split(\",\")]\n",
    "                    year = None\n",
    "                    journal = \"\"\n",
    "                    authors = \"\"\n",
    "                    \n",
    "                    for part in detail_parts:\n",
    "                        if part.isdigit() and len(part) == 4:  # Likely a year\n",
    "                            year = int(part)\n",
    "                        elif year is None:\n",
    "                            authors = part if not authors else authors + \", \" + part\n",
    "                        else:\n",
    "                            journal = part if not journal else journal + \", \" + part\n",
    "                    \n",
    "                    references.append({\n",
    "                        \"title\": title,\n",
    "                        \"authors\": authors,\n",
    "                        \"year\": year,\n",
    "                        \"journal\": journal\n",
    "                    })\n",
    "                else:\n",
    "                    # Fallback: treat whole thing as title\n",
    "                    references.append({\n",
    "                        \"title\": ref_part,\n",
    "                        \"authors\": \"\",\n",
    "                        \"year\": None,\n",
    "                        \"journal\": \"\"\n",
    "                    })\n",
    "            return references[:5]  # Limit to 5\n",
    "            \n",
    "        elif field == \"funding_sources\":\n",
    "            if response.lower() == \"none found\" or not response.strip():\n",
    "                return []\n",
    "            \n",
    "            funding = [f.strip() for f in response.split(\",\") if f.strip() and f.strip().lower() != \"none found\"]\n",
    "            return funding[:5]  # Limit to 5\n",
    "            \n",
    "        elif field == \"conference_info\":\n",
    "            if response.lower() == \"none found\" or not response.strip():\n",
    "                return {\"location\": None, \"date\": None}\n",
    "            \n",
    "            location = None\n",
    "            date = None\n",
    "            \n",
    "            # Handle format: \"Location: City, Country | Date: date range\"\n",
    "            if \"|\" in response:\n",
    "                parts = response.split(\"|\")\n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    if part.lower().startswith(\"location:\"):\n",
    "                        location = part.split(\":\", 1)[1].strip()\n",
    "                    elif part.lower().startswith(\"date:\"):\n",
    "                        date = part.split(\":\", 1)[1].strip()\n",
    "            else:\n",
    "                # Try to detect location/date in single string\n",
    "                if any(word in response.lower() for word in [\"location\", \"city\", \"country\"]):\n",
    "                    location = response.strip()\n",
    "                elif any(word in response.lower() for word in [\"date\", \"may\", \"june\", \"july\", \"august\", \"september\"]):\n",
    "                    date = response.strip()\n",
    "                else:\n",
    "                    # Assume it's location if no clear indicator\n",
    "                    location = response.strip()\n",
    "            \n",
    "            return {\"location\": location, \"date\": date}\n",
    "        else:\n",
    "            return response\n",
    "\n",
    "print(\"✅ LlamaExtractor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T01:53:45.420790Z",
     "iopub.status.busy": "2025-08-30T01:53:45.420553Z",
     "iopub.status.idle": "2025-08-30T01:54:14.925929Z",
     "shell.execute_reply": "2025-08-30T01:54:14.925432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running Method 2a: Llama 3.2 8B Local Extraction\n",
      "============================================================\n",
      "📏 Extracted 3732 characters\n",
      "🤖 Initializing Llama 3.2 8B model...\n",
      "📥 Loading meta-llama/Meta-Llama-3-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756518827.475614 2459311 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756518827.481348 2459311 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756518827.497305 2459311 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756518827.497329 2459311 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756518827.497331 2459311 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756518827.497333 2459311 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d91396cec6a45aa84db71ac043ce1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully\n",
      "🔍 Extracting metadata components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 TITLE: INFLUENCE OF DRUG-POLYMER INTERACTIONS ON RELEASE KINETICS OF PLGA AND PLA/PEG NPS\n",
      "\n",
      "👥 AUTHORS: 5 found\n",
      "   • Merve Gul\n",
      "   • Ida Genta\n",
      "   • Maria M. Perez Madrigal\n",
      "   • Carlos Aleman\n",
      "   • Enrica Chiesa\n",
      "\n",
      "📝 SUMMARY: The study investigates the influence of drug-polymer interactions on release kinetics of poly(lactic...\n",
      "\n",
      "🔑 KEYWORDS: PLGA, PLA, PEG, NPS, AMR\n",
      "\n",
      "🔬 METHODS: • Microfluidic-based synthesis of nano-sized carriers for drug delivery systems (NDDS)...\n",
      "\n",
      "📊 RESULTS: • The release kinetics of PLGA and PLA/PEG NPs were influenced by drug-polymer interactions.\n",
      "• The e...\n",
      "\n",
      "📚 REFERENCES: 2 found\n",
      "   • ...\n",
      "   • ...\n",
      "\n",
      "💰 FUNDING: 1 sources\n",
      "   • None found....\n",
      "\n",
      "🏛️  CONFERENCE: None | None\n",
      "⏱️  Processing time: 29.50s\n",
      "💾 Results saved to: ../output/method2a_llama_results.json\n",
      "✅ Method 2a (Llama) completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Run extraction\n",
    "pdf_path = \"../data/test-poster.pdf\"\n",
    "\n",
    "if Path(pdf_path).exists():\n",
    "    print(\"🚀 Running Method 2a: Llama 3.2 8B Local Extraction\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract text\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"📏 Extracted {len(text)} characters\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize extractor\n",
    "        print(\"🤖 Initializing Llama 3.2 8B model...\")\n",
    "        extractor = LlamaExtractor()\n",
    "        \n",
    "        # Extract each field\n",
    "        print(\"🔍 Extracting metadata components...\")\n",
    "        \n",
    "        title = extractor.extract_field(text, \"title\")\n",
    "        authors = extractor.extract_field(text, \"authors\")\n",
    "        summary = extractor.extract_field(text, \"summary\")\n",
    "        keywords = extractor.extract_field(text, \"keywords\")\n",
    "        methods = extractor.extract_field(text, \"methods\")\n",
    "        results_text = extractor.extract_field(text, \"results\")\n",
    "        references = extractor.extract_field(text, \"references\")\n",
    "        funding_sources = extractor.extract_field(text, \"funding_sources\")\n",
    "        conference_info = extractor.extract_field(text, \"conference_info\")\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"summary\": summary,\n",
    "            \"keywords\": keywords,\n",
    "            \"methods\": methods,\n",
    "            \"results\": results_text,\n",
    "            \"references\": references,\n",
    "            \"funding_sources\": funding_sources,\n",
    "            \"conference_info\": conference_info,\n",
    "            \"extraction_metadata\": {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"processing_time\": time.time() - start_time,\n",
    "                \"method\": \"llama_local\",\n",
    "                \"model\": \"Meta-Llama-3-8B-Instruct\",\n",
    "                \"device\": str(next(extractor.model.parameters()).device),\n",
    "                \"text_length\": len(text),\n",
    "                \"do_sample\": False,\n",
    "                \"max_tokens\": 150\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n📄 TITLE: {results['title'][:100]}\")\n",
    "        print(f\"\\n👥 AUTHORS: {len(results['authors'])} found\")\n",
    "        for author in results[\"authors\"]:\n",
    "            affil_str = f\" ({', '.join(author['affiliations'])})\" if author['affiliations'] else \"\"\n",
    "            print(f\"   • {author['name']}{affil_str}\")\n",
    "        \n",
    "        print(f\"\\n📝 SUMMARY: {results['summary'][:100]}...\")\n",
    "        print(f\"\\n🔑 KEYWORDS: {', '.join(results['keywords'][:5])}\")\n",
    "        print(f\"\\n🔬 METHODS: {results['methods'][:100]}...\")\n",
    "        print(f\"\\n📊 RESULTS: {results['results'][:100]}...\")\n",
    "        print(f\"\\n📚 REFERENCES: {len(results['references'])} found\")\n",
    "        for ref in results['references'][:2]:  # Show first 2\n",
    "            print(f\"   • {ref['title'][:50]}...\")\n",
    "        print(f\"\\n💰 FUNDING: {len(results['funding_sources'])} sources\")\n",
    "        for funding in results['funding_sources'][:2]:  # Show first 2\n",
    "            print(f\"   • {funding[:50]}...\")\n",
    "        print(f\"\\n🏛️  CONFERENCE: {results['conference_info']['location']} | {results['conference_info']['date']}\")\n",
    "        print(f\"⏱️  Processing time: {results['extraction_metadata']['processing_time']:.2f}s\")\n",
    "        \n",
    "        # Save results with corrected filename\n",
    "        output_path = Path(\"../output/method2a_llama_results.json\")\n",
    "        output_path.parent.mkdir(exist_ok=True)\n",
    "        \n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"💾 Results saved to: {output_path}\")\n",
    "        print(\"✅ Method 2a (Llama) completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Llama extraction failed: {e}\")\n",
    "        print(\"   This may be due to insufficient GPU memory or model download issues\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Test poster not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a55c816f606410f99058f0381be6304": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0d91396cec6a45aa84db71ac043ce1f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_83ca0881feeb48779831cfff99f44158",
        "IPY_MODEL_69e23d75ba5c45b9be5154b2a2f0cd8f",
        "IPY_MODEL_eb9b046209504b36b2a42ae1d2cf842f"
       ],
       "layout": "IPY_MODEL_c9cfcb3119e047ee9e99f755805c8f3b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "1b119ea490284ad7ba00cc707d877a76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "69e23d75ba5c45b9be5154b2a2f0cd8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a748f4cf0e694315b29ad5cff5338fb2",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1b119ea490284ad7ba00cc707d877a76",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "7d35c4a5a3bb4c8298054678ac769bc9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "83ca0881feeb48779831cfff99f44158": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7d35c4a5a3bb4c8298054678ac769bc9",
       "placeholder": "​",
       "style": "IPY_MODEL_97d3aa8cffe647e7b44c3f52da2cd403",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "97d3aa8cffe647e7b44c3f52da2cd403": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9939cb619a054da1affdc39f26e2e0b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a748f4cf0e694315b29ad5cff5338fb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c9cfcb3119e047ee9e99f755805c8f3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb9b046209504b36b2a42ae1d2cf842f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9939cb619a054da1affdc39f26e2e0b5",
       "placeholder": "​",
       "style": "IPY_MODEL_0a55c816f606410f99058f0381be6304",
       "tabbable": null,
       "tooltip": null,
       "value": " 4/4 [00:05&lt;00:00,  1.11s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
