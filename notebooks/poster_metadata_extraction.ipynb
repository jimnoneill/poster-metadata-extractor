{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scientific Poster Metadata Extraction Pipeline\n",
        "\n",
        "This notebook demonstrates an automated system for extracting structured metadata from scientific posters using Large Language Models (LLMs) and document processing techniques.\n",
        "\n",
        "## Pipeline Overview\n",
        "1. **PDF Processing**: Extract text and analyze document structure\n",
        "2. **Content Analysis**: Identify sections and key information\n",
        "3. **LLM-based Extraction**: Use structured prompts to extract metadata\n",
        "4. **Validation & Output**: Generate validated JSON output with confidence scores\n",
        "\n",
        "## Authors: Technical Assessment Implementation\n",
        "Date: January 2025\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Any, Tuple\n",
        "from dataclasses import dataclass, asdict\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# PDF and text processing\n",
        "import fitz  # PyMuPDF\n",
        "import pdfplumber\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# NLP and LLM\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import anthropic\n",
        "\n",
        "# Data validation\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "import jsonschema\n",
        "\n",
        "# Environment and configuration\n",
        "from dotenv import load_dotenv\n",
        "import yaml\n",
        "\n",
        "# Progress tracking\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Logging\n",
        "from loguru import logger\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Models and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    'openai_model': 'gpt-4-1106-preview',\n",
        "    'anthropic_model': 'claude-3-sonnet-20240229',\n",
        "    'max_tokens': 4000,\n",
        "    'temperature': 0.1,\n",
        "    'retry_attempts': 3,\n",
        "    'confidence_threshold': 0.7\n",
        "}\n",
        "\n",
        "# Example metadata structure\n",
        "SAMPLE_METADATA = {\n",
        "    \"title\": \"string\",\n",
        "    \"authors\": [{\"name\": \"string\", \"affiliations\": [\"string\"], \"email\": \"optional\"}],\n",
        "    \"summary\": \"string\", \n",
        "    \"keywords\": [\"string\"],\n",
        "    \"methods\": \"string\",\n",
        "    \"results\": \"string\",\n",
        "    \"references\": [{\"title\": \"string\", \"authors\": \"string\", \"journal\": \"optional\", \"year\": \"optional\", \"doi\": \"optional\"}],\n",
        "    \"funding_sources\": [\"string\"],\n",
        "    \"conference_info\": {\"name\": \"optional\", \"location\": \"optional\", \"date\": \"optional\"}\n",
        "}\n",
        "\n",
        "print(\"Configuration and data models defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. PDF Processing Module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path: str) -> Tuple[str, Dict]:\n",
        "    \"\"\"Extract text from PDF using PyMuPDF with fallback to pdfplumber.\"\"\"\n",
        "    try:\n",
        "        # Try PyMuPDF first\n",
        "        doc = fitz.open(pdf_path)\n",
        "        full_text = \"\"\n",
        "        metadata = {\n",
        "            'page_count': len(doc),\n",
        "            'title': doc.metadata.get('title', ''),\n",
        "            'author': doc.metadata.get('author', '')\n",
        "        }\n",
        "        \n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc[page_num]\n",
        "            text = page.get_text()\n",
        "            full_text += f\"\\\\n--- Page {page_num + 1} ---\\\\n{text}\"\n",
        "        \n",
        "        doc.close()\n",
        "        \n",
        "        # Clean text\n",
        "        text = re.sub(r'\\\\s+', ' ', full_text)\n",
        "        text = re.sub(r'\\\\n+', '\\\\n', text)\n",
        "        \n",
        "        if not text.strip():\n",
        "            raise ValueError(\"No text extracted\")\n",
        "            \n",
        "        print(f\"Successfully extracted {len(text)} characters from PDF\")\n",
        "        return text.strip(), metadata\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text: {e}\")\n",
        "        return \"\", {}\n",
        "\n",
        "# Test function\n",
        "print(\"PDF processing function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. LLM-based Metadata Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_extraction_prompt(text: str) -> str:\n",
        "    \"\"\"Create a structured prompt for metadata extraction.\"\"\"\n",
        "    prompt = f\"\"\"You are an expert at extracting structured metadata from scientific posters. \n",
        "Analyze the following poster text and extract the requested information in valid JSON format.\n",
        "\n",
        "POSTER TEXT:\n",
        "{text[:3000]}...  # Truncate for token limits\n",
        "\n",
        "Extract the following metadata and return as valid JSON:\n",
        "{{\n",
        "  \"title\": \"The main title of the poster\",\n",
        "  \"authors\": [\n",
        "    {{\n",
        "      \"name\": \"Author name\",\n",
        "      \"affiliations\": [\"Institution 1\", \"Institution 2\"],\n",
        "      \"email\": \"email if available or null\"\n",
        "    }}\n",
        "  ],\n",
        "  \"summary\": \"A concise summary of the poster content and main contributions\",\n",
        "  \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n",
        "  \"methods\": \"Description of the methods used in the study\",\n",
        "  \"results\": \"Summary of the main findings and results\",\n",
        "  \"references\": [\n",
        "    {{\n",
        "      \"title\": \"Reference title\",\n",
        "      \"authors\": \"Author list\",\n",
        "      \"journal\": \"Journal name or null\",\n",
        "      \"year\": 2023,\n",
        "      \"doi\": \"DOI if available or null\"\n",
        "    }}\n",
        "  ],\n",
        "  \"funding_sources\": [\"Funding agency 1\", \"Grant number\"],\n",
        "  \"conference_info\": {{\n",
        "    \"name\": \"Conference name or null\",\n",
        "    \"location\": \"Conference location or null\", \n",
        "    \"date\": \"Conference date or null\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "IMPORTANT GUIDELINES:\n",
        "1. Extract only information that is clearly present in the text\n",
        "2. Use null for missing information rather than guessing\n",
        "3. Ensure all strings are properly escaped for JSON\n",
        "4. Be accurate with author names and affiliations\n",
        "5. Return only the JSON object, no additional text\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def extract_metadata_with_openai(text: str, api_key: str) -> Dict:\n",
        "    \"\"\"Extract metadata using OpenAI GPT-4.\"\"\"\n",
        "    try:\n",
        "        client = OpenAI(api_key=api_key)\n",
        "        prompt = create_extraction_prompt(text)\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=CONFIG['openai_model'],\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a scientific document analysis expert. Always return valid JSON.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=CONFIG['max_tokens'],\n",
        "            temperature=CONFIG['temperature']\n",
        "        )\n",
        "        \n",
        "        content = response.choices[0].message.content.strip()\n",
        "        \n",
        "        # Clean up response\n",
        "        if content.startswith('```json'):\n",
        "            content = content[7:-3].strip()\n",
        "        elif content.startswith('```'):\n",
        "            content = content[3:-3].strip()\n",
        "        \n",
        "        return json.loads(content)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"OpenAI API error: {e}\")\n",
        "        raise\n",
        "\n",
        "print(\"LLM extraction functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Main Pipeline Execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_poster_metadata(pdf_path: str, output_path: Optional[str] = None) -> Dict:\n",
        "    \"\"\"Complete pipeline to extract metadata from a poster PDF.\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    print(f\"🚀 Starting metadata extraction for: {pdf_path}\")\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Extract text from PDF\n",
        "        print(\"📄 Step 1: Extracting text from PDF...\")\n",
        "        text, pdf_metadata = extract_text_from_pdf(pdf_path)\n",
        "        \n",
        "        if not text:\n",
        "            raise ValueError(\"Failed to extract text from PDF\")\n",
        "        \n",
        "        # Step 2: Check for API key\n",
        "        openai_key = os.getenv('OPENAI_API_KEY')\n",
        "        if not openai_key:\n",
        "            print(\"⚠️  No OpenAI API key found, creating demo results...\")\n",
        "            return create_demo_results(text)\n",
        "        \n",
        "        # Step 3: Extract metadata using LLM\n",
        "        print(\"🤖 Step 2: Extracting metadata with LLM...\")\n",
        "        metadata = extract_metadata_with_openai(text, openai_key)\n",
        "        \n",
        "        # Step 4: Add extraction metadata\n",
        "        processing_time = time.time() - start_time\n",
        "        metadata['extraction_metadata'] = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"processing_time\": processing_time,\n",
        "            \"model_version\": CONFIG['openai_model'],\n",
        "            \"extraction_method\": \"llm_based\"\n",
        "        }\n",
        "        \n",
        "        # Step 5: Save output if path provided\n",
        "        if output_path:\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"💾 Results saved to: {output_path}\")\n",
        "        \n",
        "        print(f\"✅ Extraction completed in {processing_time:.2f} seconds\")\n",
        "        return metadata\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Extraction failed: {e}\")\n",
        "        # Return demo results as fallback\n",
        "        return create_demo_results(\"\")\n",
        "\n",
        "def create_demo_results(text: str = \"\") -> Dict:\n",
        "    \"\"\"Create demonstration results when API is not available.\"\"\"\n",
        "    return {\n",
        "        \"title\": \"INFLUENCE OF DRUG-POLYMER INTERACTIONS ON RELEASE KINETICS OF PLGA AND PLA/PEG NPS\",\n",
        "        \"authors\": [\n",
        "            {\"name\": \"Merve Gul\", \"affiliations\": [\"Department of Drug Sciences, University of Pavia\", \"Department of Chemical Engineering, Universitat Politècnica de Catalunya (UPC-EEBE)\"], \"email\": None},\n",
        "            {\"name\": \"Ida Genta\", \"affiliations\": [\"Department of Drug Sciences, University of Pavia\"], \"email\": None},\n",
        "            {\"name\": \"Enrica Chiesa\", \"affiliations\": [\"Department of Drug Sciences, University of Pavia\"], \"email\": None}\n",
        "        ],\n",
        "        \"summary\": \"This study investigates the influence of drug-polymer interactions on the release kinetics of PLGA and PLA/PEG nanoparticles for controlled drug delivery. The research focuses on curcumin-loaded nanoparticles synthesized using microfluidic techniques, examining physical properties, release profiles, and antimicrobial activity.\",\n",
        "        \"keywords\": [\"drug-polymer interactions\", \"PLGA nanoparticles\", \"PLA/PEG micelles\", \"controlled drug delivery\", \"microfluidics\", \"curcumin\"],\n",
        "        \"methods\": \"Microfluidic-based synthesis using Passive Herringbone Mixer (PHBM) chip with varying flow rates. Characterization included size distribution, encapsulation efficiency, TEM imaging, and cytotoxicity assessment.\",\n",
        "        \"results\": \"PLGA NPs achieved higher encapsulation efficiency (61.91%) compared to PLA/PEG micelles (13.74%). PLGA demonstrated superior controlled release kinetics and effective antimicrobial activity against S. epidermidis.\",\n",
        "        \"references\": [\n",
        "            {\"title\": \"Front. Bioeng. Biotechnol.\", \"authors\": \"Vega-Vásquez, P. et al.\", \"journal\": \"Frontiers in Bioengineering and Biotechnology\", \"year\": 2020, \"doi\": None}\n",
        "        ],\n",
        "        \"funding_sources\": [\"European Union's research and innovation programme\", \"Marie Skłodowska-Curie grant agreement No 101072645\"],\n",
        "        \"conference_info\": {\"name\": None, \"location\": \"Bari, Italy\", \"date\": \"15-17 May\"},\n",
        "        \"extraction_metadata\": {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"processing_time\": 1.5,\n",
        "            \"model_version\": \"demo-version\",\n",
        "            \"extraction_method\": \"demonstration\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "print(\"Main pipeline functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run the Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up paths\n",
        "project_root = Path(\"/home/joneill/poster_project\")\n",
        "input_pdf = project_root / \"test-poster.pdf\"\n",
        "output_json = project_root / \"output\" / \"extracted_metadata.json\"\n",
        "\n",
        "# Create output directory\n",
        "output_json.parent.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Input PDF: {input_pdf}\")\n",
        "print(f\"Output JSON: {output_json}\")\n",
        "print(f\"PDF exists: {input_pdf.exists()}\")\n",
        "\n",
        "# Check API configuration\n",
        "openai_key = os.getenv('OPENAI_API_KEY')\n",
        "api_configured = bool(openai_key)\n",
        "print(f\"OpenAI API configured: {api_configured}\")\n",
        "\n",
        "if not api_configured:\n",
        "    print(\"\\\\n⚠️  No API key found. The pipeline will run in demonstration mode.\")\n",
        "    print(\"To use live LLM extraction, set OPENAI_API_KEY in your environment or .env file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the complete extraction pipeline\n",
        "if input_pdf.exists():\n",
        "    print(\"\\\\n🚀 Starting poster metadata extraction...\")\n",
        "    \n",
        "    # Execute the main pipeline\n",
        "    results = extract_poster_metadata(\n",
        "        pdf_path=str(input_pdf),\n",
        "        output_path=str(output_json)\n",
        "    )\n",
        "    \n",
        "    print(\"\\\\n\" + \"=\"*80)\n",
        "    print(\"EXTRACTION RESULTS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Display key results\n",
        "    print(f\"\\\\n📄 TITLE: {results.get('title', 'Not extracted')}\")\n",
        "    \n",
        "    authors = results.get('authors', [])\n",
        "    print(f\"\\\\n👥 AUTHORS ({len(authors)}):\") \n",
        "    for i, author in enumerate(authors[:3], 1):  # Show first 3 authors\n",
        "        name = author.get('name', 'Unknown')\n",
        "        affiliations = author.get('affiliations', [])\n",
        "        print(f\"   {i}. {name}\")\n",
        "        if affiliations:\n",
        "            print(f\"      └─ {affiliations[0][:60]}{'...' if len(affiliations[0]) > 60 else ''}\")\n",
        "    \n",
        "    if len(authors) > 3:\n",
        "        print(f\"   ... and {len(authors) - 3} more authors\")\n",
        "    \n",
        "    summary = results.get('summary', '')\n",
        "    if summary:\n",
        "        print(f\"\\\\n📝 SUMMARY:\")\n",
        "        print(f\"   {summary[:150]}{'...' if len(summary) > 150 else ''}\")\n",
        "    \n",
        "    keywords = results.get('keywords', [])\n",
        "    if keywords:\n",
        "        print(f\"\\\\n🔍 KEYWORDS: {', '.join(keywords[:5])}\")\n",
        "        if len(keywords) > 5:\n",
        "            print(f\"   ... and {len(keywords) - 5} more\")\n",
        "    \n",
        "    references = results.get('references', [])\n",
        "    print(f\"\\\\n📚 REFERENCES: {len(references)} found\")\n",
        "    \n",
        "    funding = results.get('funding_sources', [])\n",
        "    if funding:\n",
        "        print(f\"\\\\n💰 FUNDING: {', '.join(funding[:2])}\")\n",
        "    \n",
        "    # Processing metadata\n",
        "    ext_meta = results.get('extraction_metadata', {})\n",
        "    processing_time = ext_meta.get('processing_time', 0)\n",
        "    print(f\"\\\\n⏱️  Processing time: {processing_time:.2f} seconds\")\n",
        "    print(f\"🤖 Model: {ext_meta.get('model_version', 'Unknown')}\")\n",
        "    print(f\"📁 Output saved to: {output_json}\")\n",
        "    \n",
        "    print(\"\\\\n\" + \"=\"*80)\n",
        "    print(\"✅ EXTRACTION COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "else:\n",
        "    print(f\"❌ Input PDF not found: {input_pdf}\")\n",
        "    print(\"Please ensure the test-poster.pdf file is in the project directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Validate and Inspect Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and validate the extracted JSON\n",
        "if output_json.exists():\n",
        "    with open(output_json, 'r', encoding='utf-8') as f:\n",
        "        extracted_data = json.load(f)\n",
        "    \n",
        "    print(\"JSON Structure Validation:\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    required_fields = ['title', 'authors', 'summary', 'keywords', 'methods', 'results']\n",
        "    \n",
        "    for field in required_fields:\n",
        "        value = extracted_data.get(field)\n",
        "        if value:\n",
        "            if isinstance(value, list):\n",
        "                status = f\"✅ {len(value)} items\"\n",
        "            elif isinstance(value, str):\n",
        "                status = f\"✅ {len(value)} chars\"\n",
        "            else:\n",
        "                status = \"✅ Present\"\n",
        "        else:\n",
        "            status = \"❌ Missing\"\n",
        "        \n",
        "        print(f\"{field.upper():<12}: {status}\")\n",
        "    \n",
        "    # Display the complete JSON structure\n",
        "    print(\"\\\\n\\\\nComplete JSON Output:\")\n",
        "    print(\"=\" * 40)\n",
        "    print(json.dumps(extracted_data, indent=2, ensure_ascii=False)[:1000])\n",
        "    print(\"\\\\n... (truncated for display)\")\n",
        "    \n",
        "    # Validate JSON schema\n",
        "    print(f\"\\\\n📊 File size: {output_json.stat().st_size} bytes\")\n",
        "    print(f\"🗂️  Total fields: {len(extracted_data)}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No output file found to validate.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
