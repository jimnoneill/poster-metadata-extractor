{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Qwen Local Extraction\\n\\n## Overview\\nLocal small language model for cost-effective poster metadata extraction. Runs entirely on your hardware without API dependencies.\\n\\n## Accuracy Note\\nThe 80-85% accuracy estimate is unvalidated - based on limited testing only. Actual accuracy must be determined through proper Cochran sampling validation before production use.\\n\\n## Performance Characteristics\\n- **Estimated Accuracy**: 80-85% (unvalidated - requires Cochran sampling validation)\\n- **Cost**: $0 (runs locally, only electricity costs)\\n- **Speed**: 10-40 seconds per poster (single), ~1.1s per poster (RTX 4090 batched)\\n- **Hallucination Risk**: Low (structured prompting)\\n- **Setup**: Medium - requires model download and GPU memory\\n\\n## RTX 4090 Batching Capacity\\n- **Recommended batch size**: 32 posters simultaneously\\n- **Throughput**: ~3,273 posters/hour, ~26,182 posters/day (8hrs)\\n\\n## Best For\\n- Privacy-sensitive environments\\n- Budget-conscious deployments\\n- Edge computing scenarios\\n- Development and experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:49:37.109627Z",
     "iopub.status.busy": "2025-08-18T21:49:37.109228Z",
     "iopub.status.idle": "2025-08-18T21:49:39.992736Z",
     "shell.execute_reply": "2025-08-18T21:49:39.992255Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Method 2: Qwen Local Extraction\n",
    "Local small language model for cost-effective poster metadata extraction\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress all warnings and errors\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Suppress protobuf and transformers warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"torch\").setLevel(logging.ERROR)\n",
    "\n",
    "# Try to suppress specific protobuf MessageFactory warnings\n",
    "try:\n",
    "    import google.protobuf.message\n",
    "    # Monkey patch to suppress the GetPrototype AttributeError\n",
    "    if hasattr(google.protobuf.message, 'MessageFactory'):\n",
    "        original_init = google.protobuf.message.MessageFactory.__init__\n",
    "        def patched_init(self):\n",
    "            try:\n",
    "                original_init(self)\n",
    "            except AttributeError:\n",
    "                pass\n",
    "        google.protobuf.message.MessageFactory.__init__ = patched_init\n",
    "except (ImportError, AttributeError):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:49:39.994758Z",
     "iopub.status.busy": "2025-08-18T21:49:39.994536Z",
     "iopub.status.idle": "2025-08-18T21:49:40.005272Z",
     "shell.execute_reply": "2025-08-18T21:49:40.004911Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        page_text = page.get_text()\n",
    "        if page_text:\n",
    "            text += f\"\\\\n--- Page {page_num + 1} ---\\\\n{page_text}\"\n",
    "    \n",
    "    doc.close()\n",
    "    return text.strip()\n",
    "\n",
    "class QwenExtractor:\n",
    "    \"\"\"Qwen2.5-1.5B-Instruct based metadata extractor\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"):\n",
    "        print(f\"üì• Loading {model_name}...\")\n",
    "        \n",
    "        # Load tokenizer with stderr redirection\n",
    "        import sys\n",
    "        from contextlib import redirect_stderr\n",
    "        import io\n",
    "        \n",
    "        with redirect_stderr(io.StringIO()):\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # Load model with quantization if CUDA available\n",
    "            if torch.cuda.is_available():\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,\n",
    "                    bnb_8bit_compute_dtype=torch.float16\n",
    "                )\n",
    "                \n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    quantization_config=bnb_config,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "            else:\n",
    "                # CPU loading\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=torch.float32\n",
    "                )\n",
    "                device = torch.device('cpu')\n",
    "                self.model = self.model.to(device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(f\"‚úÖ Model loaded successfully\")\n",
    "    \n",
    "    def extract_field(self, text: str, field: str) -> Any:\n",
    "        \"\"\"Extract specific field using few-shot prompting\"\"\"\n",
    "        \n",
    "        prompts = {\n",
    "            'title': f\"\"\"Extract the title from this poster text:\n",
    "\n",
    "Text: \"{text[:500]}\"\n",
    "\n",
    "Title:\"\"\",\n",
    "            \n",
    "            'authors': f\"\"\"Extract author names (comma-separated) from this poster:\n",
    "\n",
    "Text: \"{text[:500]}\"\n",
    "\n",
    "Authors:\"\"\",\n",
    "            \n",
    "            'summary': f\"\"\"Write a 2-sentence summary of this poster:\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Summary:\"\"\",\n",
    "            \n",
    "            'keywords': f\"\"\"Extract 5-6 keywords from this poster:\n",
    "\n",
    "Text: \"{text[:600]}\"\n",
    "\n",
    "Keywords:\"\"\",\n",
    "            \n",
    "            'methods': f\"\"\"Extract the main methods from this research:\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Methods:\"\"\",\n",
    "            \n",
    "            'results': f\"\"\"Extract the main results from this poster:\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Results:\"\"\"\n",
    "        }\n",
    "        \n",
    "        if field not in prompts:\n",
    "            return \"\"\n",
    "        \n",
    "        prompt = prompts[field]\n",
    "        \n",
    "        # Create chat template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Extract information precisely as requested.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text_input = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text_input,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.1,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Parse response based on field\n",
    "        if field == 'authors':\n",
    "            authors = [a.strip() for a in response.split(',') if a.strip()]\n",
    "            return [{'name': author} for author in authors[:6]]  # Limit to 6\n",
    "        elif field == 'keywords':\n",
    "            keywords = [k.strip() for k in response.split(',') if k.strip()]\n",
    "            return keywords[:8]  # Limit to 8\n",
    "        else:\n",
    "            return response.strip()\n",
    "\n",
    "def extract_poster_metadata_qwen(pdf_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract metadata using Qwen model\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"üìÑ Processing: {Path(pdf_path).name}\")\n",
    "    \n",
    "    # Extract text\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"üìè Extracted {len(text)} characters\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize extractor\n",
    "        print(\"ü§ñ Initializing Qwen2.5-1.5B model...\")\n",
    "        extractor = QwenExtractor()\n",
    "        \n",
    "        # Extract each field\n",
    "        print(\"üîç Extracting metadata components...\")\n",
    "        \n",
    "        metadata = {\n",
    "            'title': extractor.extract_field(text, 'title'),\n",
    "            'authors': extractor.extract_field(text, 'authors'),\n",
    "            'summary': extractor.extract_field(text, 'summary'),\n",
    "            'keywords': extractor.extract_field(text, 'keywords'),\n",
    "            'methods': extractor.extract_field(text, 'methods'),\n",
    "            'results': extractor.extract_field(text, 'results'),\n",
    "            'references': [],  # Would need more complex extraction\n",
    "            'funding_sources': [],  # Would need pattern matching\n",
    "            'conference_info': {'location': None, 'date': None},\n",
    "            'extraction_metadata': {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'processing_time': time.time() - start_time,\n",
    "                'method': 'qwen_local',\n",
    "                'model': 'Qwen2.5-1.5B-Instruct',\n",
    "                'device': str(next(extractor.model.parameters()).device),\n",
    "                'text_length': len(text)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Qwen extraction failed: {e}\")\n",
    "        return {\n",
    "            'error': str(e),\n",
    "            'extraction_metadata': {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'method': 'qwen_local_failed',\n",
    "                'processing_time': time.time() - start_time\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:49:40.006766Z",
     "iopub.status.busy": "2025-08-18T21:49:40.006643Z",
     "iopub.status.idle": "2025-08-18T21:50:39.318689Z",
     "shell.execute_reply": "2025-08-18T21:50:39.318149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Method 2: Qwen Local Extraction\n",
      "============================================================\n",
      "üìÑ Processing: test-poster.pdf\n",
      "üìè Extracted 3734 characters\n",
      "ü§ñ Initializing Qwen2.5-1.5B model...\n",
      "üì• Loading Qwen/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755553781.986338 1240135 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755553781.992055 1240135 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755553782.007891 1240135 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755553782.007906 1240135 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755553782.007908 1240135 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755553782.007910 1240135 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully\n",
      "üîç Extracting metadata components...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nüìÑ TITLE: Influence of Drug-Polymer Interactions on Release Kinetics of PLGA and PLA/PET Nanoparticles\n",
      "üë• AUTHORS: 5 found\n",
      "   ‚Ä¢ Merve Gul\n",
      "   ‚Ä¢ Ida Genta\n",
      "   ‚Ä¢ Maria M. Perez Madrigal\n",
      "   ‚Ä¢ Carlos Aleman\n",
      "   ‚Ä¢ Enrica Chiesa\n",
      "\\nüìù SUMMARY: The poster discusses the influence of drug-polymer interactions on release kinetics in poly(lactic-c...\n",
      "üîë KEYWORDS: ANTIMICROBIAL RESISTANCE, DRUG POLYMERS, RELEASE KINETICS, PLGA, PLA/PEG\n",
      "‚è±Ô∏è  Processing time: 59.28s\n",
      "üíæ Results saved to: /home/joneill/poster_project/output/method2_qwen_results.json\n",
      "‚úÖ Method 2 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test the extraction\n",
    "    pdf_path = \"/home/joneill/poster_project/test-poster.pdf\"\n",
    "    if Path(pdf_path).exists():\n",
    "        print(\"üöÄ Running Method 2: Qwen Local Extraction\")\n",
    "        print(\"=\" * 60)\n",
    "        results = extract_poster_metadata_qwen(pdf_path)\n",
    "        if 'error' not in results:\n",
    "            # Display results\n",
    "            print(f\"\\\\nüìÑ TITLE: {results['title']}\")\n",
    "            print(f\"üë• AUTHORS: {len(results['authors'])} found\")\n",
    "            for author in results['authors']:\n",
    "                print(f\"   ‚Ä¢ {author['name']}\")\n",
    "            print(f\"\\\\nüìù SUMMARY: {results['summary'][:100]}...\")\n",
    "            print(f\"üîë KEYWORDS: {', '.join(results['keywords'][:5])}\")\n",
    "            print(f\"‚è±Ô∏è  Processing time: {results['extraction_metadata']['processing_time']:.2f}s\")\n",
    "            # Save results\n",
    "            output_path = Path(\"/home/joneill/poster_project/output/method2_qwen_results.json\")\n",
    "            output_path.parent.mkdir(exist_ok=True)\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            print(f\"üíæ Results saved to: {output_path}\")\n",
    "            print(\"‚úÖ Method 2 completed successfully!\")\n",
    "        else:\n",
    "            print(f\"‚ùå Extraction failed: {results['error']}\")\n",
    "    else:\n",
    "        print(\"‚ùå Test poster not found\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
