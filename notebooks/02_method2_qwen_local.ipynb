{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Qwen Local Extraction\n",
    "\n",
    "## Overview\n",
    "Local small language model for cost-effective poster metadata extraction. Runs entirely on your hardware without API dependencies.\n",
    "\n",
    "## ‚ö†Ô∏è ACCURACY DISCLAIMER\n",
    "**The 80-85% accuracy estimate is UNVALIDATED** - based on limited testing only. Actual accuracy must be determined through proper Cochran sampling validation before production use.\n",
    "\n",
    "## Performance Characteristics\n",
    "- **Estimated Accuracy**: 80-85% ‚ö†Ô∏è **(UNVALIDATED - requires Cochran sampling validation)**\n",
    "- **Cost**: $0 (runs locally, only electricity costs)\n",
    "- **Speed**: 10-40 seconds per poster (single), ~1.1s per poster (RTX 4090 batched)\n",
    "- **Hallucination Risk**: Low (structured prompting)\n",
    "- **Setup**: Medium - requires model download and GPU memory\n",
    "\n",
    "## RTX 4090 Batching Capacity\n",
    "- **Recommended batch size**: 32 posters simultaneously\n",
    "- **Throughput**: ~3,273 posters/hour, ~26,182 posters/day (8hrs)\n",
    "\n",
    "## Best For\n",
    "- Privacy-sensitive environments\n",
    "- Budget-conscious deployments\n",
    "- Edge computing scenarios\n",
    "- Development and experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:51:46.632109Z",
     "iopub.status.busy": "2025-08-18T20:51:46.631685Z",
     "iopub.status.idle": "2025-08-18T20:51:49.619095Z",
     "shell.execute_reply": "2025-08-18T20:51:49.618474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: cuda\n",
      "üíæ GPU memory: 25.3GB\n",
      "‚úÖ Environment ready for Method 2: Qwen Local\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "import warnings\n",
    "# Suppress TensorFlow and CUDA initialization warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "print(f\"üíæ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\" if torch.cuda.is_available() else \"Using CPU\")\n",
    "print(\"‚úÖ Environment ready for Method 2: Qwen Local\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:51:49.650674Z",
     "iopub.status.busy": "2025-08-18T20:51:49.650170Z",
     "iopub.status.idle": "2025-08-18T20:51:49.658949Z",
     "shell.execute_reply": "2025-08-18T20:51:49.658552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ QwenExtractor class defined\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        page_text = page.get_text()\n",
    "        if page_text:\n",
    "            text += f\"\\\\n--- Page {page_num + 1} ---\\\\n{page_text}\"\n",
    "    \n",
    "    doc.close()\n",
    "    return text.strip()\n",
    "\n",
    "class QwenExtractor:\n",
    "    \"\"\"Qwen2.5-1.5B-Instruct based metadata extractor\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"):\n",
    "        print(f\"üì• Loading {model_name}...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model with quantization if CUDA available\n",
    "        if torch.cuda.is_available():\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                bnb_8bit_compute_dtype=torch.float16\n",
    "            )\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        else:\n",
    "            # CPU loading\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float32\n",
    "            )\n",
    "            device = torch.device('cpu')\n",
    "            self.model = self.model.to(device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(f\"‚úÖ Model loaded successfully\")\n",
    "    \n",
    "    def extract_field(self, text: str, field: str) -> Any:\n",
    "        \"\"\"Extract specific field using few-shot prompting\"\"\"\n",
    "        \n",
    "        prompts = {\n",
    "            'title': f\"\"\"Extract the title from this poster text:\n",
    "\n",
    "Text: \"{text[:500]}\"\n",
    "\n",
    "Title:\"\"\",\n",
    "            \n",
    "            'authors': f\"\"\"Extract author names (comma-separated) from this poster:\n",
    "\n",
    "Text: \"{text[:500]}\"\n",
    "\n",
    "Authors:\"\"\",\n",
    "            \n",
    "            'summary': f\"\"\"Write a 2-sentence summary of this poster:\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Summary:\"\"\",\n",
    "            \n",
    "            'keywords': f\"\"\"Extract 5-6 keywords from this poster:\n",
    "\n",
    "Text: \"{text[:600]}\"\n",
    "\n",
    "Keywords:\"\"\",\n",
    "            \n",
    "            'methods': f\"\"\"Extract the main methods from this research:\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Methods:\"\"\",\n",
    "            \n",
    "            'results': f\"\"\"Extract the main results from this poster:\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Results:\"\"\"\n",
    "        }\n",
    "        \n",
    "        if field not in prompts:\n",
    "            return \"\"\n",
    "        \n",
    "        prompt = prompts[field]\n",
    "        \n",
    "        # Create chat template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Extract information precisely as requested.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text_input = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text_input,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.1,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Parse response based on field\n",
    "        if field == 'authors':\n",
    "            authors = [a.strip() for a in response.split(',') if a.strip()]\n",
    "            return [{'name': author} for author in authors[:6]]  # Limit to 6\n",
    "        elif field == 'keywords':\n",
    "            keywords = [k.strip() for k in response.split(',') if k.strip()]\n",
    "            return keywords[:8]  # Limit to 8\n",
    "        else:\n",
    "            return response.strip()\n",
    "\n",
    "print(\"‚úÖ QwenExtractor class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:51:49.660344Z",
     "iopub.status.busy": "2025-08-18T20:51:49.660020Z",
     "iopub.status.idle": "2025-08-18T20:52:27.855256Z",
     "shell.execute_reply": "2025-08-18T20:52:27.854713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Method 2: Qwen Local Extraction\n",
      "============================================================\n",
      "üìè Extracted 3734 characters\n",
      "ü§ñ Initializing Qwen2.5-1.5B model...\n",
      "üì• Loading Qwen/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755550311.613229 1221977 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755550311.618935 1221977 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755550311.634675 1221977 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755550311.634690 1221977 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755550311.634692 1221977 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755550311.634694 1221977 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully\n",
      "üîç Extracting metadata components...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nüìÑ TITLE: Influence of Drug-Polymer Interactions on Release Kinetics of PLGA and PLA/PET Nanoparticles\n",
      "üë• AUTHORS: 5 found\n",
      "   ‚Ä¢ Merve Gul\n",
      "   ‚Ä¢ Ida Genta\n",
      "   ‚Ä¢ Maria M. Perez Madrigal\n",
      "   ‚Ä¢ Carlos Aleman\n",
      "   ‚Ä¢ Enrica Chiesa\n",
      "\\nüìù SUMMARY: The poster discusses the influence of drug-polymer interactions on release kinetics in poly(lactic-c...\n",
      "üîë KEYWORDS: ANTIMICROBIAL RESISTANCE, DRUG POLYMERS, RELEASE KINETICS, PLGA, PLA/PEG\n",
      "‚è±Ô∏è  Processing time: 38.19s\n",
      "üíæ Results saved to: /home/joneill/poster_project/output/method2_qwen_results.json\n",
      "‚úÖ Method 2 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Run extraction\n",
    "pdf_path = \"/home/joneill/poster_project/test-poster.pdf\"\n",
    "\n",
    "if Path(pdf_path).exists():\n",
    "    print(\"üöÄ Running Method 2: Qwen Local Extraction\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract text\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"üìè Extracted {len(text)} characters\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize extractor\n",
    "        print(\"ü§ñ Initializing Qwen2.5-1.5B model...\")\n",
    "        extractor = QwenExtractor()\n",
    "        \n",
    "        # Extract each field\n",
    "        print(\"üîç Extracting metadata components...\")\n",
    "        \n",
    "        metadata = {\n",
    "            'title': extractor.extract_field(text, 'title'),\n",
    "            'authors': extractor.extract_field(text, 'authors'),\n",
    "            'summary': extractor.extract_field(text, 'summary'),\n",
    "            'keywords': extractor.extract_field(text, 'keywords'),\n",
    "            'methods': extractor.extract_field(text, 'methods'),\n",
    "            'results': extractor.extract_field(text, 'results'),\n",
    "            'references': [],  # Would need more complex extraction\n",
    "            'funding_sources': [],  # Would need pattern matching\n",
    "            'conference_info': {'location': None, 'date': None},\n",
    "            'extraction_metadata': {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'processing_time': time.time() - start_time,\n",
    "                'method': 'qwen_local',\n",
    "                'model': 'Qwen2.5-1.5B-Instruct',\n",
    "                'device': str(next(extractor.model.parameters()).device),\n",
    "                'text_length': len(text)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\\\nüìÑ TITLE: {metadata['title']}\")\n",
    "        print(f\"üë• AUTHORS: {len(metadata['authors'])} found\")\n",
    "        for author in metadata['authors']:\n",
    "            print(f\"   ‚Ä¢ {author['name']}\")\n",
    "        \n",
    "        print(f\"\\\\nüìù SUMMARY: {metadata['summary'][:100]}...\")\n",
    "        print(f\"üîë KEYWORDS: {', '.join(metadata['keywords'][:5])}\")\n",
    "        print(f\"‚è±Ô∏è  Processing time: {metadata['extraction_metadata']['processing_time']:.2f}s\")\n",
    "        \n",
    "        # Save results\n",
    "        output_path = Path(\"/home/joneill/poster_project/output/method2_qwen_results.json\")\n",
    "        output_path.parent.mkdir(exist_ok=True)\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Results saved to: {output_path}\")\n",
    "        print(\"‚úÖ Method 2 completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Qwen extraction failed: {e}\")\n",
    "        print(\"   This may be due to insufficient GPU memory or model download issues\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Test poster not found\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
