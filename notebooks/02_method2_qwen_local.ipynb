{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Qwen Local Extraction\n",
    "\n",
    "## Overview\n",
    "Local small language model for cost-effective poster metadata extraction. Runs entirely on your hardware without API dependencies.\n",
    "\n",
    "## Accuracy Note\n",
    "The 80-85% accuracy estimate is unvalidated - based on limited testing only. Actual accuracy must be determined through proper Cochran sampling validation before production use.\n",
    "\n",
    "## Performance Characteristics\n",
    "- **Estimated Accuracy**: 80-85% (unvalidated - requires Cochran sampling validation)\n",
    "- **Cost**: $0 (runs locally, only electricity costs)\n",
    "- **Speed**: 10-40 seconds per poster (single), ~1.1s per poster (RTX 4090 batched)\n",
    "- **Hallucination Risk**: Low (structured prompting)\n",
    "- **Setup**: Medium - requires model download and GPU memory\n",
    "\n",
    "## RTX 4090 Batching Capacity\n",
    "- **Recommended batch size**: 32 posters simultaneously\n",
    "- **Throughput**: ~3,273 posters/hour, ~26,182 posters/day (8hrs)\n",
    "\n",
    "## Best For\n",
    "- Privacy-sensitive environments\n",
    "- Budget-conscious deployments\n",
    "- Edge computing scenarios\n",
    "- Development and experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:37:49.476187Z",
     "iopub.status.busy": "2025-08-30T02:37:49.475893Z",
     "iopub.status.idle": "2025-08-30T02:37:52.472989Z",
     "shell.execute_reply": "2025-08-30T02:37:52.472464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: cuda\n",
      "üíæ GPU memory: 25.3GB\n",
      "‚úÖ Environment ready for Method 2: Qwen Local\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "import warnings\n",
    "import contextlib\n",
    "import io\n",
    "import logging\n",
    "# Suppress TensorFlow and CUDA initialization warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#from jtools import normalize_characters\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "print(f\"üíæ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\" if torch.cuda.is_available() else \"Using CPU\")\n",
    "print(\"‚úÖ Environment ready for Method 2: Qwen Local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:37:52.504465Z",
     "iopub.status.busy": "2025-08-30T02:37:52.503909Z",
     "iopub.status.idle": "2025-08-30T02:37:52.513321Z",
     "shell.execute_reply": "2025-08-30T02:37:52.512897Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import unicodedata\n",
    "import re\n",
    "def remove_quotes(text):\n",
    "    \"\"\"Remove surrounding quotes from text\"\"\"\n",
    "    text = text.strip()\n",
    "    if (text.startswith(\"'\") and text.endswith(\"'\")) or (text.startswith('\"') and text.endswith('\"')):\n",
    "        return text[1:-1]\n",
    "    return text\n",
    "\n",
    "def clean_qwen_response(response: str, field_type: str) -> str:\n",
    "    \"\"\"Clean up verbose Qwen responses to extract just the content\"\"\"\n",
    "    response = response.strip()\n",
    "    \n",
    "    # Remove common verbose prefixes\n",
    "    prefixes_to_remove = [\n",
    "        \"The title of the poster is:\",\n",
    "        \"Here are the author names extracted in a comma-separated list:\",\n",
    "        \"Here is a 2-sentence summary of the poster:\",\n",
    "        \"Here are 5-6 keywords extracted from the poster:\",\n",
    "        \"Here are the methods mentioned in the poster:\",\n",
    "        \"Here are the main results extracted from the poster:\",\n",
    "        \"Here are the references found in the poster:\",\n",
    "        \"Here are the funding sources found:\",\n",
    "        \"Here is the conference information:\",\n",
    "        \"The title is:\",\n",
    "        \"Authors:\",\n",
    "        \"Summary:\",\n",
    "        \"Keywords:\",\n",
    "        \"Methods:\",\n",
    "        \"Results:\",\n",
    "        \"References:\",\n",
    "        \"Funding:\",\n",
    "        \"Conference:\"\n",
    "    ]\n",
    "    \n",
    "    for prefix in prefixes_to_remove:\n",
    "        if response.lower().startswith(prefix.lower()):\n",
    "            response = response[len(prefix):].strip()\n",
    "    \n",
    "    # Remove numbered lists (1., 2., etc.)\n",
    "    if field_type in ['keywords', 'methods', 'funding_sources']:\n",
    "        lines = response.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            # Remove numbering like \"1.\", \"2.\", \"*\", \"-\" at start of line\n",
    "            line = re.sub(r'^\\s*[\\d]+\\.\\s*', '', line)\n",
    "            line = re.sub(r'^\\s*[\\*\\-]\\s*', '', line)\n",
    "            if line.strip():\n",
    "                cleaned_lines.append(line.strip())\n",
    "        response = '\\n'.join(cleaned_lines) if field_type == 'methods' else ', '.join(cleaned_lines)\n",
    "    \n",
    "    # Remove quotes and extra whitespace\n",
    "    response = remove_quotes(response)\n",
    "    \n",
    "    return response.strip()\n",
    "def normalize_characters(text):\n",
    "    # Normalize Greek characters\n",
    "    greek_chars = ['Œ±', 'Œ≤', 'Œ≥', 'Œ¥', 'Œµ', 'Œ∂', 'Œ∑', 'Œ∏', 'Œπ', 'Œ∫', 'Œª', 'Œº', 'ŒΩ', 'Œæ', 'Œø', 'œÄ', 'œÅ', 'œÇ', 'œÉ', 'œÑ', 'œÖ', 'œÜ', 'œá', 'œà', 'œâ', 'Œë', 'Œí', 'Œì', 'Œî', 'Œï', 'Œñ', 'Œó', 'Œò', 'Œô', 'Œö', 'Œõ', 'Œú', 'Œù', 'Œû', 'Œü', 'Œ†', 'Œ°', 'Œ£', 'Œ§', 'Œ•', 'Œ¶', 'Œß', 'Œ®', 'Œ©']\n",
    "    for char in greek_chars:\n",
    "        text = text.replace(char, unicodedata.normalize('NFC', char))\n",
    "\n",
    "    # Normalize space characters\n",
    "    space_chars = ['\\xa0', '\\u2000', '\\u2001', '\\u2002', '\\u2003', '\\u2004', '\\u2005', '\\u2006', '\\u2007', '\\u2008', '\\u2009', '\\u200a', '\\u202f', '\\u205f', '\\u3000']\n",
    "    for space in space_chars:\n",
    "        text = text.replace(space, ' ')\n",
    "\n",
    "    # Normalize single quotes\n",
    "    single_quotes = ['‚Äò', '‚Äô', '‚Äõ', '‚Ä≤', '‚Äπ', '‚Ä∫', '‚Äö', '‚Äü']\n",
    "    for quote in single_quotes:\n",
    "        text = text.replace(quote, \"'\")\n",
    "\n",
    "    # Normalize double quotes\n",
    "    double_quotes = ['‚Äú', '‚Äù', '‚Äû', '‚Äü', '¬´', '¬ª', '„Äù', '„Äû', '„Äü', 'ÔºÇ']\n",
    "    for quote in double_quotes:\n",
    "        text = text.replace(quote, '\"')\n",
    "\n",
    "    # Normalize brackets\n",
    "    brackets = {\n",
    "        '„Äê': '[', '„Äë': ']',\n",
    "        'Ôºà': '(', 'Ôºâ': ')',\n",
    "        'ÔΩõ': '{', 'ÔΩù': '}',\n",
    "        '„Äö': '[', '„Äõ': ']',\n",
    "        '„Äà': '<', '„Äâ': '>',\n",
    "        '„Ää': '<', '„Äã': '>',\n",
    "        '„Äå': '[', '„Äç': ']',\n",
    "        '„Äé': '[', '„Äé': ']',\n",
    "        '„Äî': '[', '„Äï': ']',\n",
    "        '„Äñ': '[', '„Äó': ']'\n",
    "    }\n",
    "    for old, new in brackets.items():\n",
    "        text = text.replace(old, new)\n",
    "\n",
    "    # Normalize hyphens and dashes\n",
    "    hyphens_and_dashes = ['‚Äê', '‚Äë', '‚Äí', '‚Äì', '‚Äî', '‚Äï']\n",
    "    for dash in hyphens_and_dashes:\n",
    "        text = text.replace(dash, '-')\n",
    "\n",
    "    # Normalize line breaks\n",
    "    line_breaks = ['\\r\\n', '\\r']\n",
    "    for line_break in line_breaks:\n",
    "        text = text.replace(line_break, '\\n')\n",
    "\n",
    "    # Normalize superscripts and subscripts to normal numbers\n",
    "    superscripts = '‚Å∞¬π¬≤¬≥‚Å¥‚Åµ‚Å∂‚Å∑‚Å∏‚Åπ'\n",
    "    subscripts = '‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚Çâ'\n",
    "    normal_numbers = '0123456789'\n",
    "\n",
    "    for super_, sub_, normal in zip(superscripts, subscripts, normal_numbers):\n",
    "        text = text.replace(super_, normal).replace(sub_, normal)\n",
    "\n",
    "    # Remove or normalize any remaining special characters using the 'NFKD' method\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "    return remove_quotes(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:37:52.514821Z",
     "iopub.status.busy": "2025-08-30T02:37:52.514695Z",
     "iopub.status.idle": "2025-08-30T02:37:52.530938Z",
     "shell.execute_reply": "2025-08-30T02:37:52.530552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ QwenExtractor class defined\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        page_text = page.get_text()\n",
    "        if page_text:\n",
    "            text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    # Apply normalize_characters to the ENTIRE extracted text\n",
    "    text = normalize_characters(text)\n",
    "    return text.strip()\n",
    "\n",
    "class QwenExtractor:\n",
    "    \"\"\"Qwen2.5-1.5B-Instruct based metadata extractor\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"):\n",
    "        print(f\"üì• Loading {model_name}...\")\n",
    "        \n",
    "        # Load tokenizer with stderr suppression\n",
    "        with contextlib.redirect_stderr(io.StringIO()):\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model with quantization if CUDA available\n",
    "        if torch.cuda.is_available():\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                bnb_8bit_compute_dtype=torch.float16\n",
    "            )\n",
    "            \n",
    "            # Load model with stderr suppression\n",
    "            with contextlib.redirect_stderr(io.StringIO()):\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    quantization_config=bnb_config,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "        else:\n",
    "            # CPU loading\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float32\n",
    "            )\n",
    "            device = torch.device(\"cpu\")\n",
    "            self.model = self.model.to(device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(f\"‚úÖ Model loaded successfully\")\n",
    "    \n",
    "    def extract_field(self, text: str, field: str) -> Any:\n",
    "        \"\"\"Extract specific field using few-shot prompting\"\"\"\n",
    "        \n",
    "        # More explicit prompts that discourage verbose responses\n",
    "        prompts = {\n",
    "            'title': f\"\"\"Extract only the title from this poster text. Provide just the title text, nothing else.\n",
    "\n",
    "Text: \"{text[:500]}\"\n",
    "\n",
    "Title:\"\"\",\n",
    "            \n",
    "            'authors': f\"\"\"Extract the complete author names and their affiliations from this poster. Look for the author section near the title, and match superscript numbers to institutions listed nearby.\n",
    "\n",
    "IMPORTANT: Extract COMPLETE names (first and last name) and link them to their affiliations using superscript numbers.\n",
    "\n",
    "Format as: \"Author Name (Affiliation)\" for each author, separated by \" | \"\n",
    "\n",
    "Examples:\n",
    "- If you see \"Merve Gul¬π, Ida Genta¬≤\" and \"¬πUniversity of Pavia, ¬≤University of Rome\", extract:\n",
    "  \"Merve Gul (University of Pavia) | Ida Genta (University of Rome)\"\n",
    "- If you see \"John Smith¬π'¬≤, Mary Johnson¬≥\" and affiliations listed, match the numbers.\n",
    "\n",
    "Text: \"{text[:1200]}\"\n",
    "\n",
    "Authors with Affiliations:\"\"\",\n",
    "            \n",
    "            'summary': f\"\"\"Write a concise 2-sentence summary of this poster's research. Be direct and factual.\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Summary:\"\"\",\n",
    "            \n",
    "            'keywords': f\"\"\"Extract 5-6 key technical terms from this poster. List only the keywords separated by commas.\n",
    "\n",
    "Text: \"{text[:600]}\"\n",
    "\n",
    "Keywords:\"\"\",\n",
    "            \n",
    "            'methods': f\"\"\"Extract the research methods described in this poster. Be concise and specific.\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Methods:\"\"\",\n",
    "            \n",
    "            'results': f\"\"\"Extract the main research findings from this poster. Include specific numbers/measurements if present.\n",
    "\n",
    "Text: \"{text[:800]}\"\n",
    "\n",
    "Results:\"\"\",\n",
    "            \n",
    "            'references': f\"\"\"Extract references or citations from this poster. Look for sections titled \"References\", \"Bibliography\", \"Citations\", or numbered reference lists, usually at the bottom. Look for patterns like:\n",
    "- [1], [2], [3] followed by citation details\n",
    "- 1., 2., 3. followed by publication info\n",
    "- Author names followed by titles and years\n",
    "- Journal names, publication years, volume/page numbers\n",
    "\n",
    "Extract complete references in the format: \"Title - Authors (Year) Journal\" separated by \" | \" for multiple references.\n",
    "\n",
    "Examples:\n",
    "- \"Drug delivery systems - Smith et al. (2023) Nature\"\n",
    "- \"PLGA nanoparticles - Jones, M. et al. (2022) Advanced Materials\"\n",
    "\n",
    "If you find numbered references but can't parse full details, extract what's available.\n",
    "\n",
    "Text: \"{text[:2000]}\"\n",
    "\n",
    "References:\"\"\",\n",
    "            \n",
    "            'funding_sources': f\"\"\"Extract funding sources, grants, or acknowledgments from this poster. Look for sections like \"Acknowledgments\", \"Funding\", \"Grants\", or \"Support\". Look for patterns like grant numbers, funding agencies, or financial support mentions.\n",
    "\n",
    "Common patterns to look for:\n",
    "- \"Grant No. XXX\", \"Grant #XXX\"  \n",
    "- \"NSF\", \"NIH\", \"EU\", \"Horizon 2020\", etc.\n",
    "- \"supported by\", \"funded by\", \"financial support\"\n",
    "- Specific grant identifiers (letters/numbers combinations)\n",
    "\n",
    "List funding agencies or grant numbers separated by commas, or \"None found\".\n",
    "\n",
    "Text: \"{text[:1200]}\"\n",
    "\n",
    "Funding Sources:\"\"\",\n",
    "            \n",
    "            'conference_info': f\"\"\"Extract conference information from this poster. Look for location names (cities, countries) and dates. This information is often at the bottom of the poster or near the title.\n",
    "\n",
    "Look for patterns like:\n",
    "- City names (Bari, Rome, Paris, etc.)\n",
    "- Countries (Italy, France, USA, etc.) \n",
    "- Dates (May 15-17, June 2024, etc.)\n",
    "\n",
    "Format as: \"Location: City, Country | Date: date range\" or just the location/date if found.\n",
    "\n",
    "Text: \"{text[:1200]}\"\n",
    "\n",
    "Conference Info:\"\"\"\n",
    "        }\n",
    "        \n",
    "        if field not in prompts:\n",
    "            return \"\"\n",
    "        \n",
    "        prompt = prompts[field]\n",
    "        \n",
    "        # Create chat template with PDF context and explicit instructions for conciseness\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise data extraction assistant working with unstructured text converted from conference poster PDFs. The text may have formatting issues, scattered layout, and mixed content. Focus on extracting the specific requested information. Provide only the requested information without explanatory text, prefixes, or formatting. Be direct and concise.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text_input = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text_input,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # Generate with greedy decoding for deterministic output\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                do_sample=False,     # Greedy decoding = deterministic (most probable token)\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1  # Prevent repetition\n",
    "                # Note: temperature/top_p not needed with do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Clean up the response\n",
    "        response = clean_qwen_response(response, field)\n",
    "        \n",
    "        # Parse response based on field\n",
    "        if field == \"authors\":\n",
    "            authors = []\n",
    "            # Handle format: \"Author Name (Affiliation) | Author Name (Affiliation)\"\n",
    "            if \"|\" in response:\n",
    "                author_entries = response.split(\"|\")\n",
    "            else:\n",
    "                # Fallback: try comma separation\n",
    "                author_entries = response.split(\",\")\n",
    "            \n",
    "            for entry in author_entries:\n",
    "                entry = entry.strip()\n",
    "                if not entry:\n",
    "                    continue\n",
    "                \n",
    "                name = \"\"\n",
    "                affiliations = []\n",
    "                \n",
    "                # Try to parse \"Name (Affiliation)\" format\n",
    "                if \"(\" in entry and \")\" in entry:\n",
    "                    name_part = entry.split(\"(\")[0].strip()\n",
    "                    affiliation_part = entry.split(\"(\")[1].split(\")\")[0].strip()\n",
    "                    \n",
    "                    # Clean up the name\n",
    "                    name = name_part\n",
    "                    \n",
    "                    # Add affiliation if it looks valid\n",
    "                    if affiliation_part and len(affiliation_part) > 2:\n",
    "                        affiliations.append(affiliation_part)\n",
    "                else:\n",
    "                    # No parentheses, just the name\n",
    "                    name = entry\n",
    "                \n",
    "                # Filter out non-author entries\n",
    "                institutional_keywords = [\n",
    "                    'department', 'university', 'institute', 'center', 'centre', \n",
    "                    'school', 'college', 'laboratory', 'lab', 'division',\n",
    "                    'research', 'faculty', 'hospital', 'clinic'\n",
    "                ]\n",
    "                \n",
    "                name_lower = name.lower()\n",
    "                \n",
    "                # Skip if the \"name\" is clearly an institution\n",
    "                if any(keyword in name_lower for keyword in institutional_keywords):\n",
    "                    continue\n",
    "                \n",
    "                # Skip if it starts with prepositions or looks like an affiliation\n",
    "                if (name_lower.startswith(('of ', 'for ', 'and ', 'the ')) or\n",
    "                    len(name.split()) > 4):  # Names shouldn't be too long\n",
    "                    continue\n",
    "                \n",
    "                if name and len(name.split()) >= 2:  # Should have at least first and last name\n",
    "                    authors.append({\n",
    "                        \"name\": name,\n",
    "                        \"affiliations\": affiliations,\n",
    "                        \"email\": None\n",
    "                    })\n",
    "                    \n",
    "                    if len(authors) >= 6:  # Limit to 6 authors\n",
    "                        break\n",
    "            \n",
    "            return authors\n",
    "            \n",
    "        elif field == \"keywords\":\n",
    "            keywords = [k.strip() for k in response.split(\",\") if k.strip()]\n",
    "            return keywords[:8]  # Limit to 8\n",
    "            \n",
    "        elif field == \"references\":\n",
    "            if response.lower() == \"none found\" or not response.strip():\n",
    "                return []\n",
    "            \n",
    "            references = []\n",
    "            # Handle format: \"Title1 - Authors (Year) Journal | Title2 - Authors (Year) Journal\"\n",
    "            if \"|\" in response:\n",
    "                ref_parts = response.split(\"|\")\n",
    "            else:\n",
    "                ref_parts = [response]\n",
    "                \n",
    "            for ref_part in ref_parts:\n",
    "                ref_part = ref_part.strip()\n",
    "                if not ref_part or ref_part.lower() == \"none found\":\n",
    "                    continue\n",
    "                \n",
    "                title = \"\"\n",
    "                authors = \"\"\n",
    "                year = None\n",
    "                journal = \"\"\n",
    "                \n",
    "                # Try to parse \"Title - Authors (Year) Journal\" format\n",
    "                if \" - \" in ref_part:\n",
    "                    title_part, rest = ref_part.split(\" - \", 1)\n",
    "                    title = title_part.strip()\n",
    "                    \n",
    "                    # Look for (Year) pattern in the rest\n",
    "                    import re\n",
    "                    year_match = re.search(r'\\((\\d{4})\\)', rest)\n",
    "                    if year_match:\n",
    "                        year = int(year_match.group(1))\n",
    "                        # Split around the year to get authors and journal\n",
    "                        before_year = rest[:year_match.start()].strip()\n",
    "                        after_year = rest[year_match.end():].strip()\n",
    "                        authors = before_year\n",
    "                        journal = after_year\n",
    "                    else:\n",
    "                        # No year found, treat rest as authors\n",
    "                        authors = rest\n",
    "                else:\n",
    "                    # No \" - \" separator, treat whole thing as title\n",
    "                    title = ref_part\n",
    "                \n",
    "                references.append({\n",
    "                    \"title\": title,\n",
    "                    \"authors\": authors,\n",
    "                    \"year\": year,\n",
    "                    \"journal\": journal\n",
    "                })\n",
    "                \n",
    "            return references[:5]  # Limit to 5\n",
    "            \n",
    "        elif field == \"funding_sources\":\n",
    "            if response.lower() == \"none found\" or not response.strip():\n",
    "                return []\n",
    "            \n",
    "            funding = [f.strip() for f in response.split(\",\") if f.strip() and f.strip().lower() != \"none found\"]\n",
    "            return funding[:5]  # Limit to 5\n",
    "            \n",
    "        elif field == \"conference_info\":\n",
    "            if response.lower() == \"none found\" or not response.strip():\n",
    "                return {\"location\": None, \"date\": None}\n",
    "            \n",
    "            location = None\n",
    "            date = None\n",
    "            \n",
    "            # Handle format: \"Location: City, Country | Date: date range\"\n",
    "            if \"|\" in response:\n",
    "                parts = response.split(\"|\")\n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    if part.lower().startswith(\"location:\"):\n",
    "                        location = part.split(\":\", 1)[1].strip()\n",
    "                    elif part.lower().startswith(\"date:\"):\n",
    "                        date = part.split(\":\", 1)[1].strip()\n",
    "            else:\n",
    "                # Try to detect location/date in single string\n",
    "                if any(word in response.lower() for word in [\"location\", \"city\", \"country\"]):\n",
    "                    location = response.strip()\n",
    "                elif any(word in response.lower() for word in [\"date\", \"may\", \"june\", \"july\", \"august\", \"september\"]):\n",
    "                    date = response.strip()\n",
    "                else:\n",
    "                    # Assume it's location if no clear indicator\n",
    "                    location = response.strip()\n",
    "            \n",
    "            return {\"location\": location, \"date\": date}\n",
    "        else:\n",
    "            return response\n",
    "\n",
    "print(\"‚úÖ QwenExtractor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:37:52.532143Z",
     "iopub.status.busy": "2025-08-30T02:37:52.532012Z",
     "iopub.status.idle": "2025-08-30T02:38:21.765781Z",
     "shell.execute_reply": "2025-08-30T02:38:21.765371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Method 2: Qwen Local Extraction\n",
      "============================================================\n",
      "üìè Extracted 3732 characters\n",
      "ü§ñ Initializing Qwen2.5-1.5B model...\n",
      "üì• Loading Qwen/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756521474.524981 2471982 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756521474.530754 2471982 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756521474.546670 2471982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756521474.546697 2471982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756521474.546699 2471982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756521474.546701 2471982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully\n",
      "üîç Extracting metadata components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ TITLE: Influence of Drug-Polymer Interactions on Release Kinetics of PLGA and PLA/PET Nanoparticles\n",
      "\n",
      "üë• AUTHORS: 5 found\n",
      "   ‚Ä¢ Merve Gul (University of Pavia)\n",
      "   ‚Ä¢ Ida Genta (University of Pavia)\n",
      "   ‚Ä¢ Maria M. Perez Madrigal (Universitat Polit√®cnica de Catalunya)\n",
      "   ‚Ä¢ Carlos Aleman (Universitat Polit√®cnica de Catalunya)\n",
      "   ‚Ä¢ Enrica Chiesa (University of Pavia)\n",
      "\n",
      "üìù SUMMARY: The study investigates how drug-polymer interactions affect release kinetics in poly(lactic-co-glyco...\n",
      "\n",
      "üîë KEYWORDS: drug-polymer interactions, release kinetics, PLGA, PLA/PEG, nps\n",
      "\n",
      "üî¨ METHODS: Microfluidic-based synthesis of nano-sized carriers for drug delivery systems (NDDS), precise contro...\n",
      "\n",
      "üìä RESULTS: Release kinetics of PLGA nanoparticles were significantly influenced by drug-polymer interactions co...\n",
      "\n",
      "üìö REFERENCES: 1 found\n",
      "   ‚Ä¢ --- Page 1 ---...\n",
      "\n",
      "üí∞ FUNDING: 0 sources\n",
      "\n",
      "üèõÔ∏è  CONFERENCE: Bari, Italy | May 15-17\n",
      "‚è±Ô∏è  Processing time: 29.23s\n",
      "üíæ Results saved to: ../output/method2_qwen_results.json\n",
      "‚úÖ Method 2 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Run extraction\n",
    "pdf_path = \"../data/test-poster.pdf\"\n",
    "\n",
    "if Path(pdf_path).exists():\n",
    "    print(\"üöÄ Running Method 2: Qwen Local Extraction\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract text\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"üìè Extracted {len(text)} characters\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize extractor\n",
    "        print(\"ü§ñ Initializing Qwen2.5-1.5B model...\")\n",
    "        extractor = QwenExtractor()\n",
    "        \n",
    "        # Extract each field\n",
    "        print(\"üîç Extracting metadata components...\")\n",
    "        \n",
    "        title = extractor.extract_field(text, \"title\")\n",
    "        authors = extractor.extract_field(text, \"authors\")\n",
    "        summary = extractor.extract_field(text, \"summary\")\n",
    "        keywords = extractor.extract_field(text, \"keywords\")\n",
    "        methods = extractor.extract_field(text, \"methods\")\n",
    "        results_text = extractor.extract_field(text, \"results\")\n",
    "        references = extractor.extract_field(text, \"references\")\n",
    "        funding_sources = extractor.extract_field(text, \"funding_sources\")\n",
    "        conference_info = extractor.extract_field(text, \"conference_info\")\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"summary\": summary,\n",
    "            \"keywords\": keywords,\n",
    "            \"methods\": methods,\n",
    "            \"results\": results_text,\n",
    "            \"references\": references,\n",
    "            \"funding_sources\": funding_sources,\n",
    "            \"conference_info\": conference_info,\n",
    "            \"extraction_metadata\": {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"processing_time\": time.time() - start_time,\n",
    "                \"method\": \"qwen_local\",\n",
    "                \"model\": \"Qwen2.5-1.5B-Instruct\",\n",
    "                \"device\": str(next(extractor.model.parameters()).device),\n",
    "                \"text_length\": len(text),\n",
    "                \"do_sample\": False,\n",
    "                \"max_tokens\": 150\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüìÑ TITLE: {results['title'][:100]}\")\n",
    "        print(f\"\\nüë• AUTHORS: {len(results['authors'])} found\")\n",
    "        for author in results[\"authors\"]:\n",
    "            affil_str = f\" ({', '.join(author['affiliations'])})\" if author['affiliations'] else \"\"\n",
    "            print(f\"   ‚Ä¢ {author['name']}{affil_str}\")\n",
    "        \n",
    "        print(f\"\\nüìù SUMMARY: {results['summary'][:100]}...\")\n",
    "        print(f\"\\nüîë KEYWORDS: {', '.join(results['keywords'][:5])}\")\n",
    "        print(f\"\\nüî¨ METHODS: {results['methods'][:100]}...\")\n",
    "        print(f\"\\nüìä RESULTS: {results['results'][:100]}...\")\n",
    "        print(f\"\\nüìö REFERENCES: {len(results['references'])} found\")\n",
    "        for ref in results['references'][:2]:  # Show first 2\n",
    "            print(f\"   ‚Ä¢ {ref['title'][:50]}...\")\n",
    "        print(f\"\\nüí∞ FUNDING: {len(results['funding_sources'])} sources\")\n",
    "        for funding in results['funding_sources'][:2]:  # Show first 2\n",
    "            print(f\"   ‚Ä¢ {funding[:50]}...\")\n",
    "        print(f\"\\nüèõÔ∏è  CONFERENCE: {results['conference_info']['location']} | {results['conference_info']['date']}\")\n",
    "        print(f\"‚è±Ô∏è  Processing time: {results['extraction_metadata']['processing_time']:.2f}s\")\n",
    "        \n",
    "        # Save results\n",
    "        output_path = Path(\"../output/method2_qwen_results.json\")\n",
    "        output_path.parent.mkdir(exist_ok=True)\n",
    "        \n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Results saved to: {output_path}\")\n",
    "        print(\"‚úÖ Method 2 completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Qwen extraction failed: {e}\")\n",
    "        print(\"   This may be due to insufficient GPU memory or model download issues\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Test poster not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
