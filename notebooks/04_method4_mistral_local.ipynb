{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 4: Mistral-7B-Instruct Local Extraction\n",
    "\n",
    "**Simple Poster Metadata Extraction**\n",
    "\n",
    "This notebook demonstrates how to use **Mistral-7B-Instruct-v0.3** for scientific poster metadata extraction using the same direct prompt style as DeepSeek but running locally.\n",
    "\n",
    "## ‚ú® Key Advantages:\n",
    "- **Simple & Direct**: Uses the same prompt as DeepSeek\n",
    "- **Local Processing**: No API costs, complete privacy\n",
    "- **Good JSON Output**: Mistral handles structured data generation well\n",
    "- **Efficient**: 7B parameters with 8-bit quantization\n",
    "- **Reliable**: Consistent, deterministic results\n",
    "\n",
    "## üéØ Results Preview:\n",
    "- ‚úÖ **5/5 Authors** extracted with affiliations\n",
    "- ‚úÖ **Complete JSON** structure in one go\n",
    "- ‚úÖ **Complete metadata** (title, summary, keywords, methods, results, references, conference)\n",
    "- ‚úÖ **~2 minutes** processing time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T21:22:07.276919Z",
     "iopub.status.busy": "2025-08-30T21:22:07.276641Z",
     "iopub.status.idle": "2025-08-30T21:22:10.285443Z",
     "shell.execute_reply": "2025-08-30T21:22:10.285051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ All imports successful!\n",
      "üî• CUDA available: True\n",
      "üéÆ GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "import time\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "print(\"üì¶ All imports successful!\")\n",
    "print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ The DeepSeek-Style Prompt\n",
    "\n",
    "This is the same simple, direct prompt that works so well with DeepSeek API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T21:22:10.316536Z",
     "iopub.status.busy": "2025-08-30T21:22:10.316174Z",
     "iopub.status.idle": "2025-08-30T21:22:10.320876Z",
     "shell.execute_reply": "2025-08-30T21:22:10.320569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core functions defined\n",
      "üéØ Same direct style as DeepSeek - no complex multi-step logic needed!\n"
     ]
    }
   ],
   "source": [
    "def normalize_characters(text: str) -> str:\n",
    "    \"\"\"Clean up text encoding issues\"\"\"\n",
    "    replacements = {\n",
    "        '\\u2019': \"'\", '\\u2018': \"'\", '\\u201c': '\"', '\\u201d': '\"',\n",
    "        '\\u2013': '-', '\\u2014': '--', '\\u2026': '...', '\\u00a0': ' ',\n",
    "        '\\u2022': '‚Ä¢', '\\u00b0': '¬∞', '\\u03b1': 'alpha', '\\u03b2': 'beta',\n",
    "        '\\u03bc': 'mu', '\\u2264': '<=', '\\u2265': '>=', '\\u00b1': '¬±'\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract and normalize text from PDF using PyMuPDF\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        page_text = page.get_text()\n",
    "        if page_text:\n",
    "            text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
    "    doc.close()\n",
    "    return normalize_characters(text.strip())\n",
    "\n",
    "def create_mistral_prompt(text: str) -> str:\n",
    "    \"\"\"Create the same prompt style as DeepSeek for Mistral - using FULL text\"\"\"\n",
    "    return f\"\"\"<s>[INST] You are a scientific metadata extraction expert. Extract structured information from this poster text with high precision.\n",
    "\n",
    "POSTER TEXT:\n",
    "{text}\n",
    "\n",
    "EXTRACTION INSTRUCTIONS:\n",
    "1. Look for title in ALL CAPS or large text at the top\n",
    "2. Find all author names (often with superscript numbers for affiliations)  \n",
    "3. Identify institutional affiliations (usually below authors)\n",
    "4. Extract 6-8 specific keywords from methods and results sections\n",
    "5. Summarize key findings concisely\n",
    "6. Find funding acknowledgments (often at bottom) - look for \"Acknowledgements\" section, grant numbers, Marie Curie fellowships, EU funding\n",
    "\n",
    "Return ONLY valid JSON in this exact format:\n",
    "{{\n",
    "  \"title\": \"exact poster title as written\",\n",
    "  \"authors\": [\n",
    "    {{\"name\": \"Full Name\", \"affiliations\": [\"University/Institution\"], \"email\": \"email@domain.com or null\"}}\n",
    "  ],\n",
    "  \"summary\": \"2-sentence summary of research objective and main finding\",\n",
    "  \"keywords\": [\"specific\", \"technical\", \"terms\", \"from\", \"poster\", \"content\"],\n",
    "  \"methods\": \"detailed methodology description from poster\",\n",
    "  \"results\": \"quantitative results and key findings with numbers if present\",\n",
    "  \"references\": [\n",
    "    {{\"title\": \"paper title\", \"authors\": \"author names\", \"year\": 2024, \"journal\": \"journal name\"}}\n",
    "  ],\n",
    "  \"funding_sources\": [\"specific funding agency or grant numbers\"],\n",
    "  \"conference_info\": {{\"location\": \"city, country\", \"date\": \"date range\"}}\n",
    "}}\n",
    "\n",
    "Be precise and thorough. Extract only information explicitly present in the text. [/INST]\"\"\"\n",
    "\n",
    "print(\"‚úÖ Core functions defined\")\n",
    "print(\"üéØ Same direct style as DeepSeek - no complex multi-step logic needed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Complete Extraction Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T21:22:10.322375Z",
     "iopub.status.busy": "2025-08-30T21:22:10.322099Z",
     "iopub.status.idle": "2025-08-30T21:23:58.780464Z",
     "shell.execute_reply": "2025-08-30T21:23:58.779932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Method 4: Mistral-7B-Instruct Local Extraction\n",
      "=================================================================\n",
      "üìè Extracted 3732 characters\n",
      "ü§ñ Loading mistralai/Mistral-7B-Instruct-v0.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 14:22:12.293375: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756588932.311254 2707665 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756588932.316823 2707665 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756588932.331965 2707665 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756588932.331982 2707665 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756588932.331984 2707665 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756588932.331986 2707665 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-30 14:22:12.336721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ddf87a6b9143e9bf1c1f92adef4a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mistral model loaded successfully!\n",
      "üîÑ Generating response...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Raw response length: 2846 chars\n",
      "\n",
      "üìÑ TITLE: INFLUENCE OF DRUG-POLYMER INTERACTIONS ON RELEASE KINETICS OF PLGA AND PLA/PEG NPS\n",
      "üë• AUTHORS: 5 found\n",
      "   ‚Ä¢ Merve Gul (University of Pavia)\n",
      "   ‚Ä¢ Ida Genta (University of Pavia)\n",
      "   ‚Ä¢ Maria M. Perez Madrigal (Universitat PoliteÃÄcnica de Catalunya (UPC-EEBE))\n",
      "   ‚Ä¢ Carlos Aleman (Universitat PoliteÃÄcnica de Catalunya (UPC-EEBE), Barcelona Research Center for Multiscale Science and Engineering, EEBE, Universitat PoliteÃÄcnica de Catalunya)\n",
      "   ‚Ä¢ Enrica Chiesa (1Department of Drug Sciences, University of Pavia)\n",
      "\n",
      "üìù SUMMARY: Research investigates antimicrobial drug delivery systems using microfluidic-synthesized nano-carrie...\n",
      "üîë KEYWORDS: antimicrobial resistance, drug delivery systems, nanocarriers, microfluidics, PLGA\n",
      "üí∞ FUNDING: 1 sources\n",
      "üìö REFERENCES: 3 found\n",
      "‚è±Ô∏è  Processing time: 108.39s\n",
      "üíæ Results saved to: ../output/method4_mistral_results.json\n",
      "‚úÖ Method 4 completed successfully!\n",
      "üéØ Same prompt as DeepSeek, working local results!\n"
     ]
    }
   ],
   "source": [
    "# Complete extraction pipeline\n",
    "def load_mistral_model():\n",
    "    \"\"\"Load Mistral-7B-Instruct model with 8-bit quantization\"\"\"\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    print(f\"ü§ñ Loading {model_name}...\")\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_threshold=6.0,\n",
    "        llm_int8_has_fp16_weight=False,\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Mistral model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def clean_mistral_response(response: str) -> str:\n",
    "    \"\"\"Clean Mistral response to extract pure JSON\"\"\"\n",
    "    prefixes_to_remove = [\n",
    "        \"Here's the extracted metadata in JSON format:\",\n",
    "        \"Here is the extracted metadata:\",\n",
    "        \"Based on the poster text, here's the extracted metadata:\",\n",
    "        \"The extracted metadata is:\",\n",
    "        \"```json\", \"```\"\n",
    "    ]\n",
    "    \n",
    "    cleaned = response.strip()\n",
    "    for prefix in prefixes_to_remove:\n",
    "        if cleaned.startswith(prefix):\n",
    "            cleaned = cleaned[len(prefix):].strip()\n",
    "    \n",
    "    if cleaned.endswith(\"```\"):\n",
    "        cleaned = cleaned[:-3].strip()\n",
    "    \n",
    "    start_idx = cleaned.find('{')\n",
    "    end_idx = cleaned.rfind('}')\n",
    "    \n",
    "    if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
    "        cleaned = cleaned[start_idx:end_idx + 1]\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def extract_with_mistral(text: str, model, tokenizer) -> Dict:\n",
    "    \"\"\"Extract metadata using Mistral-7B-Instruct\"\"\"\n",
    "    prompt = create_mistral_prompt(text)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4000)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    print(\"üîÑ Generating response...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1500,\n",
    "            do_sample=False,  # Deterministic output\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    prompt_length = len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))\n",
    "    generated_text = response[prompt_length:].strip()\n",
    "    \n",
    "    print(f\"üìù Raw response length: {len(generated_text)} chars\")\n",
    "    \n",
    "    cleaned_response = clean_mistral_response(generated_text)\n",
    "    \n",
    "    try:\n",
    "        return json.loads(cleaned_response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON parsing error: {e}\")\n",
    "        print(f\"üîç Cleaned response: {cleaned_response[:500]}...\")\n",
    "        raise\n",
    "\n",
    "# Run the complete extraction\n",
    "pdf_path = \"../data/test-poster.pdf\"\n",
    "\n",
    "if Path(pdf_path).exists():\n",
    "    print(\"üöÄ Running Method 4: Mistral-7B-Instruct Local Extraction\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract text\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"üìè Extracted {len(text)} characters\")\n",
    "    \n",
    "    # Load model and extract\n",
    "    model, tokenizer = load_mistral_model()\n",
    "    metadata = extract_with_mistral(text, model, tokenizer)\n",
    "    \n",
    "    # Add processing metadata\n",
    "    processing_time = time.time() - start_time\n",
    "    metadata['extraction_metadata'] = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'processing_time': processing_time,\n",
    "        'method': 'mistral_7b_instruct_local',\n",
    "        'model': 'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "        'text_length': len(text),\n",
    "        'quantization': '8-bit'\n",
    "    }\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüìÑ TITLE: {metadata['title']}\")\n",
    "    print(f\"üë• AUTHORS: {len(metadata['authors'])} found\")\n",
    "    for author in metadata['authors']:\n",
    "        affiliations = ', '.join(author['affiliations']) if author['affiliations'] else 'None'\n",
    "        print(f\"   ‚Ä¢ {author['name']} ({affiliations})\")\n",
    "    \n",
    "    print(f\"\\nüìù SUMMARY: {metadata['summary'][:100]}...\")\n",
    "    print(f\"üîë KEYWORDS: {', '.join(metadata['keywords'][:5])}\")\n",
    "    print(f\"üí∞ FUNDING: {len(metadata.get('funding_sources', []))} sources\")\n",
    "    print(f\"üìö REFERENCES: {len(metadata.get('references', []))} found\")\n",
    "    print(f\"‚è±Ô∏è  Processing time: {processing_time:.2f}s\")\n",
    "    \n",
    "    # Save results\n",
    "    output_path = Path(\"../output/method4_mistral_results.json\")\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Results saved to: {output_path}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"‚úÖ Method 4 completed successfully!\")\n",
    "    print(\"üéØ Same prompt as DeepSeek, working local results!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Test poster not found: {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f6382f949a948449788cb265fafa37b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "272cf96e16474e4da71b28c0a24ef73c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a263113b00c4ff4a82e070136616e75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7b585a93de444c5987e82316f5fc063a",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_80d4c429d55143a79a3865d30d397707",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "7ad2437d99e04118b65b309e627ed9d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b585a93de444c5987e82316f5fc063a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80d4c429d55143a79a3865d30d397707": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "92310fbdba014387a5ed4f05d7c89c39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_272cf96e16474e4da71b28c0a24ef73c",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_ee3ad5a54d914b7e9f1ec6016306c124",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
      }
     },
     "e7ddf87a6b9143e9bf1c1f92adef4a1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_92310fbdba014387a5ed4f05d7c89c39",
        "IPY_MODEL_7a263113b00c4ff4a82e070136616e75",
        "IPY_MODEL_ecedd7823879483d9217efa8b6dabbae"
       ],
       "layout": "IPY_MODEL_7ad2437d99e04118b65b309e627ed9d2",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ecedd7823879483d9217efa8b6dabbae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f6382f949a948449788cb265fafa37b",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_fc4725e183fb4b9a88cf1b424bf7c92f",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá3/3‚Äá[00:04&lt;00:00,‚Äá‚Äá1.53s/it]"
      }
     },
     "ee3ad5a54d914b7e9f1ec6016306c124": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fc4725e183fb4b9a88cf1b424bf7c92f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
