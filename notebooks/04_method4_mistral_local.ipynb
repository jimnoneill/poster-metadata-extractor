{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Method 4: Mistral-7B-Instruct Local Extraction\n",
        "\n",
        "**Simple Poster Metadata Extraction**\n",
        "\n",
        "This notebook demonstrates how to use **Mistral-7B-Instruct-v0.3** for scientific poster metadata extraction using the same direct prompt style as DeepSeek but running locally.\n",
        "\n",
        "## âœ¨ Key Advantages:\n",
        "- **Simple & Direct**: Uses the same prompt as DeepSeek\n",
        "- **Local Processing**: No API costs, complete privacy\n",
        "- **Good JSON Output**: Mistral handles structured data generation well\n",
        "- **Efficient**: 7B parameters with 8-bit quantization\n",
        "- **Reliable**: Consistent, deterministic results\n",
        "\n",
        "## ğŸ¯ Results Preview:\n",
        "- âœ… **5/5 Authors** extracted with affiliations\n",
        "- âœ… **Complete JSON** structure in one go\n",
        "- âœ… **Complete metadata** (title, summary, keywords, methods, results, references, conference)\n",
        "- âœ… **~2 minutes** processing time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¦ Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import json\n",
        "import fitz  # PyMuPDF\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Any\n",
        "import time\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "print(\"ğŸ“¦ All imports successful!\")\n",
        "print(f\"ğŸ”¥ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ The DeepSeek-Style Prompt\n",
        "\n",
        "This is the same simple, direct prompt that works so well with DeepSeek API:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_characters(text: str) -> str:\n",
        "    \"\"\"Clean up text encoding issues\"\"\"\n",
        "    replacements = {\n",
        "        '\\u2019': \"'\", '\\u2018': \"'\", '\\u201c': '\"', '\\u201d': '\"',\n",
        "        '\\u2013': '-', '\\u2014': '--', '\\u2026': '...', '\\u00a0': ' ',\n",
        "        '\\u2022': 'â€¢', '\\u00b0': 'Â°', '\\u03b1': 'alpha', '\\u03b2': 'beta',\n",
        "        '\\u03bc': 'mu', '\\u2264': '<=', '\\u2265': '>=', '\\u00b1': 'Â±'\n",
        "    }\n",
        "    for old, new in replacements.items():\n",
        "        text = text.replace(old, new)\n",
        "    return text\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"Extract and normalize text from PDF using PyMuPDF\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num, page in enumerate(doc):\n",
        "        page_text = page.get_text()\n",
        "        if page_text:\n",
        "            text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
        "    doc.close()\n",
        "    return normalize_characters(text.strip())\n",
        "\n",
        "def create_mistral_prompt(text: str) -> str:\n",
        "    \"\"\"Create the same prompt style as DeepSeek for Mistral - using FULL text\"\"\"\n",
        "    return f\"\"\"<s>[INST] You are a scientific metadata extraction expert. Extract structured information from this poster text with high precision.\n",
        "\n",
        "POSTER TEXT:\n",
        "{text}\n",
        "\n",
        "EXTRACTION INSTRUCTIONS:\n",
        "1. Look for title in ALL CAPS or large text at the top\n",
        "2. Find all author names (often with superscript numbers for affiliations)  \n",
        "3. Identify institutional affiliations (usually below authors)\n",
        "4. Extract 6-8 specific keywords from methods and results sections\n",
        "5. Summarize key findings concisely\n",
        "6. Find funding acknowledgments (often at bottom) - look for \"Acknowledgements\" section, grant numbers, Marie Curie fellowships, EU funding\n",
        "\n",
        "Return ONLY valid JSON in this exact format:\n",
        "{{\n",
        "  \"title\": \"exact poster title as written\",\n",
        "  \"authors\": [\n",
        "    {{\"name\": \"Full Name\", \"affiliations\": [\"University/Institution\"], \"email\": \"email@domain.com or null\"}}\n",
        "  ],\n",
        "  \"summary\": \"2-sentence summary of research objective and main finding\",\n",
        "  \"keywords\": [\"specific\", \"technical\", \"terms\", \"from\", \"poster\", \"content\"],\n",
        "  \"methods\": \"detailed methodology description from poster\",\n",
        "  \"results\": \"quantitative results and key findings with numbers if present\",\n",
        "  \"references\": [\n",
        "    {{\"title\": \"paper title\", \"authors\": \"author names\", \"year\": 2024, \"journal\": \"journal name\"}}\n",
        "  ],\n",
        "  \"funding_sources\": [\"specific funding agency or grant numbers\"],\n",
        "  \"conference_info\": {{\"location\": \"city, country\", \"date\": \"date range\"}}\n",
        "}}\n",
        "\n",
        "Be precise and thorough. Extract only information explicitly present in the text. [/INST]\"\"\"\n",
        "\n",
        "print(\"âœ… Core functions defined\")\n",
        "print(\"ğŸ¯ Same direct style as DeepSeek - no complex multi-step logic needed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ Complete Extraction Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete extraction pipeline\n",
        "def load_mistral_model():\n",
        "    \"\"\"Load Mistral-7B-Instruct model with 8-bit quantization\"\"\"\n",
        "    model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "    print(f\"ğŸ¤– Loading {model_name}...\")\n",
        "    \n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        llm_int8_threshold=6.0,\n",
        "        llm_int8_has_fp16_weight=False,\n",
        "    )\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    print(\"âœ… Mistral model loaded successfully!\")\n",
        "    return model, tokenizer\n",
        "\n",
        "def clean_mistral_response(response: str) -> str:\n",
        "    \"\"\"Clean Mistral response to extract pure JSON\"\"\"\n",
        "    prefixes_to_remove = [\n",
        "        \"Here's the extracted metadata in JSON format:\",\n",
        "        \"Here is the extracted metadata:\",\n",
        "        \"Based on the poster text, here's the extracted metadata:\",\n",
        "        \"The extracted metadata is:\",\n",
        "        \"```json\", \"```\"\n",
        "    ]\n",
        "    \n",
        "    cleaned = response.strip()\n",
        "    for prefix in prefixes_to_remove:\n",
        "        if cleaned.startswith(prefix):\n",
        "            cleaned = cleaned[len(prefix):].strip()\n",
        "    \n",
        "    if cleaned.endswith(\"```\"):\n",
        "        cleaned = cleaned[:-3].strip()\n",
        "    \n",
        "    start_idx = cleaned.find('{')\n",
        "    end_idx = cleaned.rfind('}')\n",
        "    \n",
        "    if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
        "        cleaned = cleaned[start_idx:end_idx + 1]\n",
        "    \n",
        "    return cleaned\n",
        "\n",
        "def extract_with_mistral(text: str, model, tokenizer) -> Dict:\n",
        "    \"\"\"Extract metadata using Mistral-7B-Instruct\"\"\"\n",
        "    prompt = create_mistral_prompt(text)\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4000)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    print(\"ğŸ”„ Generating response...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=1500,\n",
        "            do_sample=False,  # Deterministic output\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1,\n",
        "            use_cache=True\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    prompt_length = len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))\n",
        "    generated_text = response[prompt_length:].strip()\n",
        "    \n",
        "    print(f\"ğŸ“ Raw response length: {len(generated_text)} chars\")\n",
        "    \n",
        "    cleaned_response = clean_mistral_response(generated_text)\n",
        "    \n",
        "    try:\n",
        "        return json.loads(cleaned_response)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"âŒ JSON parsing error: {e}\")\n",
        "        print(f\"ğŸ” Cleaned response: {cleaned_response[:500]}...\")\n",
        "        raise\n",
        "\n",
        "# Run the complete extraction\n",
        "pdf_path = \"../data/test-poster.pdf\"\n",
        "\n",
        "if Path(pdf_path).exists():\n",
        "    print(\"ğŸš€ Running Method 4: Mistral-7B-Instruct Local Extraction\")\n",
        "    print(\"=\" * 65)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Extract text\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    print(f\"ğŸ“ Extracted {len(text)} characters\")\n",
        "    \n",
        "    # Load model and extract\n",
        "    model, tokenizer = load_mistral_model()\n",
        "    metadata = extract_with_mistral(text, model, tokenizer)\n",
        "    \n",
        "    # Add processing metadata\n",
        "    processing_time = time.time() - start_time\n",
        "    metadata['extraction_metadata'] = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'processing_time': processing_time,\n",
        "        'method': 'mistral_7b_instruct_local',\n",
        "        'model': 'mistralai/Mistral-7B-Instruct-v0.3',\n",
        "        'text_length': len(text),\n",
        "        'quantization': '8-bit'\n",
        "    }\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"\\nğŸ“„ TITLE: {metadata['title']}\")\n",
        "    print(f\"ğŸ‘¥ AUTHORS: {len(metadata['authors'])} found\")\n",
        "    for author in metadata['authors']:\n",
        "        affiliations = ', '.join(author['affiliations']) if author['affiliations'] else 'None'\n",
        "        print(f\"   â€¢ {author['name']} ({affiliations})\")\n",
        "    \n",
        "    print(f\"\\nğŸ“ SUMMARY: {metadata['summary'][:100]}...\")\n",
        "    print(f\"ğŸ”‘ KEYWORDS: {', '.join(metadata['keywords'][:5])}\")\n",
        "    print(f\"ğŸ’° FUNDING: {len(metadata.get('funding_sources', []))} sources\")\n",
        "    print(f\"ğŸ“š REFERENCES: {len(metadata.get('references', []))} found\")\n",
        "    print(f\"â±ï¸  Processing time: {processing_time:.2f}s\")\n",
        "    \n",
        "    # Save results\n",
        "    output_path = Path(\"../output/method4_mistral_results.json\")\n",
        "    output_path.parent.mkdir(exist_ok=True)\n",
        "    \n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    print(f\"ğŸ’¾ Results saved to: {output_path}\")\n",
        "    \n",
        "    # Clean up\n",
        "    del model, tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    print(\"âœ… Method 4 completed successfully!\")\n",
        "    print(\"ğŸ¯ Same prompt as DeepSeek, working local results!\")\n",
        "    \n",
        "else:\n",
        "    print(f\"âŒ Test poster not found: {pdf_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
